<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>study on しさく</title><link>https://iimuz.github.io/scrapbook/tags/study/</link><description>Recent content in study on しさく</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sat, 01 Feb 2020 14:39:41 +0900</lastBuildDate><atom:link href="https://iimuz.github.io/scrapbook/tags/study/index.xml" rel="self" type="application/rss+xml"/><item><title>Poisson distribution</title><link>https://iimuz.github.io/scrapbook/study/poisson_distribution/</link><pubDate>Sat, 01 Feb 2020 14:39:41 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/poisson_distribution/</guid><description> Wikipedia ポワソン分布 定義: $ Poiss(\lambda) = \frac{\lambda^k e^{-\lambda}}{\lambda !} $ $lambda$ は 0 以上。</description></item><item><title>Monte Carlo method</title><link>https://iimuz.github.io/scrapbook/study/monte_carlo_method/</link><pubDate>Thu, 30 Jan 2020 09:05:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/monte_carlo_method/</guid><description>Wikipedia モンテカルロ法 シミュレーションや数値計算を乱数を用いて行う手法の総称。
略称: MC 法 ランダムな値を生成する方法。</description></item><item><title>Hamiltonian Monte Carlo</title><link>https://iimuz.github.io/scrapbook/book/hamiltonian_monte_carlo/</link><pubDate>Thu, 30 Jan 2020 00:34:45 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/book/hamiltonian_monte_carlo/</guid><description> Wikipedia Hamiltonian Monte Carlo Metropolis Hastings Algorithm の改良版。</description></item><item><title>Metropolis-Hastings Algorithm</title><link>https://iimuz.github.io/scrapbook/study/metropolis_hasings_algorithm/</link><pubDate>Thu, 30 Jan 2020 00:29:01 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/metropolis_hasings_algorithm/</guid><description>Wikipedia [メトロポリス・ヘイスティングス法] 直接サンプリングするのが難しい確率分布から 統計標本の配列を生成するのに用いられるマルコフ連鎖を構築するのに用いられる手法
ランダムに取得した値が前回の値よりもカーネル比の値が高い場合に採用するプロセスを繰り返す。</description></item><item><title>A Style-Based Generator Architecture for Generative Adversarial Networks</title><link>https://iimuz.github.io/scrapbook/study/a_style_based_generator_architecture_for_generative_adversarial_networks/</link><pubDate>Sun, 26 Jan 2020 19:36:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/a_style_based_generator_architecture_for_generative_adversarial_networks/</guid><description> arXiv 著者: Tero Karras, Samuli Laine, Timo Aila 投稿日: 2018.12.12 Official Tensorflow Implementation: GitHub NVlabas/stylegan 参考資料 2019.12.21 Qiita StyleGAN を遊び尽くせ!! ~追加学習不要の画像編集~</description></item><item><title>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title><link>https://iimuz.github.io/scrapbook/study/efficientnet_rethinking_model_scaling_for_convolutional_neural_networks/</link><pubDate>Sat, 25 Jan 2020 14:17:02 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/efficientnet_rethinking_model_scaling_for_convolutional_neural_networks/</guid><description> arXiv 著者: Mingxing Tan, Quoc V. Le 投稿日: 2019.5.28 モデルのスケールアップを行う際に必要な方法を考察した文献です。 考察結果を利用してモデルを大きくすることで、SoTA を実現した。
参考資料 2019.10.30 Qiita 2019 年最強の画像認識モデル EfficientNet 解説</description></item><item><title>Prognostics</title><link>https://iimuz.github.io/scrapbook/study/prognostics/</link><pubDate>Thu, 23 Jan 2020 11:10:35 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/prognostics/</guid><description> Wikipedia Prognostics 類義語 Prognostics &amp;amp; Health Management (PHM)</description></item><item><title>Mathworks 予知保全</title><link>https://iimuz.github.io/scrapbook/study/mathworks_predictive_maintenance/</link><pubDate>Thu, 23 Jan 2020 01:26:52 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/mathworks_predictive_maintenance/</guid><description>Mathworks の公開している予知保全に関係する資料に対するメモです。
予知保全で直面しやすい 4 つの課題とその対処法 予知保全に関して全体像を記述している文献です。 細かい方法に関しては記載されていませんが、 予知保全技術の開発に際しての心構えのようなものを記載しています。 実例に学ぶ予知保全向けデータ活用 課題として下記 2 点が挙げられています。 装置の時系列データを保存するためのストレージなどの制限 解析を実行する人員の不足 Mondi Gronau の事例を用いて予知保全のシステム構築の例を示しています。 ただし、個別技術に関しては記載されていません。 MATLAB/Simulink による予知保全ビデオシリーズ 予知保全シリーズ Part 1: 予知保全の概要と開発事例 タイトルの通り概要と事例の紹介及び製品の紹介です。 技術的な内容には踏み込んでいません。 予知保全シリーズ Part 2: 予知保全システムの開発フロー 基本的なモデル(SVM や Decision Tree 系など)を利用して、 異常検知のシステムを Matlab で構築する手順の紹介です。 予知保全シリーズ Part 3: 予知保全を可能にする特徴量選択 Matlab を利用して特徴量をヒストグラムなどで可視化して、 分類に寄与しそうな特徴量を手動で選択する方法の紹介です。 予知保全シリーズ Part 4: 機器の寿命を予測するモデル構築 寿命予測の方式に関して説明しています。 機器の寿命予測を行う方法として、下記の分類を利用しています。 ただし、各手法に関する説明はありませんでした。 類似性モデル: 故障までのセンサー情報が多数ある場合 Hash Similarity Model: データ量が膨大な場合 Pairwise Similarity Model: 信号形状を基準に判断する場合 Residual Similarity Model: 劣化の時系列モデルを構築する場合 劣化モデル: 閾値が設定できる場合 Linear Degradation Model: ダメージの蓄積がない場合 Exponential Degradation Model: ダメージの蓄積がある場合 生存モデル: 故障までに掛かった時間情報がある場合 Reliability Survival Model: 関連変数がない場合 Covariance Survival Model: 関連変数がある場合 ここでは、 Exponential Degradation Model を Matlab で利用する方法を紹介しています。 最適化処理などは全て Matlab 関数で記載されているため、内容は確認できません。 下記の関数に対して事後確率分布を求めることで推定しているようです。 Exponentail function: $ h(t) = \phi + \theta \exp\left( \beta t + \epsilon - \frac{\sigma^2}{2} \right) $ 補足資料として、類似性モデルを使った例が示されています。 方法の詳細は示されていませんが、 過去の寿命までの時系列データの中から類似するサンプルを検索し寿命予測を行うようです。 予知保全シリーズ Part 5: 「故障データが無い」場合のアプローチ 故障データがない場合の方法に関して、PCA とマハラノビス距離を利用した方法が述べられています。 もう一つは、故障モデルを考えシミュレーションなどにより故障を予測する例が記載されています。</description></item><item><title>Forecasting remaining useful life: Interpretable deep learning approach via variational Bayesian inferences</title><link>https://iimuz.github.io/scrapbook/study/forecasting_remaining_useful_life_interpretable_deep_leanring_approach_via_variational_bayesian_inferences/</link><pubDate>Tue, 21 Jan 2020 06:39:31 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/forecasting_remaining_useful_life_interpretable_deep_leanring_approach_via_variational_bayesian_inferences/</guid><description> arXiv 著者: Mathias Kraus, Stefan Feuerriegel 投稿日: 2019.7.11 GitHub Mathias/PredictiveMaintenance</description></item><item><title>Decision Tree</title><link>https://iimuz.github.io/scrapbook/study/decision_tree/</link><pubDate>Sun, 19 Jan 2020 20:46:27 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/decision_tree/</guid><description> Wikipedia JP 決定木 Wikipedia EN Decision tree 参考資料 2019.9.16 キヨシの命題 決定木アルゴリズムの重要度(importance)を正しく解釈しよう</description></item><item><title>Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI</title><link>https://iimuz.github.io/scrapbook/study/explainable_artificial_intelligence_xai_concepts_taxonomies_opportunities_and_challenges_toward_responsible_ai/</link><pubDate>Sun, 19 Jan 2020 20:25:33 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/explainable_artificial_intelligence_xai_concepts_taxonomies_opportunities_and_challenges_toward_responsible_ai/</guid><description> arXiv 著者: Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera 投稿日: 2019.10.22</description></item><item><title>Datascience</title><link>https://iimuz.github.io/scrapbook/study/datascience/</link><pubDate>Sun, 19 Jan 2020 20:05:05 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/datascience/</guid><description> Wikipedia データサイエンス 参考資料 2019.12.14 Qiita 半導体生産技術者がデータ分析業界に飛び込んでみて感じたこと</description></item><item><title>Modern Neural Networks Generalize on Small Data Sets</title><link>https://iimuz.github.io/scrapbook/study/modern_neural_networks_generalize_on_small_data_sets/</link><pubDate>Sun, 19 Jan 2020 19:15:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/modern_neural_networks_generalize_on_small_data_sets/</guid><description> NIPS Proceedings 著者: Matthew Olson, Abraham Wyner, Richard Berk 投稿日: NIPS 2018</description></item><item><title>Deep Learning on Small Datasets without Pre-Training using Cosine Loss</title><link>https://iimuz.github.io/scrapbook/study/deep_learning_on_small_datasets_without_pre_training_using_cosine_loss/</link><pubDate>Sun, 19 Jan 2020 18:41:55 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_learning_on_small_datasets_without_pre_training_using_cosine_loss/</guid><description> arXiv 著者: Björn Barz, Joachim Denzler 投稿日: 2019.1.25</description></item><item><title>CondConv: Conditionally Parameterized Convolutions for Efficient Inference</title><link>https://iimuz.github.io/scrapbook/study/condconv_conditional_parameterized_convolutions_for_efficient_inference/</link><pubDate>Sun, 19 Jan 2020 17:56:07 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/condconv_conditional_parameterized_convolutions_for_efficient_inference/</guid><description> arXiv NIPS 2019 著者: Brandon Yang, Gabriel Bender, Quoc V. Le, Jiquan Ngiam 投稿日: 2019.4.10 畳み込み計算において、サンプル毎に畳み込みの重み係数を変える研究。 畳み込みに利用する係数は、プールされている重み係数の線形和で生成されるようです。
参考資料 2019.12.11 Qiita 次世代の畳み込み?! CondConv TensorFlow GitHub tensorflow/tpu</description></item><item><title>Dynamic Convolution: Attention over Convolution Kernels</title><link>https://iimuz.github.io/scrapbook/study/dynamic_convolution_attention_over_convolution_kernels/</link><pubDate>Sun, 19 Jan 2020 17:53:23 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/dynamic_convolution_attention_over_convolution_kernels/</guid><description> arXiv 著者: Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, Zicheng Liu 投稿日: 2019.12.7 参考資料 2019.12.11 GitHub arXivTimes/arXivTimes</description></item><item><title>ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring</title><link>https://iimuz.github.io/scrapbook/study/remixmatch_semi_supervised_learning_with_distribution_alignment_and_augmentation_anchoring/</link><pubDate>Sun, 19 Jan 2020 17:36:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/remixmatch_semi_supervised_learning_with_distribution_alignment_and_augmentation_anchoring/</guid><description> arXiv OpenReview 少量のラベル付き画像を用意し、そこから他の画像に対するラベリングを行い精度を高めた方法。
参考資料 2019.12.6 Qiita 【論文読み】クラスタリングが無くなる日？</description></item><item><title>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</title><link>https://iimuz.github.io/scrapbook/study/enhancing_the_locality_and_breaking_the_memory_bottleneck_of_transofrmer_on_time_series_forecasting/</link><pubDate>Fri, 17 Jan 2020 16:00:31 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/enhancing_the_locality_and_breaking_the_memory_bottleneck_of_transofrmer_on_time_series_forecasting/</guid><description>arXiv 著者: Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, Xifeng Yan 投稿日: 2019.6.29 Transformer の機構を時系列データに適用した文献。 単純に時系列データに適用するとメモリ部分の使用量が大きくなりすぎる。 そこで、畳み込みと local attention と restart attention という方法を提案している。</description></item><item><title>CDSA: Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation</title><link>https://iimuz.github.io/scrapbook/study/cdsa_cross_dimensional_self_attention_for_multivariate_geo_tagged_time_series_imputation/</link><pubDate>Fri, 17 Jan 2020 15:43:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cdsa_cross_dimensional_self_attention_for_multivariate_geo_tagged_time_series_imputation/</guid><description>arXiv 著者: Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, Shih-Fu Chang 投稿日: 2019.5.23 センサデータ(気温とか)の時系列とジオタグ(位置情報)を合わせて、 欠損しているセンサデータを復元する、または予測するタスク。 Cross Dimensional SElf-Attention という方法を提案し、各情報を統合的に扱えるようにしている。</description></item><item><title>Attend and Diagnose: Clinical Time Series Analysis using Attention Models</title><link>https://iimuz.github.io/scrapbook/study/attend_and_diagnose_clinical_time_series_analysis_using_attention_models/</link><pubDate>Fri, 17 Jan 2020 15:13:17 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/attend_and_diagnose_clinical_time_series_analysis_using_attention_models/</guid><description>arXiv 著者: Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan, Andreas Spanias 投稿日: 2017.11.10 時系列データに対して Self-Attention を用いています。</description></item><item><title>Convolutional Newral Network (CNN)</title><link>https://iimuz.github.io/scrapbook/study/cnn/</link><pubDate>Fri, 17 Jan 2020 14:58:21 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cnn/</guid><description> Wikipedia 畳み込みニューラルネットワーク</description></item><item><title>Pay Less Attention with Lightweight and Dynamic Convolutions</title><link>https://iimuz.github.io/scrapbook/study/pay_less_attention_with_lightweight_and_dynamic_convolutions/</link><pubDate>Fri, 17 Jan 2020 14:56:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/pay_less_attention_with_lightweight_and_dynamic_convolutions/</guid><description>arXiv 著者: Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, Michael Auli 投稿日: 2019.1.29 Self-Attention の代替として Dynamic CNN を提案している。 Self-Attention よりも軽量で高速に計算可能。</description></item><item><title>Are Sixteen Heads Really Better than One?</title><link>https://iimuz.github.io/scrapbook/study/are_sixteen_heads_really_better_than_one/</link><pubDate>Fri, 17 Jan 2020 10:18:37 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/are_sixteen_heads_really_better_than_one/</guid><description>arXiv 著者: Paul Michel, Omer Levy, Graham Neubig 投稿日: 2019.5.25 Transformer 系などでは、 Multi-headed Attention を利用します。 そこで、 Multi-headed Attention がどこまで有効なのかを検証した論文です。 結論としては、全てのケースで Multi-headed Attention が有効なわけではなく、 統計的に優位な劣化なしに Multi-headed Attention の数を減らすことができます。 そして、 Self-Attention よりも Encoder-Decoder Layer の Attention の方が有効のようです。 加えて、各 Head の有効性は、学習の初期に決定されるようです。</description></item><item><title>What Does BERT Look At? An Analysis of BERT's Attention</title><link>https://iimuz.github.io/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/</link><pubDate>Fri, 17 Jan 2020 10:06:35 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/</guid><description> arXiv 著者: Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning 投稿日: 2019.6.11</description></item><item><title>Self-attention with Functional Time Representation Learning</title><link>https://iimuz.github.io/scrapbook/study/self_attention_with_functional_time_representation_learning/</link><pubDate>Fri, 17 Jan 2020 09:58:53 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/self_attention_with_functional_time_representation_learning/</guid><description> arXiv 著者: Da Xu, Chuanwei Ruan, Sushant Kumar, Evren Korpeoglu, Kannan Achan 投稿日: 2019.11.28</description></item><item><title>XLNet: Generalized Autoregressive Pretraining for Language Understanding</title><link>https://iimuz.github.io/scrapbook/study/xlnet/</link><pubDate>Fri, 17 Jan 2020 09:55:57 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/xlnet/</guid><description> arXiv 著者: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le 投稿日: 2019.6.19</description></item><item><title>Temporal Pattern Attention for Multivariate Time Series Forecasting</title><link>https://iimuz.github.io/scrapbook/study/temporal_pattern_attention_for_multivariate_time_series_forecasting/</link><pubDate>Fri, 17 Jan 2020 09:08:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/temporal_pattern_attention_for_multivariate_time_series_forecasting/</guid><description> arXiv 著者: Shun-Yao Shih, Fan-Keng Sun, Hung-yi Lee 投稿日: 2018.9.12 実装: GitHub gantheory/TPA-LSTM</description></item><item><title>Recurrent Neural Network (RNN)</title><link>https://iimuz.github.io/scrapbook/study/rnn/</link><pubDate>Fri, 17 Jan 2020 09:00:58 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/rnn/</guid><description> Wikipedia 回帰型ニューラルネットワーク</description></item><item><title>Long Short-Term Memory (LSTM)</title><link>https://iimuz.github.io/scrapbook/study/lstm/</link><pubDate>Fri, 17 Jan 2020 09:00:07 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/lstm/</guid><description> Wikipedia 長・短期記憶</description></item><item><title>Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks</title><link>https://iimuz.github.io/scrapbook/study/modeling_long_and_short_term_temporal_patterns_with_deep_neural_networks/</link><pubDate>Fri, 17 Jan 2020 08:58:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/modeling_long_and_short_term_temporal_patterns_with_deep_neural_networks/</guid><description> arXiv 著者: Guokun Lai, Wei-Cheng Chang, Yiming Yang, Hanxiao Liu 投稿日: 2017.3.21</description></item><item><title>Time Series Anomaly Detection / Prediction</title><link>https://iimuz.github.io/scrapbook/study/time_series_anomaly_detection_prediction/</link><pubDate>Fri, 17 Jan 2020 08:50:33 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/time_series_anomaly_detection_prediction/</guid><description>時系列データに対して異常検知/予測を行う方法に関するメモです。
参考資料 2019.3.3 Anomaly Detection with Time Series Forecasting Anomaly Detection with Time Series Forecasting ARIMA や SARIMA で時系列予測を行い、差分から異常検知するということを書いている文献です。 LSTM を利用するところまで実装が載っていますが、結局精度よく予測しすぎると異常検知できないことを示しています。</description></item><item><title>Transformer</title><link>https://iimuz.github.io/scrapbook/study/transformer/</link><pubDate>Thu, 16 Jan 2020 11:41:24 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/transformer/</guid><description> 2017.8.31 Google AI Blog Transformer: A Novel Neural Network Architecture for Language Understanding 2018.6.27 The Illustrated Transformer 2019.8.22 Reddit Positional Encoding in Transformer Positional Encoding が有効なのは、 Word Embedding における高次元空間において、 凡そ直行するからではないかということらしい。</description></item><item><title>Flow-based Deep Generative Models</title><link>https://iimuz.github.io/scrapbook/study/flow_based_deep_generative_models/</link><pubDate>Thu, 16 Jan 2020 11:32:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/flow_based_deep_generative_models/</guid><description> 2018.10.13 Lil'Log Flow-based Deep Generative Models 参考資料 2019.3.28 SlideShare DL輪読会: Flow-based Deep Generative Models</description></item><item><title>Generative Model</title><link>https://iimuz.github.io/scrapbook/study/generative_model/</link><pubDate>Thu, 16 Jan 2020 11:03:23 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/generative_model/</guid><description>Wikipedia Generative Model メモ 生成モデルには、下記のパターンがある。
自己回帰型モデル: MADE1, Dependency Network, DRAW2, PixelRNN, PixelCNN, SkecthRNN 潜在変数モデル: VAE, GAN 参考資料 2017.6.16 ZOZO Tech Blog 自己回帰型モデルの深層学習 2016.5.24 Sequential Neural Models with Stochastic Layers: VAE の decoder として RNN や自己回帰モデルを用いる例 2016.11.15 PixelVAE: A Latent Variable Model for Natural Images: VAE の decoder として PixelCNN を利用する例 2016.1.25 Pixel Recurrent Neural Networks 2017.4.20 Fast Generation for Convolutional Autoregressive Models: 自己回帰型の例 2015.</description></item><item><title>統計学</title><link>https://iimuz.github.io/scrapbook/study/statistics/</link><pubDate>Tue, 14 Jan 2020 08:53:29 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/statistics/</guid><description>Wikipedia 統計学 経験的に得られたばらつきのあるデータから、 応用数学の手法を用いて数値上の性質や規則性、不規則性を見出す。</description></item><item><title>Automated Machine Learning</title><link>https://iimuz.github.io/scrapbook/study/automl/</link><pubDate>Mon, 13 Jan 2020 20:02:13 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/automl/</guid><description>Wikipedia Automated machine learning ツール or ライブラリ Auto-Sklearn: Scikit-Learn の自動化ライブラリ。 Auto-WEKA: Java 製の統計解析ツール。 adanet: tensorflow ベースのネットワーク自動構築ライブラリ。 クラウドサービス Amazon AWS Machine Learning Google Cloud Cloud AutoML Microsoft Azure What is automated machine learning? 参考資料 2018.10.10 Qiita Automated Machine Learning(@Azure ML)を試す 2019.9.30 Qiita 【機械学習ツール徹底比較】Amazon SageMaker / Google AutoML / Microsoft ML / IBM AutoAI 使用事例と強み 2019.11.17 SlideShare Azure Machine Learning アップデートセミナー 20191127 Azure のエコシステムを利用した機械学習フローを実現するための入門のような感じです。 2019.</description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>https://iimuz.github.io/scrapbook/study/bert/</link><pubDate>Mon, 13 Jan 2020 17:29:43 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/bert/</guid><description> arXiv 投稿日: 2018.10.11 著者: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova 参考資料 2019.12.31 Kaggle Notebook BERT Baseline PyTorch の transformers ライブラリを用いて BERT の Fine tuning から予測までが書かれている。 2020.1.9 SlideShare BERT 入門</description></item><item><title>因果推論 (Causal Inference)</title><link>https://iimuz.github.io/scrapbook/study/causal_inference/</link><pubDate>Sun, 12 Jan 2020 21:37:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/causal_inference/</guid><description> Wikipedia Causal Inference 参考資料 2020.1.7 COMEMO 2020 年こそ理解したい「因果推論」の勉強はじめました</description></item><item><title>DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection</title><link>https://iimuz.github.io/scrapbook/study/deepfakes_and_beyond_a_survey_of_face_manipulation_and_fake_detection/</link><pubDate>Sun, 12 Jan 2020 21:32:53 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deepfakes_and_beyond_a_survey_of_face_manipulation_and_fake_detection/</guid><description> arXiv 投稿日: 2020.1.1 著者: Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, Javier Ortega-Garcia タイトルにあるように、フェイク画像系のまとめ論文です。
参考資料 2020.1.7 arXivTimes</description></item><item><title>Smoothness and Stability in GANs</title><link>https://iimuz.github.io/scrapbook/study/smoothness_and_stability_in_gans/</link><pubDate>Sun, 12 Jan 2020 21:27:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/smoothness_and_stability_in_gans/</guid><description> OpenReview 投稿日: 2019.9.26 著者: Casey Chu, Kentaro Minami, Kenji Fukumizu GAN の収束に関する研究です。
参考資料 2020.1.6 PFN 【ICLR2020 採択論文】GAN のなめらかさと安定性 PFN の 2019年夏季インターンの成果だった用です。</description></item><item><title>2019 年論文まとめ</title><link>https://iimuz.github.io/scrapbook/study/review_2019_paperwork/</link><pubDate>Sun, 12 Jan 2020 14:47:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/review_2019_paperwork/</guid><description>参考資料 2019.12.9 HEARTBEAT 2019’s Top Machine and Deep Learning Research Papers 2019.12.25 Qiita Twitter で振り返る 2019 年の Deep Learning 論文（前編） 2020.1.1 Qiita Twitter で振り返る 2019 年の Deep Learning 論文（後編） 2019.12.31 Qiita 2019 年のおもしろかった DL/ML 論文 10 選</description></item><item><title>Graph Neural Network (GNN)</title><link>https://iimuz.github.io/scrapbook/study/graph_neural_network/</link><pubDate>Sun, 12 Jan 2020 14:47:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/graph_neural_network/</guid><description>参考資料 2019.2.3 Qiita GNN まとめ(1): GCN の導入 2019.2.28 Qiita GNN まとめ(2): 様々な Spatial GCN 2019.3.18 Qiita GNN まとめ(3): 発展編とこれから</description></item><item><title>異種混合学習</title><link>https://iimuz.github.io/scrapbook/study/multi_mixture_learning/</link><pubDate>Fri, 10 Jan 2020 15:04:31 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/multi_mixture_learning/</guid><description>NEC が開発する異種混合学習技術。
NEC [異種混合学習][mec] 異種混合学習技術とビッグデータ分析ソリューションの研究開発</description></item><item><title>不均衡データ (Imbalanced Data)</title><link>https://iimuz.github.io/scrapbook/study/imbalanced_data/</link><pubDate>Thu, 09 Jan 2020 20:21:43 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/imbalanced_data/</guid><description>機械学習において学習データセットのデータがクラスごとにサンプル数が違う状態を指します。
概要 不均衡データに対するアプローチとしては下記の手法がある。
データレベルのアプローチ: 多い方のラベルデータを減らしたり、 少ないほうのラベルデータを増やしたりする1 2 コスト考慮型学習: 少ないほうのラベルデータを誤分類することに対して、 より重いペナルティを課すように損失関数を定義する1 2 異常検知手法の適用: 少ないほうのラベルを異常値とみなす1 モデル評価値の算出手法: 通常の accuracy からデータ数を考慮した accuracy を利用して評価する2 データレベルのアプローチ Undersampling 多いほうのラベルデータを減らすことでデータの均衡をとる方法です。 一見すると簡単で有効そうに感じられるが幾つかの問題がある1。
多いほうのラベルデータからランダムサンプルするだけでは、データに偏りが発生する。 予めクラスタ分析などを実施してからサンプリングを行う必要がある。1 学習した分類器の分散が大きくなるため、 UnderBagging5 のような平均化戦略をとる。 UnderBagging では、 Bagging でアンサンブルを行う。 事後分布がゆがむため、事前分布を用いて事後分布を修正する4。 Undersampling が有効な場合はどういう場合かは [3] で説明されている。 Oversampling 少ないほうのラベルデータを水増しすることでデータの均衡をとる方法です。 代表的な手法として、 Synthetic Minority Oversampling TEchnique (SMOTE)11 がある。 SMOTE は、データ点同士をつないだ線分譲の任意の点をランダムに人口データとして生成する手法。
SMOTE の拡張は下記のような方針に分類できる1 13。
Oversampling のためのデータ点の選択方法 Undersampling を Oversampling の前後いずれで実行するか 補完方法 データ生成を次元削減などの処理に対していつ実施するか データ生成に適応的な手法を利用するかどうか 再ラベル付けをするか SMOTE 実施後のノイズ除去ステップを入れるかどうか SMOTE の拡張として、よく利用される方法は下記になる1 13。</description></item><item><title>Sanity Checks for Saliency Maps</title><link>https://iimuz.github.io/scrapbook/study/sanity_checks_for_saliency_maps/</link><pubDate>Sun, 05 Jan 2020 19:56:38 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/sanity_checks_for_saliency_maps/</guid><description> arXiv 著者: Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim 投稿日: 2018.10.8 可視化手法を比較し可視化手法が本当にモデルの状態を説明しているのかを調べた論文です。
参考資料 2019.11.24: Hatena Blog: データ分析関連のまとめ: Sanity Checks for Saliency Maps</description></item><item><title>Variational Autoencoder</title><link>https://iimuz.github.io/scrapbook/study/variational_autoencoder/</link><pubDate>Sun, 05 Jan 2020 19:45:13 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/variational_autoencoder/</guid><description> Wikipedia: Autoencoder#Variational autoencoder(VAE) 参考資料 2017.7.19: Qiita: Variational Autoencoder徹底解説</description></item><item><title>Autoencoder</title><link>https://iimuz.github.io/scrapbook/study/autoencoder/</link><pubDate>Sun, 05 Jan 2020 19:40:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/autoencoder/</guid><description> Wikipedia: オートエンコーダ 入力と出力が同じになるように学習を行う。 Deep Learning で利用する。 亜種として下記のようなネットワークもある。
Variational Autoencoder 参考文献 2016.10.9: DeepAge: オートエンコーダ：抽象的な特徴を自己学習するディープラーニングの人気者</description></item><item><title>ICLR2020</title><link>https://iimuz.github.io/scrapbook/study/iclr2020/</link><pubDate>Sun, 05 Jan 2020 19:37:51 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iclr2020/</guid><description> ICLR2020 会期: 2020.4.26 - 30</description></item><item><title>International Conference on Learning Representations (ICLR)</title><link>https://iimuz.github.io/scrapbook/study/iclr/</link><pubDate>Sun, 05 Jan 2020 19:36:29 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iclr/</guid><description> Wikipedia: International Conference on Learning Representations</description></item><item><title>Iterative energy-based projection on a normal data manifold for anomaly localization</title><link>https://iimuz.github.io/scrapbook/study/iterative_energy_based_projection_on_a_normal_data_manifold_for_anomaly_localization/</link><pubDate>Sun, 05 Jan 2020 19:32:32 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iterative_energy_based_projection_on_a_normal_data_manifold_for_anomaly_localization/</guid><description> OpenReview.net 著者: David Dehaene, Oriel Frigo, Sébastien Combrexelle, Pierre Eline 投稿日: 2019.9.26 参考資料 Qiita: ICLR2020の異常検知論文を実装してみた Slide Share: ICLR2020の異常検知論文の紹介</description></item><item><title>勉強法</title><link>https://iimuz.github.io/scrapbook/others/study_method/</link><pubDate>Sun, 05 Jan 2020 16:46:14 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/others/study_method/</guid><description>参考情報 本 2018.1.4 Foundations of Data Science Microsoft の人が書いた Data Science の本。 英語ですが、基本的なことが網羅されているようです。 オンライン講座 Chainer Tutorial Deep Learning の学習用としては良いが、 Chainer 自体の開発が終わってしまっている。 東大 松尾研究室 演習コンテンツ 公開ページ DL4US Deep Learning 基礎講座 GCI データサイエンティスト 育成講座 Kaggle Learn Coursera Andrew Ng 教授 機械学習講座 Deep Learning 講座 An MIT Presbook Deep Learning Udemy 【TensorFlow・Keras で学ぶ】時系列データ処理入門（RNN/LSTM, Word2Vec) 全くのゼロから「駆け出しデータサイエンティスト」を育てる方法論 Hatena Blog: 全くのゼロから「駆け出しデータサイエンティスト」を育てる方法論 投稿日: 2019.</description></item><item><title>Hugo Tips</title><link>https://iimuz.github.io/scrapbook/software/hugo_tips/</link><pubDate>Sun, 05 Jan 2020 16:39:47 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/software/hugo_tips/</guid><description>Hugo の Tips を記載します。
脚注の記載方法 footnote を利用することができます。(1) 書き方は下記のようになります。
hoge text[^a] [^a]: footnote hoge 2018.10.31 Hugoで脚注（footnote）を使用する方法&amp;#8617;</description></item><item><title>Pesudo-Label</title><link>https://iimuz.github.io/scrapbook/study/pesudo_label/</link><pubDate>Sun, 05 Jan 2020 16:33:26 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/pesudo_label/</guid><description>ラベル付けされたデータセットと、ラベル付けされていないデータセットが与えられる。 この時に、ラベル付けされたデータセットだけで学習したモデルで、 ラベル付けされていないデータセットのラベルを予測する。 予測したラベルを利用して、全データセットを使って再度モデルを学習することにより、 精度が向上する場合がある。
結局のところ、 MAP 推定などにおける正則化項の役割を果たすことにより精度が向上するようです(1)。
2019..11.4: Hatena Blog: なぜ疑似ラベルが効果的か調べてみた&amp;#8617;</description></item><item><title>SinGAN: Learning a Generative Model from a Single Natural Image</title><link>https://iimuz.github.io/scrapbook/study/singan_learning_a_generative_model_from_a_single_natural_image/</link><pubDate>Sun, 05 Jan 2020 16:05:01 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/singan_learning_a_generative_model_from_a_single_natural_image/</guid><description> arXiv 著者: Tamar Rott Shaham, Tali Dekel, Tomer Michaeli 投稿日: 2019.5.2 公式ページ: SinGAN: Learning a Generative Model from a Single Natural Image 公式実装: GitHub: tamarott/SinGAN 参考情報 2019.11.13: Qiita: 【論文解説】SinGAN: Learning a Generative Model from a Single Natural Image 2019.12.18: Qiita: ICCV2019 Best paper SinGAN のできる事できない事[実践編] 単一の画像を使って学習を行うことのできる方法を提案しています。 階層的な GAN を用いており、 1 つ分だけ低階層の Generator が生成した画像と、 ノイズを入力として上位階層を学習します。
ネットワークアーキテクチャ Generator のアーキテクチャ</description></item><item><title>Matrix Profile</title><link>https://iimuz.github.io/scrapbook/study/matrix_profile/</link><pubDate>Sun, 05 Jan 2020 15:47:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/matrix_profile/</guid><description>2 つの時系列において、部分時系列同士の近さを計算した行列を指します。
The UCR Matrix Profile Page ライブラリ Matrix Profile Foundation GitHub: target/matrixprofile-ts: python ライブラリ 参考情報 2019.11.15: Qiita: MatrixProfileによるECGデータの異常検知</description></item><item><title>A Closer Look at Spatiotemporal Convolutions for Action Recognition</title><link>https://iimuz.github.io/scrapbook/study/a_closer_look_at_spatiotemporal_convolutions_for_action_recognition/</link><pubDate>Sun, 05 Jan 2020 15:17:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/a_closer_look_at_spatiotemporal_convolutions_for_action_recognition/</guid><description> arXiv 投稿日: 2017.11.30 著者: Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri CVPR 2018 で採択されている。</description></item><item><title>Time Series</title><link>https://iimuz.github.io/scrapbook/study/time_series/</link><pubDate>Sun, 05 Jan 2020 15:12:41 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/time_series/</guid><description>Wikipedia: 時系列 参考資料 Everything you can do with a time series Kaggle Kernel: Everything you can do with a time series 時系列データ(特に、株価チャート)を中心としてチュートリアルを作成している。 ろうそく足チャートとかの説明もある。 ARMA モデルに始まり、SARIMA などの紹介もしている。
Time series Basics: Exploring traditional TS Kaggle Kernel: Time series Basics: Exploring traditional TS 時系列単変量データの解析が中心のようです。ARMA モデルや ARIMA モデル、SARIMA に関して記載しています。 時系列多変量データに関しても商品データ(ショップ毎、アイテムごと)を利用して説明しています。 ただし、予測自体は単変量とほぼ変わらないようです。 欠損値の取り扱いなどに関しても記載されていないです。
2018.9.23 時系列データで Variational AutoEncoder keras 学習する天然ニューラルネット 時系列データで Variational AutoEncoder keras Autoencoder に RNN を適用して時系列用にした例</description></item><item><title>International Conference on Computer Vision</title><link>https://iimuz.github.io/scrapbook/study/iccv/</link><pubDate>Sun, 05 Jan 2020 15:10:50 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iccv/</guid><description> Wikipedia: International Conference on Computer Vision</description></item><item><title>Learning Spatiotemporal Features with 3D Convolutional Networks</title><link>https://iimuz.github.io/scrapbook/study/learning_spatiotemporal_features_with_3d_convolutional_networks/</link><pubDate>Sun, 05 Jan 2020 15:01:00 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/learning_spatiotemporal_features_with_3d_convolutional_networks/</guid><description>arXiv 投稿日: 2014.12.2 著者: Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri 時空間3次元畳み込みによる 3D CNN (C3D) を提案している。</description></item><item><title>Metric Learning</title><link>https://iimuz.github.io/scrapbook/study/metric_learning/</link><pubDate>Sun, 05 Jan 2020 12:47:57 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/metric_learning/</guid><description> Wikipedia: Similarity Learning#Metric Learning</description></item><item><title>Joint Discriminative and Generative Learning for Person Re-identification</title><link>https://iimuz.github.io/scrapbook/study/joint_descriminative_and_generative_leanring_for_person_reidentification/</link><pubDate>Sun, 05 Jan 2020 12:41:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/joint_descriminative_and_generative_leanring_for_person_reidentification/</guid><description> arXiv 投稿日: 2019.4.15 著者: Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, Jan Kautz NVIDIA から CVPR2019 の Oral セッションで発表された論文です。 GAN と Metric Learning を組み合わせて人物の識別精度を向上しています。
参考文献 Person Re-identification 論文解説 Joint Discriminative and Generative Learning for Perosn Re-identification を読む Qiita: Person Re-identification 論文解説 Joint Discriminative and Generative Learning for Perosn Re-identification を読む 投稿日: 2019.11.6</description></item><item><title>Tiny Video Networks</title><link>https://iimuz.github.io/scrapbook/study/tiny_video_networks/</link><pubDate>Sun, 05 Jan 2020 12:23:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/tiny_video_networks/</guid><description> arXiv 投稿日: 2019.10.15 著者: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo 映像認識において、実行速度を向上するためにモデルを軽量化する。 モデルの探索において、実行時間やパラメータの制約をかけ探索を行う。
参考文献 Tiny Video Networks [ML論文読] 精度を保ったまま映像認識を軽量化する Qiita: Tiny Video Networks [ML論文読] 精度を保ったまま映像認識を軽量化する 投稿日: 2019.10.30</description></item><item><title>Natural Language Processing</title><link>https://iimuz.github.io/scrapbook/study/natural_language_processing/</link><pubDate>Sun, 05 Jan 2020 12:18:14 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/natural_language_processing/</guid><description> Wikipedia: 自然言語処理 参考資料 2020.1.3 Hatena Blog 機械学習 Memo φ(・ω・ ) 自然言語処理における Embedding の方法一覧とサンプルコード 幾つかのサンプルコードと共に、 BERT 手法などまでの手法解説が行われています。</description></item><item><title>Anomaly Detection</title><link>https://iimuz.github.io/scrapbook/study/anomaly_detection/</link><pubDate>Sun, 05 Jan 2020 12:16:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/anomaly_detection/</guid><description> Wikipedia: 異常検知 参考資料 2018.10.9 Deep Learning を用いた教師なし画像検査の論文調査 GAN/SVM/Autoencoder とか .pdf 2019.2.10 Anomaly Detection with Isolation Forest &amp;amp; Visualization</description></item><item><title>CVPR2018</title><link>https://iimuz.github.io/scrapbook/study/cvpr2018/</link><pubDate>Sun, 05 Jan 2020 06:34:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2018/</guid><description> CVPR2018 会期: 2018.6.19 - 21</description></item><item><title>ICCV2015</title><link>https://iimuz.github.io/scrapbook/study/iccv2015/</link><pubDate>Sun, 05 Jan 2020 06:09:19 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iccv2015/</guid><description> ICCV2015</description></item><item><title>LightGBM</title><link>https://iimuz.github.io/scrapbook/study/lightgbm/</link><pubDate>Sun, 05 Jan 2020 04:21:26 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/lightgbm/</guid><description> GitHub: microsoft/LightGBM</description></item><item><title>Hist Gradient Boosting Tree</title><link>https://iimuz.github.io/scrapbook/study/gradient_boosting_decision_tree/</link><pubDate>Sun, 05 Jan 2020 04:18:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/gradient_boosting_decision_tree/</guid><description> Wikipedia: Gradinet Boosting#Gradinet Tree Boosting</description></item><item><title>Scikit Learn</title><link>https://iimuz.github.io/scrapbook/study/scikit-learn/</link><pubDate>Sun, 05 Jan 2020 04:16:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/scikit-learn/</guid><description> Wikipedia: scikit-learn</description></item><item><title>Hist Gradient Boosting Tree</title><link>https://iimuz.github.io/scrapbook/study/hist_gradient_boosting_tree/</link><pubDate>Sun, 05 Jan 2020 04:10:55 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/hist_gradient_boosting_tree/</guid><description>sklearn.ensemble.HistGradientBoostingClassifier Scikit-Learn に実装されている GBDT の手法です。
参考文献 最新機械学習モデル HistGradientBoostingTree の性能調査(LightGBM と比較検証) Qiita: 最新機械学習モデル HistGradientBoostingTree の性能調査(LightGBM と比較検証) 投稿日: 2019.5.27 使い方から LightGBM との比較まで記載されています。</description></item><item><title>CVPR 2020</title><link>https://iimuz.github.io/scrapbook/study/cvpr2020/</link><pubDate>Sun, 05 Jan 2020 04:01:47 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2020/</guid><description> CVPR 2020 会期: 2019.6.14 - 19</description></item><item><title>CVPR 2019</title><link>https://iimuz.github.io/scrapbook/study/cvpr2019/</link><pubDate>Sun, 05 Jan 2020 04:00:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2019/</guid><description> CVPR 2019 会期: 2019.6.16 - 20 参考資料 CVPR 2019 速報 Slide Share: CVPR 2019 速報 投稿日: 2019.6.21 CVPR 2019 網羅的サーベイ報告会の抜粋 Qiita: CVPR 2019 網羅的サーベイ報告会の抜粋 投稿日: 2019.11.8 簡単なメモ
CVPR 2019 速報の簡易まとめです。 画像識別の手法において現在の主流は Residual Net です。 GAN は研究が活発な分野の一つである。主要な手法の一覧が掲載されています。 CVPR のトレンドを創っている論文で時系列を扱っていそうな文献として下記が記載されています。 Learning Spatiotemporal Features with 3D Convolutional Networks A Closer Look at Spatiotemporal Convolutions for Action Recognition</description></item><item><title>Conference</title><link>https://iimuz.github.io/scrapbook/study/conference/</link><pubDate>Sun, 05 Jan 2020 03:58:00 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/conference/</guid><description> Wikipedia: 研究会</description></item><item><title>Conference on Computer Vision and Pattern Recognition (CVPR)</title><link>https://iimuz.github.io/scrapbook/study/crpr/</link><pubDate>Sun, 05 Jan 2020 03:55:33 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/crpr/</guid><description> [Wikipedia: Conference on Computer Vision and Pattern Recognition]</description></item><item><title>Deep Affinity Network for Multiple Object Tracking</title><link>https://iimuz.github.io/scrapbook/study/deep_afinity_network_for_multiple_object_tracking/</link><pubDate>Sun, 05 Jan 2020 03:29:25 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_afinity_network_for_multiple_object_tracking/</guid><description> arXiv 投稿日: 2018.10.28 著者: ShiJie Sun, Naveed Akhtar, HuanSheng Song, Ajmal Mian, Mubarak Shah 単純に物体検出をしてトラッキングするのではなく、トラッキング対象の識別を含めて行う手法です。 この文献では、あるフレームで検出対象が隠れてしまい、トラッキングが途切れても、 それ以降で再び現れた場合に同じ物体であると識別できるようにしています。
参考文献 多人数トラッキング論文解説 Deep Affinity Network for Multiple Object Tracking を読む Qiita: 多人数トラッキング論文解説 Deep Affinity Network for Multiple Object Tracking を読む 投稿日: 2019.11.3</description></item><item><title>Attention Is All You Need</title><link>https://iimuz.github.io/scrapbook/study/attention/</link><pubDate>Fri, 03 Jan 2020 21:59:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/attention/</guid><description>arXiv 著者: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin 投稿日: 2017.6.12 参考資料 GitHub 著者らによる Tensorflow 実装 tensorflow/tensor2tensor 2017.12.21 Hatena Blog ディープラーニングブログ 論文解説 Attention Is All You Need (Transformer) 論文自体を日本語で解説してあり、非常にわかりやすい。 2018.3.24 srome.github.io Understanding Attention in Neural Networks Mathematically 2018.12.4 Qiita 作って理解する Transformer / Attention 各モジュールの意味と単純な実装が記載されており、理解しやすい。 テストコードもあり参考になる。 2019.8.13 StackExchange What exactly are keys, queries, and values in attention mechanisms?</description></item><item><title>European Union regulations on algorithmic decision-making and a "right to explanation"</title><link>https://iimuz.github.io/scrapbook/study/european_union_regulations_on_algorithmic_decision_making_and_a_right_to_explanation/</link><pubDate>Fri, 03 Jan 2020 21:50:05 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/european_union_regulations_on_algorithmic_decision_making_and_a_right_to_explanation/</guid><description> arXiv 著者: Bryce Goodman, Seth Flaxman 投稿日: 2016.6.28</description></item><item><title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title><link>https://iimuz.github.io/scrapbook/study/show_attend_and_tell_neural_image_caption_generation_with_visual_attention/</link><pubDate>Fri, 03 Jan 2020 21:46:38 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/show_attend_and_tell_neural_image_caption_generation_with_visual_attention/</guid><description> arXiv 著者: Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio 投稿日: 2015.2.10</description></item><item><title>Detecting and Explaining Crisis</title><link>https://iimuz.github.io/scrapbook/study/detecting_and_explaining_crisis/</link><pubDate>Fri, 03 Jan 2020 21:43:17 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/detecting_and_explaining_crisis/</guid><description> arXiv 著者: Rohan Kshirsagar, Robert Morris, Sam Bowman 投稿日: 2017.3.26</description></item><item><title>Interpretability</title><link>https://iimuz.github.io/scrapbook/study/interpretability/</link><pubDate>Fri, 03 Jan 2020 21:28:40 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/interpretability/</guid><description>機械学習における解釈性の話。
ディープラーニングの判断根拠を理解する手法 機械学習の解釈性に関して、日本語で色々な文献や考え方がまとめられています。
Qiita: ディープラーニングの判断根拠を理解する手法 投稿日: 2017.9.6 参考文献 SmoothGrad: removing noise by adding noise: Deep Learning において判断根拠を提示する手法 Detecting and Explaining Crisis: SNS において危機的状況を判断しまた判断根拠を提示する手法 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention: Attention を利用して判断根拠を提示する手法 European Union regulations on algorithmic decision-making and a &amp;ldquo;right to explanation&amp;rdquo;: ヨーロッパにおいて判断根拠を提示しする必要があると示している論文 【記事更新】私のブックマーク「機械学習における解釈性（Interpretability in Machine Learning）」 【記事更新】私のブックマーク「機械学習における解釈性（Interpretability in Machine Learning）」 2018.12.18 Slide Share 機械学習モデルの判断根拠の説明 keras で Score-CAM 実装．Grad-CAM との比較 Score-CAM の実装と説明が記載されています。</description></item><item><title>arXiv Times</title><link>https://iimuz.github.io/scrapbook/study/arxiv_times/</link><pubDate>Fri, 03 Jan 2020 20:14:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/arxiv_times/</guid><description> arXivTimes</description></item><item><title>Computer Vision</title><link>https://iimuz.github.io/scrapbook/study/computer_vision/</link><pubDate>Fri, 03 Jan 2020 20:09:39 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/computer_vision/</guid><description> Wikipedia: コンピュータビジョン</description></item><item><title>SmoothGrad: removing noise by adding noise</title><link>https://iimuz.github.io/scrapbook/study/smooth_grad_removing_noise_by_adding_noise/</link><pubDate>Fri, 03 Jan 2020 01:37:41 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/smooth_grad_removing_noise_by_adding_noise/</guid><description>Deep Learningにおける説明性のための手法。
arXiv SMOOTH GRAD 投稿日: 2017.6.12 著者: Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, Martin Wattenberg</description></item><item><title>Word2Vec</title><link>https://iimuz.github.io/scrapbook/study/word2vec/</link><pubDate>Thu, 02 Jan 2020 14:17:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/word2vec/</guid><description> Wikipedia: Word2vec</description></item><item><title>MobileNetV2: Inverted Residuals and Linear Bottlenecks</title><link>https://iimuz.github.io/scrapbook/study/mobilenetv2/</link><pubDate>Mon, 30 Dec 2019 12:14:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/mobilenetv2/</guid><description> 2018.4.3: MobileNetV2: The Next Generation of On-Device Computer Vision Networks</description></item><item><title>論文</title><link>https://iimuz.github.io/scrapbook/study/paper/</link><pubDate>Mon, 30 Dec 2019 11:48:40 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/paper/</guid><description> Wikipeida: 論文</description></item><item><title>Deep Learning</title><link>https://iimuz.github.io/scrapbook/study/deep_learning/</link><pubDate>Mon, 30 Dec 2019 10:41:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_learning/</guid><description>Wikipedia: Deep Learning 参考資料 Deep Learning State of the Art (2020) | MIT Deep Learning Series MIT の教授による機械学習関連の最新動向。 2019.12.25 実務で使えるニューラルネットワークの最適化手法 SGD, Adam, AdamW, AdaBound, RAdam の比較が載っています。 RAdam が一番安定性、精度、収束速度のバランスが優れているということのようです。 小さいデータにもとづいてディープラーニングを使う方法 2019.12.12 AINOW 小さいデータにもとづいてディープラーニングを使う方法 小さい学習データでディープラーニングモデルを構築する方法としえ端的には下記のようにまとめられています。
データ収集: 可能な限りデータを収集する fine tuning: 解決すべき問題が属するドメインに関する大規模なモデルを流用する data augumentation: 既存の学習データを加工して、学習データを増やす 損失関数の変更: 損失関数にコサイン類似度を利用するとパフォーマンスが向上することがある モデルの分解: ディープラーニングモデルを多数のニューラルネットワークに分解すると、学習データも小規模化できる autoencoder: オートエンコーダを利用して重みを最適化する 事前知識の組み込み: 学習データが関連するドメイン知識を学習プロセスに組み込むと、学習データを小規模化できる 本記事で紹介されている文献です。
ファインチューニングに関する内容 PyTorch FINETUNING TORCHVISION MODELS 2016.</description></item><item><title>arXiv.org</title><link>https://iimuz.github.io/scrapbook/study/arxiv/</link><pubDate>Mon, 30 Dec 2019 10:34:30 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/arxiv/</guid><description> arXiv.org</description></item><item><title>Machine Learning</title><link>https://iimuz.github.io/scrapbook/study/machine_learning/</link><pubDate>Mon, 30 Dec 2019 10:34:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/machine_learning/</guid><description>Wikipedia: Machine Learning 参考資料 Browse State-of-the-Art タスクごとに、色々な SotA の手法が掲載されています。 2017.5.18 J.P.Morgan Big Data and AI Strategies 金融サービスに関するレポート。280 ページくらいあり。 2019 令和元年 Google 機械学習技術総決算 Google が開発したサービスやツールに関する 2019 年の総まとめです。 どんな技術が公開されたかを一覧することができます。 2020.1.9 towards data science Why we’re writing machine learning infrastructure in Go, not Python 機械学習の推論部分に関しては python を利用するが、それ以外のシステム的な部分は Go のほうがいいという主張です。 システム部分を構築する場合は python は、あまり向かない気がします。 2020.1.20 ill-identified diary 計量経済学と機械学習の関係 –AI はさだめ, さだめは反事実 (転送用) 計量経済学と機械学習の関係性に関して述べられており、共通部分などが掲載されています。 Choosing the right estimator Scikit-Learn: Choosing the right estimator Scikit-Learn が提供するアルゴリズムチートシートです。</description></item><item><title>TensorFlow</title><link>https://iimuz.github.io/scrapbook/study/tensorflow/</link><pubDate>Mon, 30 Dec 2019 03:20:39 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/tensorflow/</guid><description> TensorFlow.org Tensorflow Hub Tensorflow Examples Wikipedia: TensorFlow 参考資料 2019.1.15 What’s coming in TensorFlow 2.0</description></item><item><title>Network Architecture</title><link>https://iimuz.github.io/scrapbook/study/network-architecture/</link><pubDate>Mon, 30 Dec 2019 03:16:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/network-architecture/</guid><description> Deep Learning のネットワークアーキテクチャ</description></item><item><title>Knowledge Distillation</title><link>https://iimuz.github.io/scrapbook/study/knowledge_distillation/</link><pubDate>Mon, 30 Dec 2019 03:11:03 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/knowledge_distillation/</guid><description>蒸留とは? Deep Learning における知識の蒸留
学習時には deep なネットワークで学習することで、精度が高くなる。 しかしながら、推論時には計算資源などの問題から軽量なネットワークを使いたい。 これには、相反する関係となっている。 軽量であるということは精度が犠牲になっている。 そこで、 deep なネットワークでの出力を模擬するような軽量なネットワークを学習する。 これにより、単純に軽量なネットワークを学習するよりも、 よい精度のモデルを学習することができる。</description></item><item><title>Generative adversarial network</title><link>https://iimuz.github.io/scrapbook/study/gan/</link><pubDate>Mon, 30 Dec 2019 02:04:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/gan/</guid><description> Wikipeida: Generative Adversarial Network 参考資料 2019.7.17 AINOW 敵対的生成ネットワークの台頭【前編】 GANの派生形等の有名どころを紹介している。 DCGAN, BigGAN, StyleGAN, StackGAN, CycleGAN, Pix2pix, Age-cGAN This Person does NOT exist GAN によって存在しない人物の顔画像を生成し表示してくれるサイト。 NVIDIA の StyleGAN が利用されているらしい。</description></item><item><title>Dataset</title><link>https://iimuz.github.io/scrapbook/study/dataset/</link><pubDate>Mon, 30 Dec 2019 01:59:45 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/dataset/</guid><description> Wikipedia: Data set Wikipedia: List of datasets for machine learning research Google Dataset Search Google Dataset Search
Google が公開しているデータセットを検索するためのページです。 Google 検索のように検索することができます。
参考資料
2020.1.24 Gigazine Google が機械学習用のデータセットをインターネット上から検索可能な「Dataset Search」を正式公開 参考資料 2020.1.8 HatenaBlog 渋谷駅前で働くデータサイエンティストのブログ Fashion-MNIST: 簡単になり過ぎた MNIST に代わる初心者向け画像認識ベンチマーク データセットとして新規のものが書かれているわけではないです。 これほど情報発信している人でも、関係なければ知らないのだなぁという感想を抱いたのでメモしておきます。 入門用なので十分だと思いますが、たどり着いているのが fashion-MNIST です。</description></item><item><title>feature</title><link>https://iimuz.github.io/scrapbook/study/feature/</link><pubDate>Mon, 30 Dec 2019 01:57:11 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/feature/</guid><description>Wikipedia: Feature(Machine Learning) パターン認識や機械学習の特徴量。 参考資料 KaggleのWinner solutionにもなった「K近傍を用いた特徴量抽出」のPython実装 2018.6.23 KaggleのWinner solutionにもなった「K近傍を用いた特徴量抽出」のPython実装 knn を利用した特徴量生成方法に関する説明です。 python での実装例があります。
やっていることは、 knn で任意のクラスの中で最近棒となる点との距離を特徴量として追加することです。</description></item><item><title>Point Cloud</title><link>https://iimuz.github.io/scrapbook/study/point_cloud/</link><pubDate>Mon, 30 Dec 2019 01:54:54 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/point_cloud/</guid><description>-Wikipedia: Point Cloud</description></item><item><title>Machine Vision</title><link>https://iimuz.github.io/scrapbook/study/machine_vision/</link><pubDate>Mon, 30 Dec 2019 01:47:19 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/machine_vision/</guid><description> Wikipeidia: Machine Vision</description></item><item><title>Reinforcement Learning</title><link>https://iimuz.github.io/scrapbook/study/reinforcement_learning/</link><pubDate>Mon, 30 Dec 2019 01:43:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/reinforcement_learning/</guid><description> Wikipedia: Reinforcement Learning</description></item><item><title>Counterfactual Visual Explanations</title><link>https://iimuz.github.io/scrapbook/study/counterfactual_visual_explanations/</link><pubDate>Tue, 15 Oct 2019 21:03:44 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/counterfactual_visual_explanations/</guid><description>arXiv arXiv Times 画像の分類根拠を可視化する手法。 野鳥が例に出されているが、何でも使えそう。 これ以外にも、可視化手法は調べると良いかもしれない。</description></item><item><title>Benchmarking Model-Based Reinforcement Learning</title><link>https://iimuz.github.io/scrapbook/study/benchmarking-model-based-reinforcement-learning/</link><pubDate>Tue, 13 Aug 2019 23:34:25 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/benchmarking-model-based-reinforcement-learning/</guid><description>論文情報 title: Benchmarking Model-Based Reinforcement Learning author: Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba year: 2019/7/3 arxiv issue vanity Google Translate: vanity 比較データなど どんなものか 強化学習の 14 手法に関して、同一のデータセットを利用して性能を比較した。
Figure 3: The relative performance with different planning horizon. 先行研究と比べてどこがすごいのか 従来は、各手法で優位性を述べており、同一のデータセットで比較し、 相対的な性能の優位性を適切に述べているものがなかった。
技術や手法のキモどこか OpenAI Gym という強化学習用のデータセットを利用して、 15 の環境で評価している。 また、ノイズを加えた状態も評価している。</description></item><item><title>Cold Case: the Lost MNIST Digits</title><link>https://iimuz.github.io/scrapbook/study/cold-case-the-lost-mnist-digits/</link><pubDate>Thu, 01 Aug 2019 11:54:22 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/cold-case-the-lost-mnist-digits/</guid><description>軽く読んだが再構成はして、評価しなおしているがデータセットが公開されているわけではないらしい。
arXiv issue vanity</description></item><item><title>ブラックホール撮影にも使えるスパースモデリングとは</title><link>https://iimuz.github.io/scrapbook/study/sparse_modeling_blackhole/</link><pubDate>Sun, 21 Apr 2019 11:17:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/sparse_modeling_blackhole/</guid><description>少々スパースモデリングがブラックホール撮影に使われているということで面白かったのでメモです。
ブラックホール撮影にも使える「スパースモデリング」とは？【機械学習】 要は、取得できる画像が低解像度なので高解像度にするのにスパースモデリングを使ったよということを起点としています。 この記事自体は実際に使われた手法に関して説明しているわけではなく、 スパースモデリングの基本的なことを説明しています。 Lasso を利用して下記の式を最適化しているようです。
$$ L(\bold{I}) = \parallel \bold{V} - \bold{F}\bold{I} \parallel_2^2 + \parallel \bold{I} \parallel_1 $$
$ \bold{I} $: ブラックホールの高解像度画像 $ \bold{V} $: 撮影した画像 (フーリエ変換済み) $ \bold{F} $: フーリエ変換 かなり以前から知られている範囲での説明に終わっているため、 実際には、これ以上の方法が用いられているようです。
復元結果 (引用 1) (引用 1) 本間ら「スパースモデリング天文学 ― ブラックホール撮像から時間変動減少まで」， 科学研究費補助金新学術領域研究「スパースモデリングの深化と高次元データ駆動科学の創成」 最終成果報告会 (2017/12/18-20)</description></item><item><title>三次元点群を扱うニューラルネットワークのサーベイ (ver.2)</title><link>https://iimuz.github.io/scrapbook/study/point_cloud_deep_servey/</link><pubDate>Sun, 21 Apr 2019 10:59:20 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/point_cloud_deep_servey/</guid><description>資料 資料リンク 著者などの情報 橋本、鏡研究室 どちらの方も 3 次元点群を利用する系統を研究されているようです。 産業用ロボットやプロジェクションマッピングなどを研究されています。 千葉 直也 2019/4 月現在は D3 の方</description></item><item><title>Object Recognition in 3D Scenes with Occlusions and Clustter by Hough Voting</title><link>https://iimuz.github.io/scrapbook/study/objectrecognitionin3dsceneswithocclusionsandclustterbyhoughvoting/</link><pubDate>Tue, 13 Nov 2018 13:28:54 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/objectrecognitionin3dsceneswithocclusionsandclustterbyhoughvoting/</guid><description>Abstract 2010 Fourth Pacific-Rim Symposium on Image and Video Technology
Federico Tombari, Luigi Di Stefano
In this work we propose a novel Hough voting approach for the detection of free-form shapes in a 3D space, to be used for object recognition tasks in 3D scenes with a significant degree of occlusion and clutter. The proposed method relies on matching 3D features to accumulate evidence of the presence of the objects being sought in a 3D Hough space.</description></item><item><title>Unique Signatures of Histograms for Local Surface Description (SHOT)</title><link>https://iimuz.github.io/scrapbook/study/uniquesignaturesofhistogramsforlocalsurfacedescription_shot/</link><pubDate>Tue, 06 Nov 2018 18:18:47 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/uniquesignaturesofhistogramsforlocalsurfacedescription_shot/</guid><description>Unique Signatures of Histograms for Local Surface Description (SHOT) Abstract Federico Tombari, Samuele Salti, and Luigi Di Stefano
ECCV 2010
Abstract
This paper deals with local 3D descriptors for surface matching. First, we categorize existing methods into two classes: Signatures and Histograms. Then, by discussion and experiments alike, we point out the key issues of uniqueness and repeatability of the local reference frame. Based on these observations, we formulate a novel comprehensive proposal for surface representation, which encompasses a new unique and repeatable local reference frame as well as a new 3D descriptor.</description></item><item><title>Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</title><link>https://iimuz.github.io/scrapbook/study/unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery/</link><pubDate>Sun, 02 Sep 2018 11:27:34 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery/</guid><description>arXivTimes で見つけた論文です。 GAN を利用した異常検知技術を調べていた時に見つけました。
AnoGAN というネットワークを提案しています。 医療用画像を対象に、正常画像のみから、 GAN で学習していくようです。 このとき、 Generator は潜在空間から画像生成されると仮定します。 面白いのは、 GAN だけで完結しており、 検査時の入力画像は Auto Encoder のように再構成せず、 Generator から近い画像を生成するようにしているようです。 GAN は、潜在空間から画像は生成できるが、画像から潜在空間は変換できないはずです。 そこで、潜在空間は連続的に変化することから最初はランダムランプリングで潜在空間から画像を生成し、 近い画像へ潜在空間上を移動させるという処理を行うようです。 そのため、検査時に時間がかかる可能性はあるように思います。 また、残差誤差だけでなく、 Descriminator 側の出力も考慮して異常スコアを算出しているようです。
実装例: LeeDoYup/AnoGAN 日本語で解説してくれている記事です。 GAN による医療画像の異常検知 【論文読み】GAN を利用した異常検知まとめ その他に、論文を読んでいるときに調べながら見つけた論文 Unsupervised Adversarial Anomaly Detection using One-Class Support Vector Machines One-Class Adversarial Nets for Fraud Detection</description></item><item><title>Large Scale Learning of General Visual Representations for Transfer</title><link>https://iimuz.github.io/scrapbook/study/larget_scale_learning_of_general_visual_representation_for_transfer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/larget_scale_learning_of_general_visual_representation_for_transfer/</guid><description> arXiv 著者: exander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby 日時: 2019.12.24 パラメータ数が非常に巨大な(10 億程度)のモデルです。 基本的には、事前学習によって巨大なモデルを学習し、各タスクで Fine-Tuning することでタスクを解くことを前提としています。
参考資料 2020.1.15 Qiita パラメータ数 10 億！最新の巨大画像認識モデル「BiT」爆誕 &amp;amp; 解説</description></item><item><title>Noise2Noise: Learning Image Restoration without Clean Data</title><link>https://iimuz.github.io/scrapbook/study/noise2noise_learning_image_restoration_without_clean_data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/noise2noise_learning_image_restoration_without_clean_data/</guid><description> arXiv 著者: Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila 投稿日: 2018.3.12 ノイズ画像からノイズ除去画像を生成する場合、ノイズなし画像を常に用意し続けることは難しい。 そこで、ノイズ画像を入力として、ノイズ画像を復元するというタスクを解く。 不思議なことをしているようだが、ノイズ除去した画像を復元するネットワークを学習することができる。
参考資料 2019.12.7 Qiita Noise2Noise 解説</description></item></channel></rss>