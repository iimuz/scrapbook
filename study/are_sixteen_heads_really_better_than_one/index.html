<!doctype html><html lang=en><head><title>Are sixteen heads really better than one? | しさく</title><meta charset=utf-8><meta name=generator content="Hugo 0.63.1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta http-equiv=x-ua-compatible content="IE=edge"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Are Sixteen Heads Really Better than One?"><meta name=description content="arXiv 著者: Paul Michel, Omer Levy, Graham Neubig 投稿日: 2019.5.25  Transformer 系などでは、 Multi-headed Attention を利用します。 そこで、 Multi-headed Attention がどこまで有効なのかを検証した論文で …"><meta property="og:description" content="arXiv 著者: Paul Michel, Omer Levy, Graham Neubig 投稿日: 2019.5.25  Transformer 系などでは、 Multi-headed Attention を利用します。 そこで、 Multi-headed Attention がどこまで有効なのかを検証した論文で …"><meta property="og:url" content="https://iimuz.github.io/scrapbook/study/are_sixteen_heads_really_better_than_one/"><meta property="og:image" content="images/%!s()"><meta name=twitter:card content="summary_large_image"><meta name=twitter:creator content><meta name=twitter:title content="Are Sixteen Heads Really Better than One?"><meta property="twitter:description" content="arXiv 著者: Paul Michel, Omer Levy, Graham Neubig 投稿日: 2019.5.25  Transformer 系などでは、 Multi-headed Attention を利用します。 そこで、 Multi-headed Attention がどこまで有効なのかを検証した論文で …"><meta name=twitter:image content="images/%!s()"><link rel=apple-touch-icon sizes=180x180 href=https://iimuz.github.io/scrapbook/images/icons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://iimuz.github.io/scrapbook/images/icons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://iimuz.github.io/scrapbook/images/icons/favicon-16x16.png><link rel=manifest href=https://iimuz.github.io/scrapbook/images/icons/site.webmanifest><link rel=canonical href=https://iimuz.github.io/scrapbook/study/are_sixteen_heads_really_better_than_one/><link rel=stylesheet href=https://iimuz.github.io/scrapbook/css/styles.a1e1a9f0e58a84cfca79c98c11b9e900971b61f840c5fbe7f3ddad8ebcd9798517a0ea30089562091e9105239ce941ccc4953d8c3a0c08a2f5082ace791f0186.css integrity="sha512-oeGp8OWKhM/KecmMEbnpAJcbYfhAxfvn892tjrzZeYUXoOowCJViCR6RBSOc6UHMxJU9jDoMCKL1CCrOeR8Bhg=="><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.2/css/all.min.css></head><body><div class=nav-drop><div class=nav-body><a href=https://iimuz.github.io/scrapbook/search/ class=nav_item>search</a>
<a href=https://iimuz.github.io/scrapbook/tags/ class=nav_item>tags</a>
<a href=https://iimuz.github.io/scrapbook/categories/ class=nav_item>categories</a>
<a href=https://github.com/iimuz/til/ class=nav_item>til</a><div class=nav-close></div></div></div><header class=nav><nav class=nav-menu><a href=https://iimuz.github.io/scrapbook/ class="nav-brand nav_item">しさく</a><div class=nav_bar-wrap><div class=nav_bar></div></div></nav></header><main><section class=post_header style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)><h1 class=post_title>Are Sixteen Heads Really Better than One?</h1></section><div class=post><article class=post_content><div><i class="far fa-calendar-check"></i>2020-01-17 10:18:37 +0900 +0900
<i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div><ul><li><a href=https://arxiv.org/abs/1905.10650>arXiv</a></li><li>著者: Paul Michel, Omer Levy, Graham Neubig</li><li>投稿日: 2019.5.25</li></ul><p>Transformer 系などでは、 Multi-headed Attention を利用します。
そこで、 Multi-headed Attention がどこまで有効なのかを検証した論文です。
結論としては、全てのケースで Multi-headed Attention が有効なわけではなく、
統計的に優位な劣化なしに Multi-headed Attention の数を減らすことができます。
そして、 Self-Attention よりも Encoder-Decoder Layer の Attention の方が有効のようです。
加えて、各 Head の有効性は、学習の初期に決定されるようです。</p><div class=post_extra><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div></article><aside><h3>Referenced from</h3><ul class="posts aside"></ul><h3>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h3><ul class="posts aside"><li class=post_item><a class=post_card href=/scrapbook/study/bert/ title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/bert/>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/ title="What Does BERT Look At? An Analysis of BERT's Attention" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/>What Does BERT Look At? An Analysis of BERT's Attention</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/xlnet/ title="XLNet: Generalized Autoregressive Pretraining for Language Understanding" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/xlnet/>XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/deep_learning/ title="Deep Learning" style=background-image:url(https://cdn.pixabay.com/photo/2018/11/15/00/56/neural-network-3816319_960_720.png)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/deep_learning/>Deep Learning</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/ title="What Does BERT Look At? An Analysis of BERT's Attention" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/>What Does BERT Look At? An Analysis of BERT's Attention</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/xlnet/ title="XLNet: Generalized Autoregressive Pretraining for Language Understanding" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/xlnet/>XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/natural_language_processing/ title="Natural Language Processing" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/natural_language_processing/>Natural Language Processing</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/ title="What Does BERT Look At? An Analysis of BERT's Attention" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/>What Does BERT Look At? An Analysis of BERT's Attention</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/xlnet/ title="XLNet: Generalized Autoregressive Pretraining for Language Understanding" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/xlnet/>XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/transformer/ title=Transformer style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/transformer/>Transformer</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li></ul><h3>Transformer</h3><ul class="posts aside"><li class=post_item><a class=post_card href=/scrapbook/study/transformer/ title=Transformer style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/transformer/>Transformer</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/enhancing_the_locality_and_breaking_the_memory_bottleneck_of_transofrmer_on_time_series_forecasting/ title="Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/enhancing_the_locality_and_breaking_the_memory_bottleneck_of_transofrmer_on_time_series_forecasting/>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/bert/ title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/bert/>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/attention/ title="Attention Is All You Need" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/attention/>Attention Is All You Need</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/enhancing_the_locality_and_breaking_the_memory_bottleneck_of_transofrmer_on_time_series_forecasting/ title="Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/enhancing_the_locality_and_breaking_the_memory_bottleneck_of_transofrmer_on_time_series_forecasting/>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/bert/ title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/bert/>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/deep_learning/ title="Deep Learning" style=background-image:url(https://cdn.pixabay.com/photo/2018/11/15/00/56/neural-network-3816319_960_720.png)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/deep_learning/>Deep Learning</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li></ul><h3>More from しさく</h3><ul class="posts aside"><li class=post_item><a class=post_card href=/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/ title="What Does BERT Look At? An Analysis of BERT's Attention" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/>What Does BERT Look At? An Analysis of BERT's Attention</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/self_attention_with_functional_time_representation_learning/ title="Self-attention with Functional Time Representation Learning" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/self_attention_with_functional_time_representation_learning/>Self-attention with Functional Time Representation Learning</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/xlnet/ title="XLNet: Generalized Autoregressive Pretraining for Language Understanding" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/xlnet/>XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/temporal_pattern_attention_for_multivariate_time_series_forecasting/ title="Temporal Pattern Attention for Multivariate Time Series Forecasting" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/temporal_pattern_attention_for_multivariate_time_series_forecasting/>Temporal Pattern Attention for Multivariate Time Series Forecasting</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/rnn/ title="Recurrent Neural Network (RNN)" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/rnn/>Recurrent Neural Network (RNN)</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-02-19 23:34:31 +0000 UTC</div></div></div></li></ul></aside></div><script src=https://iimuz.github.io/scrapbook/js/autosize.min.js></script><script src=https://iimuz.github.io/scrapbook/js/timeago.js></script></main><svg width="0" height="0" class="hidden"><symbol viewBox="0 0 699.428 699.428" xmlns="http://www.w3.org/2000/svg" id="copy"><path d="M502.714.0H240.428C194.178.0 153 42.425 153 87.429l-25.267.59c-46.228.0-84.019 41.834-84.019 86.838V612c0 45.004 41.179 87.428 87.429 87.428H459c46.249.0 87.428-42.424 87.428-87.428h21.857c46.25.0 87.429-42.424 87.429-87.428v-349.19zM459 655.715H131.143c-22.95.0-43.714-21.441-43.714-43.715V174.857c0-22.272 18.688-42.993 41.638-42.993l23.933-.721v393.429C153 569.576 194.178 612 240.428 612h262.286c0 22.273-20.765 43.715-43.714 43.715zm153-131.143c0 22.271-20.765 43.713-43.715 43.713H240.428c-22.95.0-43.714-21.441-43.714-43.713V87.429c0-22.272 20.764-43.714 43.714-43.714H459c-.351 50.337.0 87.975.0 87.975.0 45.419 40.872 86.882 87.428 86.882H612zm-65.572-349.715c-23.277.0-43.714-42.293-43.714-64.981V44.348L612 174.857zm-43.714 131.537H306c-12.065.0-21.857 9.77-21.857 21.835s9.792 21.835 21.857 21.835h196.714c12.065.0 21.857-9.771 21.857-21.835.0-12.065-9.792-21.835-21.857-21.835zm0 109.176H306c-12.065.0-21.857 9.77-21.857 21.834.0 12.066 9.792 21.836 21.857 21.836h196.714c12.065.0 21.857-9.77 21.857-21.836.0-12.064-9.792-21.834-21.857-21.834z"/></symbol><symbol viewBox="0 0 60.015 60.015" xmlns="http://www.w3.org/2000/svg" id="reply"><path d="M42.007.0h-24c-9.925.0-18 8.075-18 18v14c0 9.59 7.538 17.452 17 17.973v8.344a1.694 1.694.0 001.699 1.698c.44.0.873-.173 1.198-.498l1.876-1.876C26.708 52.713 33.259 50 40.227 50h1.78c9.925.0 18-8.075 18-18V18c0-9.925-8.075-18-18-18zm16 32c0 8.822-7.178 16-16 16h-1.78c-7.502.0-14.556 2.921-19.86 8.226l-1.359 1.359V44a1 1 0 10-2 0v3.949c-8.356-.52-15-7.465-15-15.949V18c0-8.822 7.178-16 16-16h24c8.822.0 16 7.178 16 16v14z"/></symbol></svg><footer class=footer><div class="footer_inner wrap pale"><p>&copy; <span class=year></span>しさく</p><p>Designed by <a href="<no value>" title="Linkedin Profile"><no value></a></p></div></footer><script src=https://iimuz.github.io/scrapbook/js/index.js></script></body></html>