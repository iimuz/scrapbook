<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Studies on しさく</title><link>https://iimuz.github.io/scrapbook/study/</link><description>Recent content in Studies on しさく</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 05 Jan 2020 19:56:38 +0900</lastBuildDate><atom:link href="https://iimuz.github.io/scrapbook/study/index.xml" rel="self" type="application/rss+xml"/><item><title>Sanity Checks for Saliency Maps</title><link>https://iimuz.github.io/scrapbook/study/sanity_checks_for_saliency_maps/</link><pubDate>Sun, 05 Jan 2020 19:56:38 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/sanity_checks_for_saliency_maps/</guid><description> arXiv 著者: Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim 投稿日: 2018.10.8 可視化手法を比較し可視化手法が本当にモデルの状態を説明しているのかを調べた論文です。
参考資料 2019.11.24: Hatena Blog: データ分析関連のまとめ: Sanity Checks for Saliency Maps</description></item><item><title>Variational Autoencoder</title><link>https://iimuz.github.io/scrapbook/study/variational_autoencoder/</link><pubDate>Sun, 05 Jan 2020 19:45:13 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/variational_autoencoder/</guid><description> Wikipedia: Autoencoder#Variational autoencoder(VAE) 参考資料 2017.7.19: Qiita: Variational Autoencoder徹底解説</description></item><item><title>Autoencoder</title><link>https://iimuz.github.io/scrapbook/study/autoencoder/</link><pubDate>Sun, 05 Jan 2020 19:40:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/autoencoder/</guid><description> Wikipedia: オートエンコーダ 入力と出力が同じになるように学習を行う。 Deep Learning で利用する。 亜種として下記のようなネットワークもある。
Variational Autoencoder 参考文献 2016.10.9: DeepAge: オートエンコーダ：抽象的な特徴を自己学習するディープラーニングの人気者</description></item><item><title>ICLR2020</title><link>https://iimuz.github.io/scrapbook/study/iclr2020/</link><pubDate>Sun, 05 Jan 2020 19:37:51 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iclr2020/</guid><description> ICLR2020 会期: 2020.4.26 - 30</description></item><item><title>International Conference on Learning Representations (ICLR)</title><link>https://iimuz.github.io/scrapbook/study/iclr/</link><pubDate>Sun, 05 Jan 2020 19:36:29 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iclr/</guid><description> Wikipedia: International Conference on Learning Representations</description></item><item><title>Iterative energy-based projection on a normal data manifold for anomaly localization</title><link>https://iimuz.github.io/scrapbook/study/iterative_energy_based_projection_on_a_normal_data_manifold_for_anomaly_localization/</link><pubDate>Sun, 05 Jan 2020 19:32:32 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iterative_energy_based_projection_on_a_normal_data_manifold_for_anomaly_localization/</guid><description> OpenReview.net 著者: David Dehaene, Oriel Frigo, Sébastien Combrexelle, Pierre Eline 投稿日: 2019.9.26 参考資料 Qiita: ICLR2020の異常検知論文を実装してみた Slide Share: ICLR2020の異常検知論文の紹介</description></item><item><title>Pesudo-Label</title><link>https://iimuz.github.io/scrapbook/study/pesudo_label/</link><pubDate>Sun, 05 Jan 2020 16:33:26 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/pesudo_label/</guid><description>ラベル付けされたデータセットと、ラベル付けされていないデータセットが与えられる。 この時に、ラベル付けされたデータセットだけで学習したモデルで、 ラベル付けされていないデータセットのラベルを予測する。 予測したラベルを利用して、全データセットを使って再度モデルを学習することにより、 精度が向上する場合がある。
結局のところ、 MAP 推定などにおける正則化項の役割を果たすことにより精度が向上するようです(1)。
2019..11.4: Hatena Blog: なぜ疑似ラベルが効果的か調べてみた&amp;#8617;</description></item><item><title>SinGAN: Learning a Generative Model from a Single Natural Image</title><link>https://iimuz.github.io/scrapbook/study/singan_learning_a_generative_model_from_a_single_natural_image/</link><pubDate>Sun, 05 Jan 2020 16:05:01 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/singan_learning_a_generative_model_from_a_single_natural_image/</guid><description> arXiv 著者: Tamar Rott Shaham, Tali Dekel, Tomer Michaeli 投稿日: 2019.5.2 公式ページ: SinGAN: Learning a Generative Model from a Single Natural Image 公式実装: GitHub: tamarott/SinGAN 参考情報 2019.11.13: Qiita: 【論文解説】SinGAN: Learning a Generative Model from a Single Natural Image 単一の画像を使って学習を行うことのできる方法を提案しています。 階層的な GAN を用いており、 1 つ分だけ低階層の Generator が生成した画像と、 ノイズを入力として上位階層を学習します。
ネットワークアーキテクチャ Generator のアーキテクチャ</description></item><item><title>Matrix Profile</title><link>https://iimuz.github.io/scrapbook/study/matrix_profile/</link><pubDate>Sun, 05 Jan 2020 15:47:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/matrix_profile/</guid><description>2 つの時系列において、部分時系列同士の近さを計算した行列を指します。
The UCR Matrix Profile Page ライブラリ Matrix Profile Foundation GitHub: target/matrixprofile-ts: python ライブラリ 参考情報 2019.11.15: Qiita: MatrixProfileによるECGデータの異常検知</description></item><item><title>A Closer Look at Spatiotemporal Convolutions for Action Recognition</title><link>https://iimuz.github.io/scrapbook/study/a_closer_look_at_spatiotemporal_convolutions_for_action_recognition/</link><pubDate>Sun, 05 Jan 2020 15:17:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/a_closer_look_at_spatiotemporal_convolutions_for_action_recognition/</guid><description> arXiv 投稿日: 2017.11.30 著者: Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri CVPR 2018 で採択されている。</description></item><item><title>Time Series</title><link>https://iimuz.github.io/scrapbook/study/time_series/</link><pubDate>Sun, 05 Jan 2020 15:12:41 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/time_series/</guid><description>Wikipedia: 時系列 参考資料 Time series forecasting: Multi-Step model Tensorflow Tutorials: Time series forecasting: Multi-Step model Everything you can do with a time series Kaggle Kernel: Everything you can do with a time series 時系列データ(特に、株価チャート)を中心としてチュートリアルを作成している。 ろうそく足チャートとかの説明もある。 ARMA モデルに始まり、SARIMA などの紹介もしている。
Time series Basics : Exploring traditional TS Kaggle Kernel: Time series Basics : Exploring traditional TS 時系列単変量データの解析が中心のようです。ARMA モデルや ARIMA モデル、SARIMA に関して記載しています。 時系列多変量データに関しても商品データ(ショップ毎、アイテムごと)を利用して説明しています。 ただし、予測自体は単変量とほぼ変わらないようです。 欠損値の取り扱いなどに関しても記載されていないです。</description></item><item><title>International Conference on Computer Vision</title><link>https://iimuz.github.io/scrapbook/study/iccv/</link><pubDate>Sun, 05 Jan 2020 15:10:50 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iccv/</guid><description> Wikipedia: International Conference on Computer Vision</description></item><item><title>Learning Spatiotemporal Features with 3D Convolutional Networks</title><link>https://iimuz.github.io/scrapbook/study/learning_spatiotemporal_features_with_3d_convolutional_networks/</link><pubDate>Sun, 05 Jan 2020 15:01:00 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/learning_spatiotemporal_features_with_3d_convolutional_networks/</guid><description>arXiv 投稿日: 2014.12.2 著者: Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri 時空間3次元畳み込みによる 3D CNN (C3D) を提案している。</description></item><item><title>Metric Learning</title><link>https://iimuz.github.io/scrapbook/study/metric_learning/</link><pubDate>Sun, 05 Jan 2020 12:47:57 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/metric_learning/</guid><description> Wikipedia: Similarity Learning#Metric Learning</description></item><item><title>Joint Discriminative and Generative Learning for Person Re-identification</title><link>https://iimuz.github.io/scrapbook/study/joint_descriminative_and_generative_leanring_for_person_reidentification/</link><pubDate>Sun, 05 Jan 2020 12:41:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/joint_descriminative_and_generative_leanring_for_person_reidentification/</guid><description> arXiv 投稿日: 2019.4.15 著者: Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, Jan Kautz NVIDIA から CVPR2019 の Oral セッションで発表された論文です。 GAN と Metric Learning を組み合わせて人物の識別精度を向上しています。
参考文献 Person Re-identification 論文解説 Joint Discriminative and Generative Learning for Perosn Re-identification を読む Qiita: Person Re-identification 論文解説 Joint Discriminative and Generative Learning for Perosn Re-identification を読む 投稿日: 2019.11.6</description></item><item><title>Tiny Video Networks</title><link>https://iimuz.github.io/scrapbook/study/tiny_video_networks/</link><pubDate>Sun, 05 Jan 2020 12:23:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/tiny_video_networks/</guid><description> arXiv 投稿日: 2019.10.15 著者: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo 映像認識において、実行速度を向上するためにモデルを軽量化する。 モデルの探索において、実行時間やパラメータの制約をかけ探索を行う。
参考文献 Tiny Video Networks [ML論文読] 精度を保ったまま映像認識を軽量化する Qiita: Tiny Video Networks [ML論文読] 精度を保ったまま映像認識を軽量化する 投稿日: 2019.10.30</description></item><item><title>Natural Language Processing</title><link>https://iimuz.github.io/scrapbook/study/natural_language_processing/</link><pubDate>Sun, 05 Jan 2020 12:18:14 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/natural_language_processing/</guid><description> Wikipedia: 自然言語処理</description></item><item><title>Anomaly Detection</title><link>https://iimuz.github.io/scrapbook/study/anomaly_detection/</link><pubDate>Sun, 05 Jan 2020 12:16:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/anomaly_detection/</guid><description> Wikipedia: 異常検知</description></item><item><title>CVPR2018</title><link>https://iimuz.github.io/scrapbook/study/cvpr2018/</link><pubDate>Sun, 05 Jan 2020 06:34:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2018/</guid><description> CVPR2018 会期: 2018.6.19 - 21</description></item><item><title>ICCV2015</title><link>https://iimuz.github.io/scrapbook/study/iccv2015/</link><pubDate>Sun, 05 Jan 2020 06:09:19 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iccv2015/</guid><description> ICCV2015</description></item><item><title>LightGBM</title><link>https://iimuz.github.io/scrapbook/study/lightgbm/</link><pubDate>Sun, 05 Jan 2020 04:21:26 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/lightgbm/</guid><description> GitHub: microsoft/LightGBM</description></item><item><title>Hist Gradient Boosting Tree</title><link>https://iimuz.github.io/scrapbook/study/gradient_boosting_decision_tree/</link><pubDate>Sun, 05 Jan 2020 04:18:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/gradient_boosting_decision_tree/</guid><description> Wikipedia: Gradinet Boosting#Gradinet Tree Boosting</description></item><item><title>Scikit Learn</title><link>https://iimuz.github.io/scrapbook/study/scikit-learn/</link><pubDate>Sun, 05 Jan 2020 04:16:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/scikit-learn/</guid><description> Wikipedia: scikit-learn</description></item><item><title>Hist Gradient Boosting Tree</title><link>https://iimuz.github.io/scrapbook/study/hist_gradient_boosting_tree/</link><pubDate>Sun, 05 Jan 2020 04:10:55 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/hist_gradient_boosting_tree/</guid><description>sklearn.ensemble.HistGradientBoostingClassifier Scikit-Learn に実装されている GBDT の手法です。
参考文献 最新機械学習モデル HistGradientBoostingTree の性能調査(LightGBM と比較検証) Qiita: 最新機械学習モデル HistGradientBoostingTree の性能調査(LightGBM と比較検証) 投稿日: 2019.5.27 使い方から LightGBM との比較まで記載されています。</description></item><item><title>CVPR 2020</title><link>https://iimuz.github.io/scrapbook/study/cvpr2020/</link><pubDate>Sun, 05 Jan 2020 04:01:47 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2020/</guid><description> CVPR 2020 会期: 2019.6.14 - 19</description></item><item><title>CVPR 2019</title><link>https://iimuz.github.io/scrapbook/study/cvpr2019/</link><pubDate>Sun, 05 Jan 2020 04:00:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2019/</guid><description> CVPR 2019 会期: 2019.6.16 - 20 参考資料 CVPR 2019 速報 Slide Share: CVPR 2019 速報 投稿日: 2019.6.21 CVPR 2019 網羅的サーベイ報告会の抜粋 Qiita: CVPR 2019 網羅的サーベイ報告会の抜粋 投稿日: 2019.11.8 簡単なメモ
CVPR 2019 速報の簡易まとめです。 画像識別の手法において現在の主流は Residual Net です。 GAN は研究が活発な分野の一つである。主要な手法の一覧が掲載されています。 CVPR のトレンドを創っている論文で時系列を扱っていそうな文献として下記が記載されています。 Learning Spatiotemporal Features with 3D Convolutional Networks A Closer Look at Spatiotemporal Convolutions for Action Recognition</description></item><item><title>Conference</title><link>https://iimuz.github.io/scrapbook/study/conference/</link><pubDate>Sun, 05 Jan 2020 03:58:00 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/conference/</guid><description> Wikipedia: 研究会</description></item><item><title>Conference on Computer Vision and Pattern Recognition (CVPR)</title><link>https://iimuz.github.io/scrapbook/study/crpr/</link><pubDate>Sun, 05 Jan 2020 03:55:33 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/crpr/</guid><description> [Wikipedia: Conference on Computer Vision and Pattern Recognition]</description></item><item><title>Deep Affinity Network for Multiple Object Tracking</title><link>https://iimuz.github.io/scrapbook/study/deep_afinity_network_for_multiple_object_tracking/</link><pubDate>Sun, 05 Jan 2020 03:29:25 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_afinity_network_for_multiple_object_tracking/</guid><description> arXiv 投稿日: 2018.10.28 著者: ShiJie Sun, Naveed Akhtar, HuanSheng Song, Ajmal Mian, Mubarak Shah 単純に物体検出をしてトラッキングするのではなく、トラッキング対象の識別を含めて行う手法です。 この文献では、あるフレームで検出対象が隠れてしまい、トラッキングが途切れても、 それ以降で再び現れた場合に同じ物体であると識別できるようにしています。
参考文献 多人数トラッキング論文解説 Deep Affinity Network for Multiple Object Tracking を読む Qiita: 多人数トラッキング論文解説 Deep Affinity Network for Multiple Object Tracking を読む 投稿日: 2019.11.3</description></item><item><title>Attention Is All You Need</title><link>https://iimuz.github.io/scrapbook/study/attention/</link><pubDate>Fri, 03 Jan 2020 21:59:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/attention/</guid><description> arXiv 著者: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin 投稿日: 2017.6.12</description></item><item><title>European Union regulations on algorithmic decision-making and a "right to explanation"</title><link>https://iimuz.github.io/scrapbook/study/european_union_regulations_on_algorithmic_decision_making_and_a_right_to_explanation/</link><pubDate>Fri, 03 Jan 2020 21:50:05 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/european_union_regulations_on_algorithmic_decision_making_and_a_right_to_explanation/</guid><description> arXiv 著者: Bryce Goodman, Seth Flaxman 投稿日: 2016.6.28</description></item><item><title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title><link>https://iimuz.github.io/scrapbook/study/show_attend_and_tell_neural_image_caption_generation_with_visual_attention/</link><pubDate>Fri, 03 Jan 2020 21:46:38 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/show_attend_and_tell_neural_image_caption_generation_with_visual_attention/</guid><description> arXiv 著者: Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio 投稿日: 2015.2.10</description></item><item><title>Detecting and Explaining Crisis</title><link>https://iimuz.github.io/scrapbook/study/detecting_and_explaining_crisis/</link><pubDate>Fri, 03 Jan 2020 21:43:17 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/detecting_and_explaining_crisis/</guid><description> arXiv 著者: Rohan Kshirsagar, Robert Morris, Sam Bowman 投稿日: 2017.3.26</description></item><item><title>Interpretability</title><link>https://iimuz.github.io/scrapbook/study/interpretability/</link><pubDate>Fri, 03 Jan 2020 21:28:40 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/interpretability/</guid><description>機械学習における解釈性の話。
ディープラーニングの判断根拠を理解する手法 機械学習の解釈性に関して、日本語で色々な文献や考え方がまとめられています。
Qiita: ディープラーニングの判断根拠を理解する手法 投稿日: 2017.9.6 参考文献 SmoothGrad: removing noise by adding noise: Deep Learningにおいて判断根拠を提示する手法 Detecting and Explaining Crisis: SNS において危機的状況を判断しまた判断根拠を提示する手法 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention: Attention を利用して判断根拠を提示する手法 European Union regulations on algorithmic decision-making and a &amp;ldquo;right to explanation&amp;rdquo;: ヨーロッパにおいて判断根拠を提示しする必要があると示している論文 【記事更新】私のブックマーク「機械学習における解釈性（Interpretability in Machine Learning）」 【記事更新】私のブックマーク「機械学習における解釈性（Interpretability in Machine Learning）」 kerasでScore-CAM実装．Grad-CAMとの比較 Score-CAM の実装と説明が記載されています。
Qiita: kerasでScore-CAM実装．Grad-CAMとの比較 投稿日: 2019.11.4</description></item><item><title>arXiv Times</title><link>https://iimuz.github.io/scrapbook/study/arxiv_times/</link><pubDate>Fri, 03 Jan 2020 20:14:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/arxiv_times/</guid><description> arXivTimes</description></item><item><title>Computer Vision</title><link>https://iimuz.github.io/scrapbook/study/computer_vision/</link><pubDate>Fri, 03 Jan 2020 20:09:39 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/computer_vision/</guid><description> Wikipedia: コンピュータビジョン</description></item><item><title>SmoothGrad: removing noise by adding noise</title><link>https://iimuz.github.io/scrapbook/study/smooth_grad_removing_noise_by_adding_noise/</link><pubDate>Fri, 03 Jan 2020 01:37:41 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/smooth_grad_removing_noise_by_adding_noise/</guid><description>Deep Learningにおける説明性のための手法。
arXiv SMOOTH GRAD 投稿日: 2017.6.12 著者: Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, Martin Wattenberg</description></item><item><title>Word2Vec</title><link>https://iimuz.github.io/scrapbook/study/word2vec/</link><pubDate>Thu, 02 Jan 2020 14:17:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/word2vec/</guid><description> Wikipedia: Word2vec</description></item><item><title>MobileNetV2: Inverted Residuals and Linear Bottlenecks</title><link>https://iimuz.github.io/scrapbook/study/mobilenetv2/</link><pubDate>Mon, 30 Dec 2019 12:14:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/mobilenetv2/</guid><description> 2018.4.3: MobileNetV2: The Next Generation of On-Device Computer Vision Networks</description></item><item><title>論文</title><link>https://iimuz.github.io/scrapbook/study/paper/</link><pubDate>Mon, 30 Dec 2019 11:48:40 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/paper/</guid><description> Wikipeida: 論文</description></item><item><title>Deep Learning</title><link>https://iimuz.github.io/scrapbook/study/deep_learning/</link><pubDate>Mon, 30 Dec 2019 10:41:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_learning/</guid><description> Wikipedia: Deep Learning</description></item><item><title>arXiv.org</title><link>https://iimuz.github.io/scrapbook/study/arxiv/</link><pubDate>Mon, 30 Dec 2019 10:34:30 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/arxiv/</guid><description> arXiv.org</description></item><item><title>Machine Learning</title><link>https://iimuz.github.io/scrapbook/study/machine_learning/</link><pubDate>Mon, 30 Dec 2019 10:34:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/machine_learning/</guid><description> Wikipedia: Machine Learning</description></item><item><title>TensorFlow</title><link>https://iimuz.github.io/scrapbook/study/tensorflow/</link><pubDate>Mon, 30 Dec 2019 03:20:39 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/tensorflow/</guid><description> TensorFlow.org Wikipedia: TensorFlow</description></item><item><title>Network Architecture</title><link>https://iimuz.github.io/scrapbook/study/network-architecture/</link><pubDate>Mon, 30 Dec 2019 03:16:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/network-architecture/</guid><description> Deep Learning のネットワークアーキテクチャ</description></item><item><title>Knowledge Distillation</title><link>https://iimuz.github.io/scrapbook/study/knowledge_distillation/</link><pubDate>Mon, 30 Dec 2019 03:11:03 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/knowledge_distillation/</guid><description>蒸留とは? Deep Learning における知識の蒸留
学習時には deep なネットワークで学習することで、精度が高くなる。 しかしながら、推論時には計算資源などの問題から軽量なネットワークを使いたい。 これには、相反する関係となっている。 軽量であるということは精度が犠牲になっている。 そこで、 deep なネットワークでの出力を模擬するような軽量なネットワークを学習する。 これにより、単純に軽量なネットワークを学習するよりも、 よい精度のモデルを学習することができる。</description></item><item><title>Generative adversarial network</title><link>https://iimuz.github.io/scrapbook/study/gan/</link><pubDate>Mon, 30 Dec 2019 02:04:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/gan/</guid><description> Wikipeida: Generative Adversarial Network</description></item><item><title>Dataset</title><link>https://iimuz.github.io/scrapbook/study/dataset/</link><pubDate>Mon, 30 Dec 2019 01:59:45 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/dataset/</guid><description> Wikipedia: Data set Wikipedia: List of datasets for machine learning research</description></item><item><title>feature</title><link>https://iimuz.github.io/scrapbook/study/feature/</link><pubDate>Mon, 30 Dec 2019 01:57:11 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/feature/</guid><description> Wikipedia: Feature(Machine Learning) パターン認識や機械学習の特徴量。</description></item><item><title>Point Cloud</title><link>https://iimuz.github.io/scrapbook/study/point_cloud/</link><pubDate>Mon, 30 Dec 2019 01:54:54 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/point_cloud/</guid><description>-Wikipedia: Point Cloud</description></item><item><title>Machine Vision</title><link>https://iimuz.github.io/scrapbook/study/machine_vision/</link><pubDate>Mon, 30 Dec 2019 01:47:19 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/machine_vision/</guid><description> Wikipeidia: Machine Vision</description></item><item><title>Reinforcement Learning</title><link>https://iimuz.github.io/scrapbook/study/reinforcement_learning/</link><pubDate>Mon, 30 Dec 2019 01:43:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/reinforcement_learning/</guid><description> Wikipedia: Reinforcement Learning</description></item><item><title>Counterfactual Visual Explanations</title><link>https://iimuz.github.io/scrapbook/study/counterfactual_visual_explanations/</link><pubDate>Tue, 15 Oct 2019 21:03:44 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/counterfactual_visual_explanations/</guid><description>arXiv arXiv Times 画像の分類根拠を可視化する手法。 野鳥が例に出されているが、何でも使えそう。 これ以外にも、可視化手法は調べると良いかもしれない。</description></item><item><title>Benchmarking Model-Based Reinforcement Learning</title><link>https://iimuz.github.io/scrapbook/study/benchmarking-model-based-reinforcement-learning/</link><pubDate>Tue, 13 Aug 2019 23:34:25 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/benchmarking-model-based-reinforcement-learning/</guid><description>論文情報 title: Benchmarking Model-Based Reinforcement Learning author: Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba year: 2019/7/3 arxiv issue vanity Google Translate: vanity 比較データなど どんなものか 強化学習の 14 手法に関して、同一のデータセットを利用して性能を比較した。
Figure 3: The relative performance with different planning horizon. 先行研究と比べてどこがすごいのか 従来は、各手法で優位性を述べており、同一のデータセットで比較し、 相対的な性能の優位性を適切に述べているものがなかった。
技術や手法のキモどこか OpenAI Gym という強化学習用のデータセットを利用して、 15 の環境で評価している。 また、ノイズを加えた状態も評価している。</description></item><item><title>Cold Case: the Lost MNIST Digits</title><link>https://iimuz.github.io/scrapbook/study/cold-case-the-lost-mnist-digits/</link><pubDate>Thu, 01 Aug 2019 11:54:22 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/cold-case-the-lost-mnist-digits/</guid><description>軽く読んだが再構成はして、評価しなおしているがデータセットが公開されているわけではないらしい。
arXiv issue vanity</description></item><item><title>ブラックホール撮影にも使えるスパースモデリングとは</title><link>https://iimuz.github.io/scrapbook/study/sparse_modeling_blackhole/</link><pubDate>Sun, 21 Apr 2019 11:17:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/sparse_modeling_blackhole/</guid><description>少々スパースモデリングがブラックホール撮影に使われているということで面白かったのでメモです。
ブラックホール撮影にも使える「スパースモデリング」とは？【機械学習】 要は、取得できる画像が低解像度なので高解像度にするのにスパースモデリングを使ったよということを起点としています。 この記事自体は実際に使われた手法に関して説明しているわけではなく、 スパースモデリングの基本的なことを説明しています。 Lasso を利用して下記の式を最適化しているようです。
$$ L(\bold{I}) = \parallel \bold{V} - \bold{F}\bold{I} \parallel_2^2 + \parallel \bold{I} \parallel_1 $$
$ \bold{I} $: ブラックホールの高解像度画像 $ \bold{V} $: 撮影した画像 (フーリエ変換済み) $ \bold{F} $: フーリエ変換 かなり以前から知られている範囲での説明に終わっているため、 実際には、これ以上の方法が用いられているようです。
復元結果 (引用 1) (引用 1) 本間ら「スパースモデリング天文学 ― ブラックホール撮像から時間変動減少まで」， 科学研究費補助金新学術領域研究「スパースモデリングの深化と高次元データ駆動科学の創成」 最終成果報告会 (2017/12/18-20)</description></item><item><title>三次元点群を扱うニューラルネットワークのサーベイ (ver.2)</title><link>https://iimuz.github.io/scrapbook/study/point_cloud_deep_servey/</link><pubDate>Sun, 21 Apr 2019 10:59:20 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/point_cloud_deep_servey/</guid><description>資料 資料リンク 著者などの情報 橋本、鏡研究室 どちらの方も 3 次元点群を利用する系統を研究されているようです。 産業用ロボットやプロジェクションマッピングなどを研究されています。 千葉 直也 2019/4 月現在は D3 の方</description></item><item><title>DCGAN のためのデータセット調査</title><link>https://iimuz.github.io/scrapbook/study/training_data_dcgan/</link><pubDate>Fri, 12 Apr 2019 06:48:27 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/training_data_dcgan/</guid><description>DCGAN で MNIST 以外を動かすとしたらどういうデータがありそうか調査。 本当は、最終的に Style GAN で適当な画像を入れたら、 それのアイコン(漫画チック)な画像を生成するようにしたい。
arXiveTimes がまとめているデータセット一覧
日本語の崩し文字: MNIST の日本語版みたいな感じか。 The Art Institute of Chicago THE COLLECTION: シカゴ美術館の絵画などの画像。絵画以外も入っている。 Painter by Numbers(PBN): 36GB くらいあるが絵画の画像が手に入るっぽい。 Kaggle データ。 How Do Humans Sketch Objects? (TU-Berlin dataset): スケッチ画像が載っています。 Manga109: 漫画データのようです。 ただし、利用にあたり学術目的のみであり、引用元の表記などが必要になるようです。 AnimeFace Character Dataset アニメの顔画像を集めたデータセット。 LLD - Large Logo Dataset: GAN を想定しているデータセットとのこと。 favicon のような画像を集めています。 Favicons: Kaggle のデータセットで favicon の寄せ集め。 Cartoon Set 2 次元のアバターイメージのデータセット。</description></item><item><title>600円のRasPiZero【単体で】Mobilenetv2 1000 class object detection【10fps弱】を達成</title><link>https://iimuz.github.io/scrapbook/study/dl_time_raspizero_mobilenet_10fps/</link><pubDate>Thu, 28 Mar 2019 23:37:03 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/dl_time_raspizero_mobilenet_10fps/</guid><description>概要 Nikkei Robotics 2019/3 において、RasPiZero で Mobilenet v2 の 10fps を実現できたことの紹介。 日本の Idein というベンチャー企業が成し遂げた。 これにより、 1000 円前後の環境で物体認識ができるということになり、エッジデバイスの単価が下がっている。 実現のためには、 RasPi に搭載している GPU を利用できるように、独自の RasPi 用の GPU ライブラリを作成し利用している。
トピック 全ての RasPi は GPU Broadcom 社製 VideoCore を標準搭載 VideoCore は 24 ～ 28.8[GFLOPS]と最新のスマホの 1/10 以下とはいえ初代 XBox よりは高速 RasPi の大ヒットを受けて 2014 年に Broadcom 社が仕様書を公開 日ベンチャー Idein 社は仕様書を読み込み VideoCore ハードウェアからソフトウェアスタックを積み上げ TensorFlow および Chainer で叩ける【VideoCore 版 CUDA のようなもの】を内製 VideoCore の DL 実行効率は 40%に達し 1GB のメインメモリに展開できるモデルならだいたい現実的な速度で叩けるらしい</description></item><item><title>Object Recognition in 3D Scenes with Occlusions and Clustter by Hough Voting</title><link>https://iimuz.github.io/scrapbook/study/objectrecognitionin3dsceneswithocclusionsandclustterbyhoughvoting/</link><pubDate>Tue, 13 Nov 2018 13:28:54 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/objectrecognitionin3dsceneswithocclusionsandclustterbyhoughvoting/</guid><description>Abstract 2010 Fourth Pacific-Rim Symposium on Image and Video Technology
Federico Tombari, Luigi Di Stefano
In this work we propose a novel Hough voting approach for the detection of free-form shapes in a 3D space, to be used for object recognition tasks in 3D scenes with a significant degree of occlusion and clutter. The proposed method relies on matching 3D features to accumulate evidence of the presence of the objects being sought in a 3D Hough space.</description></item><item><title>Unique Signatures of Histograms for Local Surface Description (SHOT)</title><link>https://iimuz.github.io/scrapbook/study/uniquesignaturesofhistogramsforlocalsurfacedescription_shot/</link><pubDate>Tue, 06 Nov 2018 18:18:47 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/uniquesignaturesofhistogramsforlocalsurfacedescription_shot/</guid><description>Unique Signatures of Histograms for Local Surface Description (SHOT) Abstract Federico Tombari, Samuele Salti, and Luigi Di Stefano
ECCV 2010
Abstract
This paper deals with local 3D descriptors for surface matching. First, we categorize existing methods into two classes: Signatures and Histograms. Then, by discussion and experiments alike, we point out the key issues of uniqueness and repeatability of the local reference frame. Based on these observations, we formulate a novel comprehensive proposal for surface representation, which encompasses a new unique and repeatable local reference frame as well as a new 3D descriptor.</description></item><item><title>Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</title><link>https://iimuz.github.io/scrapbook/study/unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery/</link><pubDate>Sun, 02 Sep 2018 11:27:34 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery/</guid><description>arXivTimes で見つけた論文です。 GAN を利用した異常検知技術を調べていた時に見つけました。
AnoGAN というネットワークを提案しています。 医療用画像を対象に、正常画像のみから、 GAN で学習していくようです。 このとき、 Generator は潜在空間から画像生成されると仮定します。 面白いのは、 GAN だけで完結しており、 検査時の入力画像は Auto Encoder のように再構成せず、 Generator から近い画像を生成するようにしているようです。 GAN は、潜在空間から画像は生成できるが、画像から潜在空間は変換できないはずです。 そこで、潜在空間は連続的に変化することから最初はランダムランプリングで潜在空間から画像を生成し、 近い画像へ潜在空間上を移動させるという処理を行うようです。 そのため、検査時に時間がかかる可能性はあるように思います。 また、残差誤差だけでなく、 Descriminator 側の出力も考慮して異常スコアを算出しているようです。
実装例: LeeDoYup/AnoGAN 日本語で解説してくれている記事です。 GAN による医療画像の異常検知 【論文読み】GAN を利用した異常検知まとめ その他に、論文を読んでいるときに調べながら見つけた論文 Unsupervised Adversarial Anomaly Detection using One-Class Support Vector Machines One-Class Adversarial Nets for Fraud Detection</description></item></channel></rss>