<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Studies on しさく</title><link>https://iimuz.github.io/scrapbook/study/</link><description>Recent content in Studies on しさく</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sat, 15 Feb 2020 12:39:08 +0900</lastBuildDate><atom:link href="https://iimuz.github.io/scrapbook/study/index.xml" rel="self" type="application/rss+xml"/><item><title>NeuralIPS</title><link>https://iimuz.github.io/scrapbook/study/neuralips/</link><pubDate>Sat, 15 Feb 2020 12:39:08 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/neuralips/</guid><description> NeuralIPS</description></item><item><title>Guided Similarity Separation for Image Retrieval</title><link>https://iimuz.github.io/scrapbook/study/guided_similarity_separation_for_image_retrieval/</link><pubDate>Sat, 15 Feb 2020 12:36:06 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/guided_similarity_separation_for_image_retrieval/</guid><description> NIPS 2019</description></item><item><title>FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</title><link>https://iimuz.github.io/scrapbook/study/fixmatch_simplifying_semi_supervised_learning_with_consistency_and_confidence/</link><pubDate>Sat, 15 Feb 2020 11:50:27 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/fixmatch_simplifying_semi_supervised_learning_with_consistency_and_confidence/</guid><description> arXiv 著者: Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel 投稿日: 2020.1.21 小数のラベルデータから他のデータにラベル付けをしつつ精度改善する方法です。 ラベル付け作業を軽減できる可能性があるので非常に面白いと思います。
参考資料 2020.1.26 Medium 1 枚しかラベルデータがなくても学習できる FixMatch 2020.2.12 AI-SCHOLAR 機械学習のためのデータ収集に新たな希望！？半教師学習の最前線！</description></item><item><title>Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</title><link>https://iimuz.github.io/scrapbook/study/unsupervised_anomaly_detction_via_variational_auto_encoder_for_seasonal_kpis_in_web_applications/</link><pubDate>Thu, 06 Feb 2020 10:21:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/unsupervised_anomaly_detction_via_variational_auto_encoder_for_seasonal_kpis_in_web_applications/</guid><description> arXiv]arxiv 著者: Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, Jie Chen, Zhaogang Wang, Honglin Qiao 投稿日: 2018.2.12</description></item><item><title>Saliency detection by multi-context deep learning</title><link>https://iimuz.github.io/scrapbook/study/saliency_detection_by_multi_context_deep_learning/</link><pubDate>Wed, 05 Feb 2020 11:14:34 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/saliency_detection_by_multi_context_deep_learning/</guid><description> IEEE Explore CVPR 2015 著者: Rui Zhao, Wanli Ouyang, Hongsheng Li, Xiaogang Wang 投稿日: 2015.6.15</description></item><item><title>Fourier Transform</title><link>https://iimuz.github.io/scrapbook/study/fourier_transform/</link><pubDate>Wed, 05 Feb 2020 11:07:30 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/fourier_transform/</guid><description> Wikipedia Fourier Transform Wikipedia フーリエ変換 Wikipedia 高速フーリエ変換</description></item><item><title>Saliency Detection: A Spectral Residual Approach</title><link>https://iimuz.github.io/scrapbook/study/saliency_detection/</link><pubDate>Wed, 05 Feb 2020 11:03:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/saliency_detection/</guid><description> IEEE Explore citeseerx 著者: Xiaodi Hou, Liqing Zhang 投稿日: 2007.6.17</description></item><item><title>Kernel Density Estimation</title><link>https://iimuz.github.io/scrapbook/study/kernel_density_estimation/</link><pubDate>Tue, 04 Feb 2020 18:17:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/kernel_density_estimation/</guid><description> Wikipedia カーネル密度推定</description></item><item><title>LOF: Identifying Density-Based Local Outliers</title><link>https://iimuz.github.io/scrapbook/study/local_outlier_factor/</link><pubDate>Tue, 04 Feb 2020 18:01:14 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/local_outlier_factor/</guid><description> PDF 著者: Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, Jörg Sander 投稿日: 2000 参考資料 Wikipedia 局所外れ値因子法 Scikit-Learn sklearn.neighbors.LocalOutlierFactor 2018.9.4 Local Outlier Factor (LOF) による外れ値検知についてまとめた</description></item><item><title>Support Vector Machine (SVM)</title><link>https://iimuz.github.io/scrapbook/study/support_vector_machine/</link><pubDate>Tue, 04 Feb 2020 17:44:53 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/support_vector_machine/</guid><description> PDF 著者: V VAPNIK 投稿日: 1963 Wikipedia Support-vector machine Wikipedia サポートベクターマシーン</description></item><item><title>Supervised Learning</title><link>https://iimuz.github.io/scrapbook/study/supervised_learning/</link><pubDate>Tue, 04 Feb 2020 17:42:07 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/supervised_learning/</guid><description> Wikipedia Supervised Learning Wikipedia 教師あり学習</description></item><item><title>Unsupervised Learning</title><link>https://iimuz.github.io/scrapbook/study/unsupervised_learning/</link><pubDate>Tue, 04 Feb 2020 17:38:56 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/unsupervised_learning/</guid><description> Wikipedia Unsupervised Learning Wikipedia 教師なし学習</description></item><item><title>One-Class SVM</title><link>https://iimuz.github.io/scrapbook/study/one_class_svm/</link><pubDate>Tue, 04 Feb 2020 17:36:30 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/one_class_svm/</guid><description> PDF 著者: Larry M. Manevitz, Malik Yousef 投稿日: 2001 参考資料 Scikit-Learn sklearn.svm.OneClassSVM One-class SVM with non-linear kernel (RBF) 2016.11.3 Qiita 異常検知のための One Class SVM 2018.5.20 SlideShare One Class SVM を用いた異常値検知 2019.5.7 One Class Support Vector Machine(One Class SVM)入門</description></item><item><title>Semi-supervised Learning</title><link>https://iimuz.github.io/scrapbook/study/semi_supervised_learning/</link><pubDate>Tue, 04 Feb 2020 17:36:30 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/semi_supervised_learning/</guid><description> Wikipedia Semi-supervised Learning 日本語: 半教師あり学習</description></item><item><title>Pixel Recurrent Neural Networks</title><link>https://iimuz.github.io/scrapbook/study/pixel_recurrent_neural_networks/</link><pubDate>Tue, 04 Feb 2020 16:57:26 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/pixel_recurrent_neural_networks/</guid><description> arXiv 著者: Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu 投稿日: 2016.1.25 参考資料 2016.7.21 SlideShare 論文紹介 Pixel Recurrent Neural Networks</description></item><item><title>Conditional Image Generation with PixelCNN Decoders</title><link>https://iimuz.github.io/scrapbook/study/conditional_image_generation_with_pixelcnn_decoders/</link><pubDate>Tue, 04 Feb 2020 16:54:59 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/conditional_image_generation_with_pixelcnn_decoders/</guid><description> arXiv 著者: Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu 投稿日: 2016.6.16</description></item><item><title>WaveNet: A Generative Model for Raw Audio</title><link>https://iimuz.github.io/scrapbook/study/wavenet/</link><pubDate>Tue, 04 Feb 2020 16:40:35 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/wavenet/</guid><description> arXiv 著者: Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu 投稿日: 2016.9.12 PixelCNN をベースにした音声波形を行うネットワークです。
参考文献 2016.9.8 DeepMind WaveNet: A generative model for raw audio 2017.12.19 Qiita WaveNet まとめ 2019.6.9 AI-SCHOLAR 自然な音声を生成し人と機械のコミュニケーションを円滑にする「WaveNet」とは</description></item><item><title>Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs</title><link>https://iimuz.github.io/scrapbook/study/real_valued_time_series_generation_with_recurrent_conditional_gans/</link><pubDate>Tue, 04 Feb 2020 15:36:49 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/real_valued_time_series_generation_with_recurrent_conditional_gans/</guid><description> arXiv 著者: Cristóbal Esteban, Stephanie L. Hyland, Gunnar Rätsch 投稿日: 2017.6.8</description></item><item><title>An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</title><link>https://iimuz.github.io/scrapbook/study/empirical_evaluation_of_generic_convolutional_and_recurrent_networks_for_sequence_modeling/</link><pubDate>Tue, 04 Feb 2020 15:33:26 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/empirical_evaluation_of_generic_convolutional_and_recurrent_networks_for_sequence_modeling/</guid><description> arXiv 著者: Shaojie Bai, J. Zico Kolter, Vladlen Koltun 投稿日: 2018.3.4</description></item><item><title>Deep Temporal Clustering : Fully Unsupervised Learning of Time-Domain Features</title><link>https://iimuz.github.io/scrapbook/study/deep_temporal_clustering/</link><pubDate>Tue, 04 Feb 2020 15:21:45 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_temporal_clustering/</guid><description> arXiv 投稿日: 2018.2.4</description></item><item><title>Detecting the Onset of Machine Failure Using Anomaly Detection Methods</title><link>https://iimuz.github.io/scrapbook/study/detecting_the_onset_of_machine_failure_using_anomaly_detection_method/</link><pubDate>Tue, 04 Feb 2020 14:55:51 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/detecting_the_onset_of_machine_failure_using_anomaly_detection_method/</guid><description>Springer 著者: Mohammad Riazi, Osmar Zaiane, Tomoharu Takeuchi, Anthony Maltais, Johannes Günther, Micheal Lipsett 投稿日: 2019.8.3 PDF 1 軸のベルト式ロボットが扉を開け閉めする時の信号を取得し異常検知する際の特徴抽出方法を提案している。 幾つかの半教師あり学習の手法と精度及び学習時間、テスト時間、メモリ使用量に関して比較している。 特徴抽出のために 1D-CNN Autoencoder を利用しており、畳み込み層と max pooling を組み合わせて 5 層である。
The proposed 1D-CNNAE consists of a CNN encoder network and a mirrored decoder network with 5 layers of alternating convolution and pooling layers. The first convolutional layer consists of 64 filters (dimensionality of output space) and kernel size of 1, followed by a max-pooling layer.</description></item><item><title>Time-Series Anomaly Detection Service at Microsoft</title><link>https://iimuz.github.io/scrapbook/study/time_series_anomaly_detection_service_at_microsoft/</link><pubDate>Tue, 04 Feb 2020 14:39:14 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/time_series_anomaly_detection_service_at_microsoft/</guid><description>KDD2019 における Microsoft の時系列データの異常検知論文。 企業で実際に機械学習を運用している際の話が書かれているようです。
arXiv 投稿日: 2019.1.10 著者: Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang, Xiaoyu Kou, Tony Xing, Mao Yang, Jie Tong, Qi Zhang Microsoft 内で時系列データに対して異常検知を行っている方法です。 提案手法は、Spectral Residual とConvolutional Neural Networkをベースに作られている。
SR による変換結果の例 SR を利用して変換することで、単純に時系列データとして利用するよりも強調した状態を作れているようです。
SR-CNN Architecture The network is composed of two 1-D convolutional layers (with filter size equals to the sliding window size ω) and two fully connected layers.</description></item><item><title>Defying the gravity of learning curve: a characteristic of nearest neighbour anomaly detectors</title><link>https://iimuz.github.io/scrapbook/study/defying_the_gravity_of_learning_curve/</link><pubDate>Tue, 04 Feb 2020 14:29:27 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/defying_the_gravity_of_learning_curve/</guid><description> Springer 著者: Ting, K. M., Washio, T., Wells, J. R., Aryal, S. 投稿日: 2017</description></item><item><title>Theoretical Foundations and Algorithms for Outlier Ensembles</title><link>https://iimuz.github.io/scrapbook/study/theoreteical_foundations_and_algorithms_for_outlier_ensembles/</link><pubDate>Tue, 04 Feb 2020 14:21:33 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/theoreteical_foundations_and_algorithms_for_outlier_ensembles/</guid><description> PDF 著者: C. C. Aggarwal and S. Sathe 投稿日: 2015</description></item><item><title>On the Evaluation of Unsupervised Outlier Detection: Measures, Datasets, and an Empirical Study</title><link>https://iimuz.github.io/scrapbook/study/evaluation_of_unsupervised_outlier_detection/</link><pubDate>Tue, 04 Feb 2020 14:14:29 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/evaluation_of_unsupervised_outlier_detection/</guid><description> Springer 投稿日: 2016.1.16 著者: G. O. Campos, A. Zimek, J. Sander, R. J. G. B. Campello, B.Micenkova, E. Schubert, I.Assent, and M. E. Houle 説明スライド</description></item><item><title>A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data</title><link>https://iimuz.github.io/scrapbook/study/comparative_evaluation_of_unsupervised_anomaly_detection_algorithms_for_multivariate_data/</link><pubDate>Tue, 04 Feb 2020 14:07:55 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/comparative_evaluation_of_unsupervised_anomaly_detection_algorithms_for_multivariate_data/</guid><description>PDF 著者: Markus Goldstein, Seiichi Uchida 投稿日: 2016.4.19 異常検知手法に関して、複数の手法を複数のデータセットで比較検証しています。</description></item><item><title>Distance-Based Outlier Detection: Consolidation and Renewed Bearing</title><link>https://iimuz.github.io/scrapbook/study/distance_based_outlier_detection_consolidation_and_renewed_bearing/</link><pubDate>Tue, 04 Feb 2020 14:01:38 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/distance_based_outlier_detection_consolidation_and_renewed_bearing/</guid><description> PDF 著者: Gustavo H. Orair, Carlos H. C. Teixeira, Wagner Meira Jr., Ye Wang, Srinivasan Parthasarathy 投稿日: 2010</description></item><item><title>Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection</title><link>https://iimuz.github.io/scrapbook/study/learning_representations_of_ultrahigh_dimensional_data_for_random_distance_based_outlier_detection/</link><pubDate>Tue, 04 Feb 2020 13:47:32 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/learning_representations_of_ultrahigh_dimensional_data_for_random_distance_based_outlier_detection/</guid><description> arXiv 著者: Guansong Pang, Longbing Cao, Ling Chen, Huan Liu 投稿日: 2018.6.13</description></item><item><title>Classification under Streaming Emerging New Classes: A Solution using Completely-random Trees</title><link>https://iimuz.github.io/scrapbook/study/classification_under_streaming_emerging_new_classes/</link><pubDate>Tue, 04 Feb 2020 13:39:13 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/classification_under_streaming_emerging_new_classes/</guid><description>PDF 投稿日: 2017 著者: Xin Mu, Kai Ming Ting, Zhi-Hua Zhou 時間経過に伴って分類対象のデータに新規のクラスが追加されるようなケースを問題としています。 新規のクラスは、既存クラスの外れ値ほどは既存クラスより離れていないという仮定の下で問題を解いています。 対処法として、完全にランダムな木を利用するようです。</description></item><item><title>Density-based spatial clustering of applications with noise (DBSCAN)</title><link>https://iimuz.github.io/scrapbook/study/dbscan/</link><pubDate>Tue, 04 Feb 2020 13:32:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/dbscan/</guid><description> Wikipedia DBSCAN</description></item><item><title>Nearest-Neighbour-Induced Isolation Similarity and its Impact on Density-Based Clustering</title><link>https://iimuz.github.io/scrapbook/study/nearest_neighbour_induced_isolation_similarity_and_its_impact_on_density_based_clustering/</link><pubDate>Tue, 04 Feb 2020 13:11:45 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/nearest_neighbour_induced_isolation_similarity_and_its_impact_on_density_based_clustering/</guid><description>arXiv 著者: Xiaoyu Qin, Kai Ming Ting, Ye Zhu, Vincent CS Lee 投稿日: 2019.6.30 DBSCAN の距離計算部分を isolation forest に変えている。</description></item><item><title>Auto Regression (AR)</title><link>https://iimuz.github.io/scrapbook/study/auto_regression/</link><pubDate>Tue, 04 Feb 2020 11:34:52 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/auto_regression/</guid><description> Wikipedia 自己回帰モデル</description></item><item><title>Distributed and parallel time series feature extraction for industrial big data applications</title><link>https://iimuz.github.io/scrapbook/study/distributed_and_parallel_time_series_feature_extraction_for_industrial_big_data_applications/</link><pubDate>Tue, 04 Feb 2020 11:22:44 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/distributed_and_parallel_time_series_feature_extraction_for_industrial_big_data_applications/</guid><description> arXiv 著者: Maximilian Christ, Andreas W. Kempa-Liehr, Michael Feindt 投稿日: 2016.10.25</description></item><item><title>Highly comparative feature-based time-series classification</title><link>https://iimuz.github.io/scrapbook/study/highly_comparative_feature_based_time_series_classification/</link><pubDate>Tue, 04 Feb 2020 11:19:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/highly_comparative_feature_based_time_series_classification/</guid><description> arXiv 著者: Ben D. Fulcher, Nick S. Jones 投稿日: 2014.1.15</description></item><item><title>k Nearest Neighbor</title><link>https://iimuz.github.io/scrapbook/study/knn/</link><pubDate>Tue, 04 Feb 2020 11:16:03 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/knn/</guid><description> Wikipedia K 近傍法</description></item><item><title>shapeDTW: shape Dynamic Time Warping</title><link>https://iimuz.github.io/scrapbook/study/shapedtw_shape_dynamic_time_warping/</link><pubDate>Tue, 04 Feb 2020 11:12:00 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/shapedtw_shape_dynamic_time_warping/</guid><description>著者: Jiaping Zhao, Laurent Itti 投稿日: 2016.6.6 DTW の亜種である。</description></item><item><title>An Experimental Evaluation of Nearest Neighbour Time Series Classification</title><link>https://iimuz.github.io/scrapbook/study/an_experimental_evaluation_of_nearest_neighbor_time_series_classification/</link><pubDate>Tue, 04 Feb 2020 11:09:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/an_experimental_evaluation_of_nearest_neighbor_time_series_classification/</guid><description>arXiv 著者: Anthony Bagnall and Jason Lines 投稿日: 2014.6.18 DTW の亜種である。</description></item><item><title>Derivative Dynamic Time Warping</title><link>https://iimuz.github.io/scrapbook/study/derivative_dynamic_time_warping/</link><pubDate>Tue, 04 Feb 2020 11:05:47 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/derivative_dynamic_time_warping/</guid><description> PDF 著者: Eamonn J. Keogh and Michael J. Pazzani 投稿日: SDM&amp;rsquo;2001 Accepted Papers DTW の亜種である。
参考文献 2019.3.5 Derivative DTW ~時系列パターンの類似度計算の手法~</description></item><item><title>Dynamic Time Warping (DTW)</title><link>https://iimuz.github.io/scrapbook/study/dynamic_time_warping/</link><pubDate>Tue, 04 Feb 2020 10:56:02 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/dynamic_time_warping/</guid><description>Table of Contents 概要 名称 実装例 関連手法 時系列データの解析方法 参考文献 概要 Dynamic Time Warping (DTW) は、時系列の非線形伸縮に対応したマッチング技術である。 単純にスライディングウィンドウで分割した区間において信号の差をユークリッド距離などで計測すると、 信号の伸縮の違いにより大きな誤差となる場合がある。 信号の伸縮を考慮して一致する形状を算出する。 変換のイメージを下記に示す。上段のグラフは 2 つの信号をそのまま比較した場合となる。 一方、下段のグラフは、2 つの信号間の伸縮を考慮して重ねたグラフとなる。
DTW のイメージ図1 名称 DTW の呼び方は分野によって異なるため、下記のような名称が使われる。
Dynamic Time Worping (DTW) 編集距離 (edit distance) 非線形マッチング (nonlinear matching) 弾性マッチング (elastic matching) アライメント (alignment) DP マッチング (Dynamic programming matching) 実装例 もっとも簡単な DTW distance の計算方法
def dtw_distance(s: np.</description></item><item><title>A Meta-Analysis of the Anomaly Detection Problem</title><link>https://iimuz.github.io/scrapbook/study/meta_analysis_of_the_anomaly_detection_problme/</link><pubDate>Tue, 04 Feb 2020 05:24:56 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/meta_analysis_of_the_anomaly_detection_problme/</guid><description> arXiv 著者: Andrew Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern, Weng-Keen Wong 投稿日: 2015.3.3</description></item><item><title>Isolation Forest</title><link>https://iimuz.github.io/scrapbook/study/isolation_forest/</link><pubDate>Tue, 04 Feb 2020 04:27:24 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/isolation_forest/</guid><description> PDF 著者: Fei Tony Liu, Kai Ming Ting, Zhi-Hua Zhou 投稿日: 2008 参考資料 Scikit-Learn sklearn.ensemble.IsolationForest IsolationForest example 2016.10.3 SlideShare Isolation forest</description></item><item><title>Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package)</title><link>https://iimuz.github.io/scrapbook/study/time_series_feature_extraction_on_basis_of_scalable_hypothesis_tests/</link><pubDate>Tue, 04 Feb 2020 01:27:21 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/time_series_feature_extraction_on_basis_of_scalable_hypothesis_tests/</guid><description>ScienceDirect 投稿日: 2017.5.31 著者: Maximilian Christ, Nils Braun, Julius Neuffer, Andreas W.Kempa-Liehr python の tsfresh というライブラリの紹介のような感じです。 元論文は、Distributed and parallel time series feature extraction for industrial big data applicationsになるようです。</description></item><item><title>Soil Moisture Active Passive (SMAP)</title><link>https://iimuz.github.io/scrapbook/study/soil_moisture_active_passive/</link><pubDate>Tue, 04 Feb 2020 00:14:02 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/soil_moisture_active_passive/</guid><description> Soil Moisture Active Passive (SMAP)</description></item><item><title>MATLAB EXPO 2019 JAPAN</title><link>https://iimuz.github.io/scrapbook/study/matlab_expo2019/</link><pubDate>Mon, 03 Feb 2020 09:07:25 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/matlab_expo2019/</guid><description>Table of Contents 講演資料 故障予測 xIoT 予知保全システムの構築 MATLAB EXPO 2019 JAPAN 講演資料 故障予測 xIoT 予知保全システムの構築 pdf 著者: 王 暁星 全体としては、Predictive Maintenance Toolbox という MATLAB の機能が予防保全で利用できるということの紹介です。 トピックだけを抜き出しておくと、下記のような感じになっています。
予防保全システム全体のワークフロー 予防保全システムの例 Edge device での計算を導入することによる高速化 システム化及び運用のイメージ 課題: 風力タービンの振動データから劣化のヒントを表す特徴量を特定すること。
課題イメージ シャフトに発生する振動データは、周波数が約 100kHz で、1 日 1 秒間 x50 日分
劣化が低い時と劣化が大きい時の 2 値問題として分離性能が良い特徴量を探索する。</description></item><item><title>MATLAB</title><link>https://iimuz.github.io/scrapbook/study/matlab/</link><pubDate>Mon, 03 Feb 2020 09:04:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/matlab/</guid><description> Wikipedia MATLAB MathWorks 社が開発している数値解析ソフトウェアであり、 その中で使うプログラミング言語の名称でもある。</description></item><item><title>Matlab EXPO</title><link>https://iimuz.github.io/scrapbook/study/matlab_expo/</link><pubDate>Mon, 03 Feb 2020 09:03:07 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/matlab_expo/</guid><description>MATLAB/Simulink の総合テクノロジカンファレンス。</description></item><item><title>From GAN to WGAN</title><link>https://iimuz.github.io/scrapbook/study/from_gan_to_wgan/</link><pubDate>Mon, 03 Feb 2020 04:29:17 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/from_gan_to_wgan/</guid><description> arXiv 著者: Lilian Weng 投稿日: 2019.4.18 参考資料 2019.4.24 GAN から Wasserstein GAN へ</description></item><item><title>Wasserstein metric</title><link>https://iimuz.github.io/scrapbook/study/wasserstein_metric/</link><pubDate>Mon, 03 Feb 2020 04:10:00 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/wasserstein_metric/</guid><description> Wikipedia ワッサースタイン計量</description></item><item><title>Earth Mover's Distance (EMD)</title><link>https://iimuz.github.io/scrapbook/study/earth_movers_distance/</link><pubDate>Mon, 03 Feb 2020 04:05:35 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/earth_movers_distance/</guid><description> Wikipedia Earth mover&amp;rsquo;s distance 2 つ以上の特徴量間の集合同士の距離を計算することができる。
参考資料 2019.8.4 HatenaBlog Earth Mover&amp;rsquo;s Distance (EMD)</description></item><item><title>Wasserstein GAN</title><link>https://iimuz.github.io/scrapbook/study/wasserstein_gan/</link><pubDate>Mon, 03 Feb 2020 03:51:27 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/wasserstein_gan/</guid><description> arXiv 著者: Martin Arjovsky, Soumith Chintala, Léon Bottou 投稿日: 2017.1.26 参考資料 2017.2.6 Wasserstein GAN &amp;ldquo;arXiv:1701.07875&amp;rdquo; 幾つかの実装例が載っています。 2017.1.30 &amp;ldquo;R&amp;rdquo; &amp;ldquo;1701.07875&amp;rdquo; Wasserstein GAN 2019.4.25 Qiita Wasserstein GAN の要約とメモ</description></item><item><title>Gradient Boosting</title><link>https://iimuz.github.io/scrapbook/study/gradient_boosting/</link><pubDate>Sun, 02 Feb 2020 13:00:55 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/gradient_boosting/</guid><description> Wikipedia Gradient boosting</description></item><item><title>NGBoost</title><link>https://iimuz.github.io/scrapbook/study/ngboost/</link><pubDate>Sun, 02 Feb 2020 12:58:06 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/ngboost/</guid><description> Stanford NL Group NGBoost: Natural Gradient Boosting for Probabilistic Prediction GitHub stanfordmlgroup/ngboost arXiv 著者: Tony Duan, Anand Avati, Daisy Yi Ding, Sanjay Basu, Andrew Y. Ng, Alejandro Schuler 投稿日: 2019.10.8 参考文献 2019.11.5 SpeakerDeck NGBoost 論文読んでみた</description></item><item><title>PyTorch</title><link>https://iimuz.github.io/scrapbook/study/pytorch/</link><pubDate>Sun, 02 Feb 2020 12:49:03 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/pytorch/</guid><description> PyTorch PyTorch Hub GitHub pytorch/pytorch Wikipedia PyTorch 参考資料 2018.12.25 HatenaBlog PyTorch-Ignite で学習用コードをスマートにする PyTorch の学習において書く量を減らせるようです。</description></item><item><title>Spiking neural network</title><link>https://iimuz.github.io/scrapbook/study/spiking_neural_network/</link><pubDate>Sun, 02 Feb 2020 10:32:23 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/spiking_neural_network/</guid><description> Wikipedia Spiking neural network 参考文献 2019.12.13 Qiita Spiking Neural Network とは何なのか 基本的な概念とライブラリが紹介されています。</description></item><item><title>Neural Network</title><link>https://iimuz.github.io/scrapbook/study/neural_network/</link><pubDate>Sun, 02 Feb 2020 01:29:07 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/neural_network/</guid><description> Wikipedia ニューラルネットワーク</description></item><item><title>Poisson distribution</title><link>https://iimuz.github.io/scrapbook/study/poisson_distribution/</link><pubDate>Sat, 01 Feb 2020 14:39:41 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/poisson_distribution/</guid><description> Wikipedia ポワソン分布 定義: $ Poiss(\lambda) = \frac{\lambda^k e^{-\lambda}}{\lambda !} $ $lambda$ は 0 以上。</description></item><item><title>Monte Carlo method</title><link>https://iimuz.github.io/scrapbook/study/monte_carlo_method/</link><pubDate>Thu, 30 Jan 2020 09:05:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/monte_carlo_method/</guid><description>Wikipedia モンテカルロ法 シミュレーションや数値計算を乱数を用いて行う手法の総称。
略称: MC 法 ランダムな値を生成する方法。</description></item><item><title>Metropolis-Hastings Algorithm</title><link>https://iimuz.github.io/scrapbook/study/metropolis_hasings_algorithm/</link><pubDate>Thu, 30 Jan 2020 00:29:01 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/metropolis_hasings_algorithm/</guid><description>Wikipedia [メトロポリス・ヘイスティングス法] 直接サンプリングするのが難しい確率分布から 統計標本の配列を生成するのに用いられるマルコフ連鎖を構築するのに用いられる手法
ランダムに取得した値が前回の値よりもカーネル比の値が高い場合に採用するプロセスを繰り返す。</description></item><item><title>A Style-Based Generator Architecture for Generative Adversarial Networks</title><link>https://iimuz.github.io/scrapbook/study/a_style_based_generator_architecture_for_generative_adversarial_networks/</link><pubDate>Sun, 26 Jan 2020 19:36:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/a_style_based_generator_architecture_for_generative_adversarial_networks/</guid><description> arXiv 著者: Tero Karras, Samuli Laine, Timo Aila 投稿日: 2018.12.12 Official Tensorflow Implementation: GitHub NVlabas/stylegan 参考資料 2019.12.21 Qiita StyleGAN を遊び尽くせ!! ~追加学習不要の画像編集~</description></item><item><title>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title><link>https://iimuz.github.io/scrapbook/study/efficientnet_rethinking_model_scaling_for_convolutional_neural_networks/</link><pubDate>Sat, 25 Jan 2020 14:17:02 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/efficientnet_rethinking_model_scaling_for_convolutional_neural_networks/</guid><description> arXiv 著者: Mingxing Tan, Quoc V. Le 投稿日: 2019.5.28 モデルのスケールアップを行う際に必要な方法を考察した文献です。 考察結果を利用してモデルを大きくすることで、SoTA を実現した。
参考資料 2019.10.30 Qiita 2019 年最強の画像認識モデル EfficientNet 解説</description></item><item><title>Prognostics</title><link>https://iimuz.github.io/scrapbook/study/prognostics/</link><pubDate>Thu, 23 Jan 2020 11:10:35 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/prognostics/</guid><description> Wikipedia Prognostics ????? Prognostics &amp;amp; Health Management (PHM)</description></item><item><title>Mathworks 予知保全</title><link>https://iimuz.github.io/scrapbook/study/mathworks_predictive_maintenance/</link><pubDate>Thu, 23 Jan 2020 01:26:52 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/mathworks_predictive_maintenance/</guid><description>Mathworks の公開している予知保全に関係する資料に対するメモです。
予知保全で直面しやすい 4 つの課題とその対処法 予知保全に関して全体像を記述している文献です。 細かい方法に関しては記載されていませんが、 予知保全技術の開発に際しての心構えのようなものを記載しています。 実例に学ぶ予知保全向けデータ活用 課題として下記 2 点が挙げられています。 装置の時系列データを保存するためのストレージなどの制限 解析を実行する人員の不足 Mondi Gronau の事例を用いて予知保全のシステム構築の例を示しています。 ただし、個別技術に関しては記載されていません。 MATLAB/Simulink による予知保全ビデオシリーズ 予知保全シリーズ Part 1: 予知保全の概要と開発事例 タイトルの通り概要と事例の紹介及び製品の紹介です。 技術的な内容には踏み込んでいません。 予知保全シリーズ Part 2: 予知保全システムの開発フロー 基本的なモデル(SVM や Decision Tree 系など)を利用して、 異常検知のシステムを Matlab で構築する手順の紹介です。 予知保全シリーズ Part 3: 予知保全を可能にする特徴量選択 Matlab を利用して特徴量をヒストグラムなどで可視化して、 分類に寄与しそうな特徴量を手動で選択する方法の紹介です。 予知保全シリーズ Part 4: 機器の寿命を予測するモデル構築 寿命予測の方式に関して説明しています。 機器の寿命予測を行う方法として、下記の分類を利用しています。 ただし、各手法に関する説明はありませんでした。 類似性モデル: 故障までのセンサー情報が多数ある場合 Hash Similarity Model: データ量が膨大な場合 Pairwise Similarity Model: 信号形状を基準に判断する場合 Residual Similarity Model: 劣化の時系列モデルを構築する場合 劣化モデル: 閾値が設定できる場合 Linear Degradation Model: ダメージの蓄積がない場合 Exponential Degradation Model: ダメージの蓄積がある場合 生存モデル: 故障までに掛かった時間情報がある場合 Reliability Survival Model: 関連変数がない場合 Covariance Survival Model: 関連変数がある場合 ここでは、 Exponential Degradation Model を Matlab で利用する方法を紹介しています。 最適化処理などは全て Matlab 関数で記載されているため、内容は確認できません。 下記の関数に対して事後確率分布を求めることで推定しているようです。 Exponentail function: $ h(t) = \phi + \theta \exp\left( \beta t + \epsilon - \frac{\sigma^2}{2} \right) $ 補足資料として、類似性モデルを使った例が示されています。 方法の詳細は示されていませんが、 過去の寿命までの時系列データの中から類似するサンプルを検索し寿命予測を行うようです。 予知保全シリーズ Part 5: 「故障データが無い」場合のアプローチ 故障データがない場合の方法に関して、PCA とマハラノビス距離を利用した方法が述べられています。 もう一つは、故障モデルを考えシミュレーションなどにより故障を予測する例が記載されています。</description></item><item><title>Forecasting remaining useful life: Interpretable deep learning approach via variational Bayesian inferences</title><link>https://iimuz.github.io/scrapbook/study/forecasting_remaining_useful_life_interpretable_deep_leanring_approach_via_variational_bayesian_inferences/</link><pubDate>Tue, 21 Jan 2020 06:39:31 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/forecasting_remaining_useful_life_interpretable_deep_leanring_approach_via_variational_bayesian_inferences/</guid><description> arXiv 著者: Mathias Kraus, Stefan Feuerriegel 投稿日: 2019.7.11 GitHub Mathias/PredictiveMaintenance</description></item><item><title>Decision Tree</title><link>https://iimuz.github.io/scrapbook/study/decision_tree/</link><pubDate>Sun, 19 Jan 2020 20:46:27 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/decision_tree/</guid><description> Wikipedia JP 決定木 Wikipedia EN Decision tree 参考資料 2019.9.16 キヨシの命題 決定木アルゴリズムの重要度(importance)を正しく解釈しよう</description></item><item><title>Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI</title><link>https://iimuz.github.io/scrapbook/study/explainable_artificial_intelligence_xai_concepts_taxonomies_opportunities_and_challenges_toward_responsible_ai/</link><pubDate>Sun, 19 Jan 2020 20:25:33 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/explainable_artificial_intelligence_xai_concepts_taxonomies_opportunities_and_challenges_toward_responsible_ai/</guid><description> arXiv 著者: Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera 投稿日: 2019.10.22</description></item><item><title>Datascience</title><link>https://iimuz.github.io/scrapbook/study/datascience/</link><pubDate>Sun, 19 Jan 2020 20:05:05 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/datascience/</guid><description> Wikipedia データサイエンス 参考資料 2019.12.14 Qiita 半導体生産技術者がデータ分析業界に飛び込んでみて感じたこと</description></item><item><title>Modern Neural Networks Generalize on Small Data Sets</title><link>https://iimuz.github.io/scrapbook/study/modern_neural_networks_generalize_on_small_data_sets/</link><pubDate>Sun, 19 Jan 2020 19:15:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/modern_neural_networks_generalize_on_small_data_sets/</guid><description> NIPS Proceedings 著者: Matthew Olson, Abraham Wyner, Richard Berk 投稿日: NIPS 2018</description></item><item><title>Deep Learning on Small Datasets without Pre-Training using Cosine Loss</title><link>https://iimuz.github.io/scrapbook/study/deep_learning_on_small_datasets_without_pre_training_using_cosine_loss/</link><pubDate>Sun, 19 Jan 2020 18:41:55 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_learning_on_small_datasets_without_pre_training_using_cosine_loss/</guid><description> arXiv 著者: Björn Barz, Joachim Denzler 投稿日: 2019.1.25</description></item><item><title>CondConv: Conditionally Parameterized Convolutions for Efficient Inference</title><link>https://iimuz.github.io/scrapbook/study/condconv_conditional_parameterized_convolutions_for_efficient_inference/</link><pubDate>Sun, 19 Jan 2020 17:56:07 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/condconv_conditional_parameterized_convolutions_for_efficient_inference/</guid><description> arXiv NIPS 2019 著者: Brandon Yang, Gabriel Bender, Quoc V. Le, Jiquan Ngiam 投稿日: 2019.4.10 畳み込み計算において、サンプル毎に畳み込みの重み係数を変える研究。 畳み込みに利用する係数は、プールされている重み係数の線形和で生成されるようです。
参考資料 2019.12.11 Qiita 次世代の畳み込み?! CondConv TensorFlow GitHub tensorflow/tpu</description></item><item><title>Dynamic Convolution: Attention over Convolution Kernels</title><link>https://iimuz.github.io/scrapbook/study/dynamic_convolution_attention_over_convolution_kernels/</link><pubDate>Sun, 19 Jan 2020 17:53:23 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/dynamic_convolution_attention_over_convolution_kernels/</guid><description> arXiv 著者: Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, Zicheng Liu 投稿日: 2019.12.7 参考資料 2019.12.11 GitHub arXivTimes/arXivTimes</description></item><item><title>ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring</title><link>https://iimuz.github.io/scrapbook/study/remixmatch_semi_supervised_learning_with_distribution_alignment_and_augmentation_anchoring/</link><pubDate>Sun, 19 Jan 2020 17:36:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/remixmatch_semi_supervised_learning_with_distribution_alignment_and_augmentation_anchoring/</guid><description> arXiv OpenReview 少量のラベル付き画像を用意し、そこから他の画像に対するラベリングを行い精度を高めた方法。
参考資料 2019.12.6 Qiita 【論文読み】クラスタリングが無くなる日？</description></item><item><title>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</title><link>https://iimuz.github.io/scrapbook/study/enhancing_the_locality_and_breaking_the_memory_bottleneck_of_transofrmer_on_time_series_forecasting/</link><pubDate>Fri, 17 Jan 2020 16:00:31 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/enhancing_the_locality_and_breaking_the_memory_bottleneck_of_transofrmer_on_time_series_forecasting/</guid><description>arXiv 著者: Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, Xifeng Yan 投稿日: 2019.6.29 Transformer の機構を時系列データに適用した文献。 単純に時系列データに適用するとメモリ部分の使用量が大きくなりすぎる。 そこで、畳み込みと local attention と restart attention という方法を提案している。</description></item><item><title>CDSA: Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation</title><link>https://iimuz.github.io/scrapbook/study/cdsa_cross_dimensional_self_attention_for_multivariate_geo_tagged_time_series_imputation/</link><pubDate>Fri, 17 Jan 2020 15:43:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cdsa_cross_dimensional_self_attention_for_multivariate_geo_tagged_time_series_imputation/</guid><description>arXiv 著者: Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, Shih-Fu Chang 投稿日: 2019.5.23 センサデータ(気温とか)の時系列とジオタグ(位置情報)を合わせて、 欠損しているセンサデータを復元する、または予測するタスク。 Cross Dimensional SElf-Attention という方法を提案し、各情報を統合的に扱えるようにしている。</description></item><item><title>Attend and Diagnose: Clinical Time Series Analysis using Attention Models</title><link>https://iimuz.github.io/scrapbook/study/attend_and_diagnose_clinical_time_series_analysis_using_attention_models/</link><pubDate>Fri, 17 Jan 2020 15:13:17 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/attend_and_diagnose_clinical_time_series_analysis_using_attention_models/</guid><description>arXiv 著者: Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan, Andreas Spanias 投稿日: 2017.11.10 時系列データに対して Self-Attention を用いています。</description></item><item><title>Convolutional Newral Network (CNN)</title><link>https://iimuz.github.io/scrapbook/study/cnn/</link><pubDate>Fri, 17 Jan 2020 14:58:21 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cnn/</guid><description> Wikipedia 畳み込みニューラルネットワーク</description></item><item><title>Pay Less Attention with Lightweight and Dynamic Convolutions</title><link>https://iimuz.github.io/scrapbook/study/pay_less_attention_with_lightweight_and_dynamic_convolutions/</link><pubDate>Fri, 17 Jan 2020 14:56:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/pay_less_attention_with_lightweight_and_dynamic_convolutions/</guid><description>arXiv 著者: Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, Michael Auli 投稿日: 2019.1.29 Self-Attention の代替として Dynamic CNN を提案している。 Self-Attention よりも軽量で高速に計算可能。</description></item><item><title>Are Sixteen Heads Really Better than One?</title><link>https://iimuz.github.io/scrapbook/study/are_sixteen_heads_really_better_than_one/</link><pubDate>Fri, 17 Jan 2020 10:18:37 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/are_sixteen_heads_really_better_than_one/</guid><description>arXiv 著者: Paul Michel, Omer Levy, Graham Neubig 投稿日: 2019.5.25 Transformer 系などでは、 Multi-headed Attention を利用します。 そこで、 Multi-headed Attention がどこまで有効なのかを検証した論文です。 結論としては、全てのケースで Multi-headed Attention が有効なわけではなく、 統計的に優位な劣化なしに Multi-headed Attention の数を減らすことができます。 そして、 Self-Attention よりも Encoder-Decoder Layer の Attention の方が有効のようです。 加えて、各 Head の有効性は、学習の初期に決定されるようです。</description></item><item><title>What Does BERT Look At? An Analysis of BERT's Attention</title><link>https://iimuz.github.io/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/</link><pubDate>Fri, 17 Jan 2020 10:06:35 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/what_does_bert_look_at_an_analsys_of_berts_attention/</guid><description> arXiv 著者: Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning 投稿日: 2019.6.11</description></item><item><title>Self-attention with Functional Time Representation Learning</title><link>https://iimuz.github.io/scrapbook/study/self_attention_with_functional_time_representation_learning/</link><pubDate>Fri, 17 Jan 2020 09:58:53 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/self_attention_with_functional_time_representation_learning/</guid><description> arXiv 著者: Da Xu, Chuanwei Ruan, Sushant Kumar, Evren Korpeoglu, Kannan Achan 投稿日: 2019.11.28</description></item><item><title>XLNet: Generalized Autoregressive Pretraining for Language Understanding</title><link>https://iimuz.github.io/scrapbook/study/xlnet/</link><pubDate>Fri, 17 Jan 2020 09:55:57 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/xlnet/</guid><description> arXiv 著者: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le 投稿日: 2019.6.19</description></item><item><title>Temporal Pattern Attention for Multivariate Time Series Forecasting</title><link>https://iimuz.github.io/scrapbook/study/temporal_pattern_attention_for_multivariate_time_series_forecasting/</link><pubDate>Fri, 17 Jan 2020 09:08:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/temporal_pattern_attention_for_multivariate_time_series_forecasting/</guid><description> arXiv 著者: Shun-Yao Shih, Fan-Keng Sun, Hung-yi Lee 投稿日: 2018.9.12 実装: GitHub gantheory/TPA-LSTM</description></item><item><title>Recurrent Neural Network (RNN)</title><link>https://iimuz.github.io/scrapbook/study/rnn/</link><pubDate>Fri, 17 Jan 2020 09:00:58 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/rnn/</guid><description> Wikipedia 回帰型ニューラルネットワーク</description></item><item><title>Long Short-Term Memory (LSTM)</title><link>https://iimuz.github.io/scrapbook/study/lstm/</link><pubDate>Fri, 17 Jan 2020 09:00:07 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/lstm/</guid><description> Wikipedia 長・短期記憶</description></item><item><title>Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks</title><link>https://iimuz.github.io/scrapbook/study/modeling_long_and_short_term_temporal_patterns_with_deep_neural_networks/</link><pubDate>Fri, 17 Jan 2020 08:58:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/modeling_long_and_short_term_temporal_patterns_with_deep_neural_networks/</guid><description> arXiv 著者: Guokun Lai, Wei-Cheng Chang, Yiming Yang, Hanxiao Liu 投稿日: 2017.3.21</description></item><item><title>Transformer</title><link>https://iimuz.github.io/scrapbook/study/transformer/</link><pubDate>Thu, 16 Jan 2020 11:41:24 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/transformer/</guid><description> 2017.8.31 Google AI Blog Transformer: A Novel Neural Network Architecture for Language Understanding 2018.6.27 The Illustrated Transformer 2019.8.22 Reddit Positional Encoding in Transformer Positional Encoding が有効なのは、 Word Embedding における高次元空間において、 凡そ直行するからではないかということらしい。</description></item><item><title>Flow-based Deep Generative Models</title><link>https://iimuz.github.io/scrapbook/study/flow_based_deep_generative_models/</link><pubDate>Thu, 16 Jan 2020 11:32:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/flow_based_deep_generative_models/</guid><description> 2018.10.13 Lil&amp;rsquo;Log Flow-based Deep Generative Models 参考資料 2019.3.28 SlideShare DL輪読会: Flow-based Deep Generative Models</description></item><item><title>Generative Model</title><link>https://iimuz.github.io/scrapbook/study/generative_model/</link><pubDate>Thu, 16 Jan 2020 11:03:23 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/generative_model/</guid><description>Wikipedia Generative Model メモ 生成モデルには、下記のパターンがある。
自己回帰型モデル: MADE1, Dependency Network, DRAW2, PixelRNN, PixelCNN, SkecthRNN 潜在変数モデル: VAE, GAN 参考資料 2017.6.16 ZOZO Tech Blog 自己回帰型モデルの深層学習 2016.5.24 Sequential Neural Models with Stochastic Layers: VAE の decoder として RNN や自己回帰モデルを用いる例 2016.11.15 PixelVAE: A Latent Variable Model for Natural Images: VAE の decoder として PixelCNN を利用する例 2016.1.25 Pixel Recurrent Neural Networks 2017.4.20 Fast Generation for Convolutional Autoregressive Models: 自己回帰型の例 2015.</description></item><item><title>統計学</title><link>https://iimuz.github.io/scrapbook/study/statistics/</link><pubDate>Tue, 14 Jan 2020 08:53:29 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/statistics/</guid><description>Wikipedia 統計学 経験的に得られたばらつきのあるデータから、 応用数学の手法を用いて数値上の性質や規則性、不規則性を見出す。</description></item><item><title>完全独習 統計学入門</title><link>https://iimuz.github.io/scrapbook/study/independent_study_statistics_tutorial/</link><pubDate>Tue, 14 Jan 2020 08:51:45 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/independent_study_statistics_tutorial/</guid><description> 出版: ダイヤモンド社 著者: 小島寛之 発行: 2006.9 ISBN: 978-4-478-82009-4 Memo 統計の種類 記述統計: 得られたデータから特徴を抜き出す 推測統計: 部分的に得られたデータから全体を推測する</description></item><item><title>Automated Machine Learning</title><link>https://iimuz.github.io/scrapbook/study/automl/</link><pubDate>Mon, 13 Jan 2020 20:02:13 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/automl/</guid><description>Wikipedia Automated machine learning ツール or ライブラリ Auto-Sklearn: Scikit-Learn の自動化ライブラリ。 Auto-WEKA: Java 製の統計解析ツール。 adanet: tensorflow ベースのネットワーク自動構築ライブラリ。 クラウドサービス Amazon AWS Machine Learning Google Cloud Cloud AutoML Microsoft Azure What is automated machine learning? 参考資料 2018.10.10 Qiita Automated Machine Learning(@Azure ML)を試す 2019.9.30 Qiita 【機械学習ツール徹底比較】Amazon SageMaker / Google AutoML / Microsoft ML / IBM AutoAI 使用事例と強み 2019.11.17 SlideShare Azure Machine Learning アップデートセミナー 20191127 Azure のエコシステムを利用した機械学習フローを実現するための入門のような感じです。 2019.</description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>https://iimuz.github.io/scrapbook/study/bert/</link><pubDate>Mon, 13 Jan 2020 17:29:43 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/bert/</guid><description> arXiv 投稿日: 2018.10.11 著者: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova 参考資料 2019.12.31 Kaggle Notebook BERT Baseline PyTorch の transformers ライブラリを用いて BERT の Fine tuning から予測までが書かれている。 2020.1.9 SlideShare BERT 入門</description></item><item><title>因果推論 (Causal Inference)</title><link>https://iimuz.github.io/scrapbook/study/causal_inference/</link><pubDate>Sun, 12 Jan 2020 21:37:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/causal_inference/</guid><description> Wikipedia Causal Inference 参考資料 2020.1.7 COMEMO 2020 年こそ理解したい「因果推論」の勉強はじめました</description></item><item><title>DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection</title><link>https://iimuz.github.io/scrapbook/study/deepfakes_and_beyond_a_survey_of_face_manipulation_and_fake_detection/</link><pubDate>Sun, 12 Jan 2020 21:32:53 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deepfakes_and_beyond_a_survey_of_face_manipulation_and_fake_detection/</guid><description> arXiv 投稿日: 2020.1.1 著者: Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, Javier Ortega-Garcia タイトルにあるように、フェイク画像系のまとめ論文です。
参考資料 2020.1.7 arXivTimes</description></item><item><title>Smoothness and Stability in GANs</title><link>https://iimuz.github.io/scrapbook/study/smoothness_and_stability_in_gans/</link><pubDate>Sun, 12 Jan 2020 21:27:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/smoothness_and_stability_in_gans/</guid><description> OpenReview 投稿日: 2019.9.26 著者: Casey Chu, Kentaro Minami, Kenji Fukumizu GAN の収束に関する研究です。
参考資料 2020.1.6 PFN 【ICLR2020 採択論文】GAN のなめらかさと安定性 PFN の 2019年夏季インターンの成果だった用です。</description></item><item><title>2019 年論文まとめ</title><link>https://iimuz.github.io/scrapbook/study/review_2019_paperwork/</link><pubDate>Sun, 12 Jan 2020 14:47:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/review_2019_paperwork/</guid><description>参考資料 2019.12.9 HEARTBEAT 2019’s Top Machine and Deep Learning Research Papers 2019.12.25 Qiita Twitter で振り返る 2019 年の Deep Learning 論文（前編） 2020.1.1 Qiita Twitter で振り返る 2019 年の Deep Learning 論文（後編） 2019.12.31 Qiita 2019 年のおもしろかった DL/ML 論文 10 選 2020.2.12 宙畑 Kaggle 上位ランカーの 5 人に聞いた、2019 年面白かったコンペ 12 選と論文 7 選 掲載されていた論文だけピックアップすると下記のようになっています。 EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks NGBoost Guided Similarity Separation for Image Retrieval ArcFace: Additive Angular Margin Loss for Deep Face Recognition Feature Gradients: Scalable Feature Selection via Discrete Relaxation Embedding Human Knowledge into Deep Neural Network via Attention Map Urban Semantic 3D Reconstruction from Multiview Satellite Imagery</description></item><item><title>Graph Neural Network (GNN)</title><link>https://iimuz.github.io/scrapbook/study/graph_neural_network/</link><pubDate>Sun, 12 Jan 2020 14:47:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/graph_neural_network/</guid><description>参考資料 2019.2.3 Qiita GNN まとめ(1): GCN の導入 2019.2.28 Qiita GNN まとめ(2): 様々な Spatial GCN 2019.3.18 Qiita GNN まとめ(3): 発展編とこれから</description></item><item><title>異種混合学習</title><link>https://iimuz.github.io/scrapbook/study/multi_mixture_learning/</link><pubDate>Fri, 10 Jan 2020 15:04:31 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/multi_mixture_learning/</guid><description>NEC が開発する異種混合学習技術。
NEC [異種混合学習][mec] 異種混合学習技術とビッグデータ分析ソリューションの研究開発</description></item><item><title>不均衡データ (Imbalanced Data)</title><link>https://iimuz.github.io/scrapbook/study/imbalanced_data/</link><pubDate>Thu, 09 Jan 2020 20:21:43 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/imbalanced_data/</guid><description>機械学習において学習データセットのデータがクラスごとにサンプル数が違う状態を指します。
概要 不均衡データに対するアプローチとしては下記の手法がある。
データレベルのアプローチ: 多い方のラベルデータを減らしたり、 少ないほうのラベルデータを増やしたりする1 2 コスト考慮型学習: 少ないほうのラベルデータを誤分類することに対して、 より重いペナルティを課すように損失関数を定義する1 2 異常検知手法の適用: 少ないほうのラベルを異常値とみなす1 モデル評価値の算出手法: 通常の accuracy からデータ数を考慮した accuracy を利用して評価する2 データレベルのアプローチ Undersampling 多いほうのラベルデータを減らすことでデータの均衡をとる方法です。 一見すると簡単で有効そうに感じられるが幾つかの問題がある1。
多いほうのラベルデータからランダムサンプルするだけでは、データに偏りが発生する。 予めクラスタ分析などを実施してからサンプリングを行う必要がある。1 学習した分類器の分散が大きくなるため、 UnderBagging3 のような平均化戦略をとる。 UnderBagging では、 Bagging でアンサンブルを行う。 事後分布がゆがむため、事前分布を用いて事後分布を修正する4。 Undersampling が有効な場合はどういう場合かは [5] で説明されている。 Oversampling 少ないほうのラベルデータを水増しすることでデータの均衡をとる方法です。 代表的な手法として、 Synthetic Minority Oversampling TEchnique (SMOTE)6 がある。 SMOTE は、データ点同士をつないだ線分譲の任意の点をランダムに人口データとして生成する手法。
SMOTE の拡張は下記のような方針に分類できる1 7。
Oversampling のためのデータ点の選択方法 Undersampling を Oversampling の前後いずれで実行するか 補完方法 データ生成を次元削減などの処理に対していつ実施するか データ生成に適応的な手法を利用するかどうか 再ラベル付けをするか SMOTE 実施後のノイズ除去ステップを入れるかどうか SMOTE の拡張として、よく利用される方法は下記になる1 7。</description></item><item><title>Sanity Checks for Saliency Maps</title><link>https://iimuz.github.io/scrapbook/study/sanity_checks_for_saliency_maps/</link><pubDate>Sun, 05 Jan 2020 19:56:38 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/sanity_checks_for_saliency_maps/</guid><description> arXiv 著者: Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim 投稿日: 2018.10.8 可視化手法を比較し可視化手法が本当にモデルの状態を説明しているのかを調べた論文です。
参考資料 2019.11.24: Hatena Blog: データ分析関連のまとめ: Sanity Checks for Saliency Maps</description></item><item><title>Variational Autoencoder</title><link>https://iimuz.github.io/scrapbook/study/variational_autoencoder/</link><pubDate>Sun, 05 Jan 2020 19:45:13 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/variational_autoencoder/</guid><description> Wikipedia: Autoencoder#Variational autoencoder(VAE) 参考資料 2017.7.19: Qiita: Variational Autoencoder徹底解説</description></item><item><title>Autoencoder</title><link>https://iimuz.github.io/scrapbook/study/autoencoder/</link><pubDate>Sun, 05 Jan 2020 19:40:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/autoencoder/</guid><description> Wikipedia: オートエンコーダ 入力と出力が同じになるように学習を行う。 Deep Learning で利用する。 亜種として下記のようなネットワークもある。
Variational Autoencoder 参考文献 2016.10.9: DeepAge: オートエンコーダ：抽象的な特徴を自己学習するディープラーニングの人気者</description></item><item><title>ICLR2020</title><link>https://iimuz.github.io/scrapbook/study/iclr2020/</link><pubDate>Sun, 05 Jan 2020 19:37:51 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iclr2020/</guid><description> ICLR2020 会期: 2020.4.26 - 30</description></item><item><title>International Conference on Learning Representations (ICLR)</title><link>https://iimuz.github.io/scrapbook/study/iclr/</link><pubDate>Sun, 05 Jan 2020 19:36:29 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iclr/</guid><description> Wikipedia: International Conference on Learning Representations</description></item><item><title>Iterative energy-based projection on a normal data manifold for anomaly localization</title><link>https://iimuz.github.io/scrapbook/study/iterative_energy_based_projection_on_a_normal_data_manifold_for_anomaly_localization/</link><pubDate>Sun, 05 Jan 2020 19:32:32 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iterative_energy_based_projection_on_a_normal_data_manifold_for_anomaly_localization/</guid><description> OpenReview.net 著者: David Dehaene, Oriel Frigo, Sébastien Combrexelle, Pierre Eline 投稿日: 2019.9.26 参考資料 Qiita: ICLR2020の異常検知論文を実装してみた Slide Share: ICLR2020の異常検知論文の紹介</description></item><item><title>Pesudo-Label</title><link>https://iimuz.github.io/scrapbook/study/pesudo_label/</link><pubDate>Sun, 05 Jan 2020 16:33:26 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/pesudo_label/</guid><description>ラベル付けされたデータセットと、ラベル付けされていないデータセットが与えられる。 この時に、ラベル付けされたデータセットだけで学習したモデルで、 ラベル付けされていないデータセットのラベルを予測する。 予測したラベルを利用して、全データセットを使って再度モデルを学習することにより、 精度が向上する場合がある。
結局のところ、 MAP 推定などにおける正則化項の役割を果たすことにより精度が向上するようです(1)。
2019..11.4: Hatena Blog: なぜ疑似ラベルが効果的か調べてみた &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>SinGAN: Learning a Generative Model from a Single Natural Image</title><link>https://iimuz.github.io/scrapbook/study/singan_learning_a_generative_model_from_a_single_natural_image/</link><pubDate>Sun, 05 Jan 2020 16:05:01 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/singan_learning_a_generative_model_from_a_single_natural_image/</guid><description> arXiv 著者: Tamar Rott Shaham, Tali Dekel, Tomer Michaeli 投稿日: 2019.5.2 公式ページ: SinGAN: Learning a Generative Model from a Single Natural Image 公式実装: GitHub: tamarott/SinGAN 参考情報 2019.11.13: Qiita: 【論文解説】SinGAN: Learning a Generative Model from a Single Natural Image 2019.12.18: Qiita: ICCV2019 Best paper SinGAN のできる事できない事[実践編] 単一の画像を使って学習を行うことのできる方法を提案しています。 階層的な GAN を用いており、 1 つ分だけ低階層の Generator が生成した画像と、 ノイズを入力として上位階層を学習します。
ネットワークアーキテクチャ Generator のアーキテクチャ</description></item><item><title>Matrix Profile</title><link>https://iimuz.github.io/scrapbook/study/matrix_profile/</link><pubDate>Sun, 05 Jan 2020 15:47:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/matrix_profile/</guid><description>2 つの時系列において、部分時系列同士の近さを計算した行列を指します。
The UCR Matrix Profile Page ライブラリ Matrix Profile Foundation GitHub: target/matrixprofile-ts: python ライブラリ 参考情報 2019.11.15: Qiita: MatrixProfileによるECGデータの異常検知</description></item><item><title>A Closer Look at Spatiotemporal Convolutions for Action Recognition</title><link>https://iimuz.github.io/scrapbook/study/a_closer_look_at_spatiotemporal_convolutions_for_action_recognition/</link><pubDate>Sun, 05 Jan 2020 15:17:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/a_closer_look_at_spatiotemporal_convolutions_for_action_recognition/</guid><description> arXiv 投稿日: 2017.11.30 著者: Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri CVPR 2018 で採択されている。</description></item><item><title>Time Series</title><link>https://iimuz.github.io/scrapbook/study/time_series/</link><pubDate>Sun, 05 Jan 2020 15:12:41 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/time_series/</guid><description>Wikipedia: 時系列 参考資料 Everything you can do with a time series Kaggle Kernel: Everything you can do with a time series 時系列データ(特に、株価チャート)を中心としてチュートリアルを作成している。 ろうそく足チャートとかの説明もある。 ARMA モデルに始まり、SARIMA などの紹介もしている。
Time series Basics: Exploring traditional TS Kaggle Kernel: Time series Basics: Exploring traditional TS 時系列単変量データの解析が中心のようです。ARMA モデルや ARIMA モデル、SARIMA に関して記載しています。 時系列多変量データに関しても商品データ(ショップ毎、アイテムごと)を利用して説明しています。 ただし、予測自体は単変量とほぼ変わらないようです。 欠損値の取り扱いなどに関しても記載されていないです。
2018.9.23 時系列データで Variational AutoEncoder keras 学習する天然ニューラルネット 時系列データで Variational AutoEncoder keras Autoencoder に RNN を適用して時系列用にした例</description></item><item><title>International Conference on Computer Vision</title><link>https://iimuz.github.io/scrapbook/study/iccv/</link><pubDate>Sun, 05 Jan 2020 15:10:50 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iccv/</guid><description> Wikipedia: International Conference on Computer Vision</description></item><item><title>Learning Spatiotemporal Features with 3D Convolutional Networks</title><link>https://iimuz.github.io/scrapbook/study/learning_spatiotemporal_features_with_3d_convolutional_networks/</link><pubDate>Sun, 05 Jan 2020 15:01:00 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/learning_spatiotemporal_features_with_3d_convolutional_networks/</guid><description>arXiv 投稿日: 2014.12.2 著者: Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri 時空間3次元畳み込みによる 3D CNN (C3D) を提案している。</description></item><item><title>Metric Learning</title><link>https://iimuz.github.io/scrapbook/study/metric_learning/</link><pubDate>Sun, 05 Jan 2020 12:47:57 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/metric_learning/</guid><description> Wikipedia: Similarity Learning#Metric Learning</description></item><item><title>Joint Discriminative and Generative Learning for Person Re-identification</title><link>https://iimuz.github.io/scrapbook/study/joint_descriminative_and_generative_leanring_for_person_reidentification/</link><pubDate>Sun, 05 Jan 2020 12:41:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/joint_descriminative_and_generative_leanring_for_person_reidentification/</guid><description> arXiv 投稿日: 2019.4.15 著者: Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, Jan Kautz NVIDIA から CVPR2019 の Oral セッションで発表された論文です。 GAN と Metric Learning を組み合わせて人物の識別精度を向上しています。
参考文献 Person Re-identification 論文解説 Joint Discriminative and Generative Learning for Perosn Re-identification を読む Qiita: Person Re-identification 論文解説 Joint Discriminative and Generative Learning for Perosn Re-identification を読む 投稿日: 2019.11.6</description></item><item><title>Tiny Video Networks</title><link>https://iimuz.github.io/scrapbook/study/tiny_video_networks/</link><pubDate>Sun, 05 Jan 2020 12:23:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/tiny_video_networks/</guid><description> arXiv 投稿日: 2019.10.15 著者: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo 映像認識において、実行速度を向上するためにモデルを軽量化する。 モデルの探索において、実行時間やパラメータの制約をかけ探索を行う。
参考文献 Tiny Video Networks [ML論文読] 精度を保ったまま映像認識を軽量化する Qiita: Tiny Video Networks [ML論文読] 精度を保ったまま映像認識を軽量化する 投稿日: 2019.10.30</description></item><item><title>Natural Language Processing</title><link>https://iimuz.github.io/scrapbook/study/natural_language_processing/</link><pubDate>Sun, 05 Jan 2020 12:18:14 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/natural_language_processing/</guid><description> Wikipedia: 自然言語処理 参考資料 transformers NLP を取り扱えるパッケージです。 2020.1.3 Hatena Blog 機械学習 Memo φ(・ω・ ) 自然言語処理における Embedding の方法一覧とサンプルコード 幾つかのサンプルコードと共に、 BERT 手法などまでの手法解説が行われています。</description></item><item><title>Anomaly Detection</title><link>https://iimuz.github.io/scrapbook/study/anomaly_detection/</link><pubDate>Sun, 05 Jan 2020 12:16:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/anomaly_detection/</guid><description>Table of Contents
概要 Point Anomaly Contextual Anomalies Collective Anomalies ラベル情報 出力 推定タイプ データ 論文系 Tools and implementatin 参考資料 『機械学習による故障予測・異常検知 事例紹介とデータ分析プロジェクト推進ポイント』 Deep learning: the final frontier for signal processing and time series analysis Which Anomaly Detector shold I use Anomaly Detection with Time Series Forecasting 概要 Anomaly Detection: A Survey より抜粋した内容を元に記載します。
Point Anomaly 特定のインスタンスそれ自体が異常のケースです。
ある点自体が異常 時系列の場合 Contextual Anomalies インスタンス自体は正常だが、状況によって異常とみなされるケースです。</description></item><item><title>CVPR2018</title><link>https://iimuz.github.io/scrapbook/study/cvpr2018/</link><pubDate>Sun, 05 Jan 2020 06:34:28 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2018/</guid><description> CVPR2018 会期: 2018.6.19 - 21</description></item><item><title>ICCV2015</title><link>https://iimuz.github.io/scrapbook/study/iccv2015/</link><pubDate>Sun, 05 Jan 2020 06:09:19 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/iccv2015/</guid><description> ICCV2015</description></item><item><title>LightGBM</title><link>https://iimuz.github.io/scrapbook/study/lightgbm/</link><pubDate>Sun, 05 Jan 2020 04:21:26 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/lightgbm/</guid><description> GitHub: microsoft/LightGBM</description></item><item><title>Hist Gradient Boosting Tree</title><link>https://iimuz.github.io/scrapbook/study/gradient_boosting_decision_tree/</link><pubDate>Sun, 05 Jan 2020 04:18:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/gradient_boosting_decision_tree/</guid><description> Wikipedia: Gradinet Boosting#Gradinet Tree Boosting</description></item><item><title>Scikit Learn</title><link>https://iimuz.github.io/scrapbook/study/scikit-learn/</link><pubDate>Sun, 05 Jan 2020 04:16:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/scikit-learn/</guid><description> Wikipedia: scikit-learn</description></item><item><title>Hist Gradient Boosting Tree</title><link>https://iimuz.github.io/scrapbook/study/hist_gradient_boosting_tree/</link><pubDate>Sun, 05 Jan 2020 04:10:55 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/hist_gradient_boosting_tree/</guid><description>sklearn.ensemble.HistGradientBoostingClassifier Scikit-Learn に実装されている GBDT の手法です。
参考文献 最新機械学習モデル HistGradientBoostingTree の性能調査(LightGBM と比較検証) Qiita: 最新機械学習モデル HistGradientBoostingTree の性能調査(LightGBM と比較検証) 投稿日: 2019.5.27 使い方から LightGBM との比較まで記載されています。</description></item><item><title>CVPR 2020</title><link>https://iimuz.github.io/scrapbook/study/cvpr2020/</link><pubDate>Sun, 05 Jan 2020 04:01:47 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2020/</guid><description> CVPR 2020 会期: 2019.6.14 - 19</description></item><item><title>CVPR 2019</title><link>https://iimuz.github.io/scrapbook/study/cvpr2019/</link><pubDate>Sun, 05 Jan 2020 04:00:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/cvpr2019/</guid><description> CVPR 2019 会期: 2019.6.16 - 20 参考資料 CVPR 2019 速報 Slide Share: CVPR 2019 速報 投稿日: 2019.6.21 CVPR 2019 網羅的サーベイ報告会の抜粋 Qiita: CVPR 2019 網羅的サーベイ報告会の抜粋 投稿日: 2019.11.8 簡単なメモ
CVPR 2019 速報の簡易まとめです。 画像識別の手法において現在の主流は Residual Net です。 GAN は研究が活発な分野の一つである。主要な手法の一覧が掲載されています。 CVPR のトレンドを創っている論文で時系列を扱っていそうな文献として下記が記載されています。 Learning Spatiotemporal Features with 3D Convolutional Networks A Closer Look at Spatiotemporal Convolutions for Action Recognition</description></item><item><title>Conference</title><link>https://iimuz.github.io/scrapbook/study/conference/</link><pubDate>Sun, 05 Jan 2020 03:58:00 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/conference/</guid><description> Wikipedia: 研究会</description></item><item><title>Conference on Computer Vision and Pattern Recognition (CVPR)</title><link>https://iimuz.github.io/scrapbook/study/crpr/</link><pubDate>Sun, 05 Jan 2020 03:55:33 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/crpr/</guid><description> [Wikipedia: Conference on Computer Vision and Pattern Recognition]</description></item><item><title>Deep Affinity Network for Multiple Object Tracking</title><link>https://iimuz.github.io/scrapbook/study/deep_afinity_network_for_multiple_object_tracking/</link><pubDate>Sun, 05 Jan 2020 03:29:25 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_afinity_network_for_multiple_object_tracking/</guid><description> arXiv 投稿日: 2018.10.28 著者: ShiJie Sun, Naveed Akhtar, HuanSheng Song, Ajmal Mian, Mubarak Shah 単純に物体検出をしてトラッキングするのではなく、トラッキング対象の識別を含めて行う手法です。 この文献では、あるフレームで検出対象が隠れてしまい、トラッキングが途切れても、 それ以降で再び現れた場合に同じ物体であると識別できるようにしています。
参考文献 多人数トラッキング論文解説 Deep Affinity Network for Multiple Object Tracking を読む Qiita: 多人数トラッキング論文解説 Deep Affinity Network for Multiple Object Tracking を読む 投稿日: 2019.11.3</description></item><item><title>Attention Is All You Need</title><link>https://iimuz.github.io/scrapbook/study/attention/</link><pubDate>Fri, 03 Jan 2020 21:59:36 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/attention/</guid><description>arXiv 著者: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin 投稿日: 2017.6.12 参考資料 GitHub 著者らによる Tensorflow 実装 tensorflow/tensor2tensor 2017.12.21 Hatena Blog ディープラーニングブログ 論文解説 Attention Is All You Need (Transformer) 論文自体を日本語で解説してあり、非常にわかりやすい。 2018.3.24 srome.github.io Understanding Attention in Neural Networks Mathematically 2018.12.4 Qiita 作って理解する Transformer / Attention 各モジュールの意味と単純な実装が記載されており、理解しやすい。 テストコードもあり参考になる。 2019.8.13 StackExchange What exactly are keys, queries, and values in attention mechanisms?</description></item><item><title>European Union regulations on algorithmic decision-making and a "right to explanation"</title><link>https://iimuz.github.io/scrapbook/study/european_union_regulations_on_algorithmic_decision_making_and_a_right_to_explanation/</link><pubDate>Fri, 03 Jan 2020 21:50:05 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/european_union_regulations_on_algorithmic_decision_making_and_a_right_to_explanation/</guid><description> arXiv 著者: Bryce Goodman, Seth Flaxman 投稿日: 2016.6.28</description></item><item><title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title><link>https://iimuz.github.io/scrapbook/study/show_attend_and_tell_neural_image_caption_generation_with_visual_attention/</link><pubDate>Fri, 03 Jan 2020 21:46:38 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/show_attend_and_tell_neural_image_caption_generation_with_visual_attention/</guid><description> arXiv 著者: Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio 投稿日: 2015.2.10</description></item><item><title>Detecting and Explaining Crisis</title><link>https://iimuz.github.io/scrapbook/study/detecting_and_explaining_crisis/</link><pubDate>Fri, 03 Jan 2020 21:43:17 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/detecting_and_explaining_crisis/</guid><description> arXiv 著者: Rohan Kshirsagar, Robert Morris, Sam Bowman 投稿日: 2017.3.26</description></item><item><title>Interpretability</title><link>https://iimuz.github.io/scrapbook/study/interpretability/</link><pubDate>Fri, 03 Jan 2020 21:28:40 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/interpretability/</guid><description>機械学習における解釈性の話。
ディープラーニングの判断根拠を理解する手法 機械学習の解釈性に関して、日本語で色々な文献や考え方がまとめられています。
Qiita: ディープラーニングの判断根拠を理解する手法 投稿日: 2017.9.6 参考文献 SmoothGrad: removing noise by adding noise: Deep Learning において判断根拠を提示する手法 Detecting and Explaining Crisis: SNS において危機的状況を判断しまた判断根拠を提示する手法 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention: Attention を利用して判断根拠を提示する手法 European Union regulations on algorithmic decision-making and a &amp;ldquo;right to explanation&amp;rdquo;: ヨーロッパにおいて判断根拠を提示しする必要があると示している論文 その他 【記事更新】私のブックマーク「機械学習における解釈性（Interpretability in Machine Learning）」 2018.12.18 Slide Share 機械学習モデルの判断根拠の説明 2019.11.4 Qiita keras で Score-CAM 実装．Grad-CAM との比較 Score-CAM の実装と説明が記載されています。 2019.</description></item><item><title>arXiv Times</title><link>https://iimuz.github.io/scrapbook/study/arxiv_times/</link><pubDate>Fri, 03 Jan 2020 20:14:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/arxiv_times/</guid><description> arXivTimes</description></item><item><title>Computer Vision</title><link>https://iimuz.github.io/scrapbook/study/computer_vision/</link><pubDate>Fri, 03 Jan 2020 20:09:39 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/computer_vision/</guid><description> Wikipedia: コンピュータビジョン</description></item><item><title>SmoothGrad: removing noise by adding noise</title><link>https://iimuz.github.io/scrapbook/study/smooth_grad_removing_noise_by_adding_noise/</link><pubDate>Fri, 03 Jan 2020 01:37:41 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/smooth_grad_removing_noise_by_adding_noise/</guid><description>Deep Learningにおける説明性のための手法。
arXiv SMOOTH GRAD 投稿日: 2017.6.12 著者: Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, Martin Wattenberg</description></item><item><title>Word2Vec</title><link>https://iimuz.github.io/scrapbook/study/word2vec/</link><pubDate>Thu, 02 Jan 2020 14:17:09 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/word2vec/</guid><description> Wikipedia: Word2vec</description></item><item><title>MobileNetV2: Inverted Residuals and Linear Bottlenecks</title><link>https://iimuz.github.io/scrapbook/study/mobilenetv2/</link><pubDate>Mon, 30 Dec 2019 12:14:12 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/mobilenetv2/</guid><description> 2018.4.3: MobileNetV2: The Next Generation of On-Device Computer Vision Networks</description></item><item><title>論文</title><link>https://iimuz.github.io/scrapbook/study/paper/</link><pubDate>Mon, 30 Dec 2019 11:48:40 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/paper/</guid><description> Wikipeida: 論文</description></item><item><title>Deep Learning</title><link>https://iimuz.github.io/scrapbook/study/deep_learning/</link><pubDate>Mon, 30 Dec 2019 10:41:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/deep_learning/</guid><description>Wikipedia: Deep Learning 参考資料 Deep Learning State of the Art (2020) | MIT Deep Learning Series MIT の教授による機械学習関連の最新動向。 2019.12.25 実務で使えるニューラルネットワークの最適化手法 SGD, Adam, AdamW, AdaBound, RAdam の比較が載っています。 RAdam が一番安定性、精度、収束速度のバランスが優れているということのようです。 小さいデータにもとづいてディープラーニングを使う方法 2019.12.12 AINOW 小さいデータにもとづいてディープラーニングを使う方法 小さい学習データでディープラーニングモデルを構築する方法としえ端的には下記のようにまとめられています。
データ収集: 可能な限りデータを収集する fine tuning: 解決すべき問題が属するドメインに関する大規模なモデルを流用する data augumentation: 既存の学習データを加工して、学習データを増やす 損失関数の変更: 損失関数にコサイン類似度を利用するとパフォーマンスが向上することがある モデルの分解: ディープラーニングモデルを多数のニューラルネットワークに分解すると、学習データも小規模化できる autoencoder: オートエンコーダを利用して重みを最適化する 事前知識の組み込み: 学習データが関連するドメイン知識を学習プロセスに組み込むと、学習データを小規模化できる 本記事で紹介されている文献です。
ファインチューニングに関する内容 PyTorch FINETUNING TORCHVISION MODELS 2016.</description></item><item><title>arXiv.org</title><link>https://iimuz.github.io/scrapbook/study/arxiv/</link><pubDate>Mon, 30 Dec 2019 10:34:30 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/arxiv/</guid><description> arXiv.org</description></item><item><title>Machine Learning</title><link>https://iimuz.github.io/scrapbook/study/machine_learning/</link><pubDate>Mon, 30 Dec 2019 10:34:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/machine_learning/</guid><description>Wikipedia: Machine Learning 参考資料 Browse State-of-the-Art タスクごとに、色々な SotA の手法が掲載されています。 2017.5.18 J.P.Morgan Big Data and AI Strategies 金融サービスに関するレポート。280 ページくらいあり。 2019 令和元年 Google 機械学習技術総決算 Google が開発したサービスやツールに関する 2019 年の総まとめです。 どんな技術が公開されたかを一覧することができます。 2020.1.9 towards data science Why we’re writing machine learning infrastructure in Go, not Python 機械学習の推論部分に関しては python を利用するが、それ以外のシステム的な部分は Go のほうがいいという主張です。 システム部分を構築する場合は python は、あまり向かない気がします。 2020.1.20 ill-identified diary 計量経済学と機械学習の関係 –AI はさだめ, さだめは反事実 (転送用) 計量経済学と機械学習の関係性に関して述べられており、共通部分などが掲載されています。 Choosing the right estimator Scikit-Learn: Choosing the right estimator Scikit-Learn が提供するアルゴリズムチートシートです。</description></item><item><title>TensorFlow</title><link>https://iimuz.github.io/scrapbook/study/tensorflow/</link><pubDate>Mon, 30 Dec 2019 03:20:39 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/tensorflow/</guid><description> TensorFlow.org Tensorflow Hub Tensorflow Examples Wikipedia: TensorFlow 参考資料 2019.1.15 What’s coming in TensorFlow 2.0</description></item><item><title>Network Architecture</title><link>https://iimuz.github.io/scrapbook/study/network-architecture/</link><pubDate>Mon, 30 Dec 2019 03:16:04 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/network-architecture/</guid><description> Deep Learning のネットワークアーキテクチャ</description></item><item><title>Knowledge Distillation</title><link>https://iimuz.github.io/scrapbook/study/knowledge_distillation/</link><pubDate>Mon, 30 Dec 2019 03:11:03 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/knowledge_distillation/</guid><description>蒸留とは? Deep Learning における知識の蒸留
学習時には deep なネットワークで学習することで、精度が高くなる。 しかしながら、推論時には計算資源などの問題から軽量なネットワークを使いたい。 これには、相反する関係となっている。 軽量であるということは精度が犠牲になっている。 そこで、 deep なネットワークでの出力を模擬するような軽量なネットワークを学習する。 これにより、単純に軽量なネットワークを学習するよりも、 よい精度のモデルを学習することができる。</description></item><item><title>Generative adversarial network</title><link>https://iimuz.github.io/scrapbook/study/gan/</link><pubDate>Mon, 30 Dec 2019 02:04:48 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/gan/</guid><description> Wikipeida: Generative Adversarial Network 参考資料 2019.7.17 AINOW 敵対的生成ネットワークの台頭【前編】 GAN の派生形等の有名どころを紹介している。 DCGAN, BigGAN, StyleGAN, StackGAN, CycleGAN, Pix2pix, Age-cGAN This Person does NOT exist GAN によって存在しない人物の顔画像を生成し表示してくれるサイト。 NVIDIA の StyleGAN が利用されているらしい。 2019.11.29 Qiita 主要な GAN 研究の歴史（2019 年 11 月現在） ネットワーク構造と参考文献が記載されており概要を確認するには良いと思います。</description></item><item><title>Dataset</title><link>https://iimuz.github.io/scrapbook/study/dataset/</link><pubDate>Mon, 30 Dec 2019 01:59:45 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/dataset/</guid><description> Wikipedia: Data set Wikipedia: List of datasets for machine learning research Google Dataset Search Google Dataset Search
Google が公開しているデータセットを検索するためのページです。 Google 検索のように検索することができます。
参考資料
2020.1.24 Gigazine Google が機械学習用のデータセットをインターネット上から検索可能な「Dataset Search」を正式公開 その他 ObjectNet 学習データセットはない。 テスト用データセットのみで 5 万枚ある。 2019.12.12 Gigazine 画像認識モデルの「盲点」を克服するための奇妙な画像ばかり集めたデータセット「ObjectNet」を MIT と IBM の研究チームが公開 Soil Moisture Active Passive SRPBS Multidisorder MRI Dataset: MRI imaging dataset 参考資料 2020.1.8 HatenaBlog 渋谷駅前で働くデータサイエンティストのブログ Fashion-MNIST: 簡単になり過ぎた MNIST に代わる初心者向け画像認識ベンチマーク データセットとして新規のものが書かれているわけではないです。 これほど情報発信している人でも、関係なければ知らないのだなぁという感想を抱いたのでメモしておきます。 入門用なので十分だと思いますが、たどり着いているのが fashion-MNIST です。</description></item><item><title>feature</title><link>https://iimuz.github.io/scrapbook/study/feature/</link><pubDate>Mon, 30 Dec 2019 01:57:11 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/feature/</guid><description>Wikipedia: Feature(Machine Learning) パターン認識や機械学習の特徴量。 参考資料 KaggleのWinner solutionにもなった「K近傍を用いた特徴量抽出」のPython実装 2018.6.23 KaggleのWinner solutionにもなった「K近傍を用いた特徴量抽出」のPython実装 knn を利用した特徴量生成方法に関する説明です。 python での実装例があります。
やっていることは、 knn で任意のクラスの中で最近棒となる点との距離を特徴量として追加することです。</description></item><item><title>Point Cloud</title><link>https://iimuz.github.io/scrapbook/study/point_cloud/</link><pubDate>Mon, 30 Dec 2019 01:54:54 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/point_cloud/</guid><description>-Wikipedia: Point Cloud
参考資料 2020.2.6 Introducing PyTorch3D: An open-source library for 3D deep learning</description></item><item><title>Machine Vision</title><link>https://iimuz.github.io/scrapbook/study/machine_vision/</link><pubDate>Mon, 30 Dec 2019 01:47:19 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/machine_vision/</guid><description> Wikipeidia: Machine Vision</description></item><item><title>Reinforcement Learning</title><link>https://iimuz.github.io/scrapbook/study/reinforcement_learning/</link><pubDate>Mon, 30 Dec 2019 01:43:15 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/reinforcement_learning/</guid><description> Wikipedia: Reinforcement Learning</description></item><item><title>Counterfactual Visual Explanations</title><link>https://iimuz.github.io/scrapbook/study/counterfactual_visual_explanations/</link><pubDate>Tue, 15 Oct 2019 21:03:44 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/counterfactual_visual_explanations/</guid><description>arXiv arXiv Times 画像の分類根拠を可視化する手法。 野鳥が例に出されているが、何でも使えそう。 これ以外にも、可視化手法は調べると良いかもしれない。</description></item><item><title>Benchmarking Model-Based Reinforcement Learning</title><link>https://iimuz.github.io/scrapbook/study/benchmarking-model-based-reinforcement-learning/</link><pubDate>Tue, 13 Aug 2019 23:34:25 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/benchmarking-model-based-reinforcement-learning/</guid><description>論文情報 title: Benchmarking Model-Based Reinforcement Learning author: Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba year: 2019/7/3 arxiv issue vanity Google Translate: vanity 比較データなど どんなものか 強化学習の 14 手法に関して、同一のデータセットを利用して性能を比較した。
Figure 3: The relative performance with different planning horizon. 先行研究と比べてどこがすごいのか 従来は、各手法で優位性を述べており、同一のデータセットで比較し、 相対的な性能の優位性を適切に述べているものがなかった。
技術や手法のキモどこか OpenAI Gym という強化学習用のデータセットを利用して、 15 の環境で評価している。 また、ノイズを加えた状態も評価している。</description></item><item><title>Cold Case: the Lost MNIST Digits</title><link>https://iimuz.github.io/scrapbook/study/cold-case-the-lost-mnist-digits/</link><pubDate>Thu, 01 Aug 2019 11:54:22 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/cold-case-the-lost-mnist-digits/</guid><description>軽く読んだが再構成はして、評価しなおしているがデータセットが公開されているわけではないらしい。
arXiv issue vanity</description></item><item><title>ブラックホール撮影にも使えるスパースモデリングとは</title><link>https://iimuz.github.io/scrapbook/study/sparse_modeling_blackhole/</link><pubDate>Sun, 21 Apr 2019 11:17:18 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/sparse_modeling_blackhole/</guid><description>少々スパースモデリングがブラックホール撮影に使われているということで面白かったのでメモです。
ブラックホール撮影にも使える「スパースモデリング」とは？【機械学習】 要は、取得できる画像が低解像度なので高解像度にするのにスパースモデリングを使ったよということを起点としています。 この記事自体は実際に使われた手法に関して説明しているわけではなく、 スパースモデリングの基本的なことを説明しています。 Lasso を利用して下記の式を最適化しているようです。
$$ L(\bold{I}) = \parallel \bold{V} - \bold{F}\bold{I} \parallel_2^2 + \parallel \bold{I} \parallel_1 $$
$ \bold{I} $: ブラックホールの高解像度画像 $ \bold{V} $: 撮影した画像 (フーリエ変換済み) $ \bold{F} $: フーリエ変換 かなり以前から知られている範囲での説明に終わっているため、 実際には、これ以上の方法が用いられているようです。
復元結果 (引用 1) (引用 1) 本間ら「スパースモデリング天文学 ― ブラックホール撮像から時間変動減少まで」， 科学研究費補助金新学術領域研究「スパースモデリングの深化と高次元データ駆動科学の創成」 最終成果報告会 (2017/12/18-20)</description></item><item><title>三次元点群を扱うニューラルネットワークのサーベイ (ver.2)</title><link>https://iimuz.github.io/scrapbook/study/point_cloud_deep_servey/</link><pubDate>Sun, 21 Apr 2019 10:59:20 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/point_cloud_deep_servey/</guid><description>資料 資料リンク 著者などの情報 橋本、鏡研究室 どちらの方も 3 次元点群を利用する系統を研究されているようです。 産業用ロボットやプロジェクションマッピングなどを研究されています。 千葉 直也 2019/4 月現在は D3 の方</description></item><item><title>DCGAN のためのデータセット調査</title><link>https://iimuz.github.io/scrapbook/study/training_data_dcgan/</link><pubDate>Fri, 12 Apr 2019 06:48:27 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/training_data_dcgan/</guid><description>DCGAN で MNIST 以外を動かすとしたらどういうデータがありそうか調査。 本当は、最終的に Style GAN で適当な画像を入れたら、 それのアイコン(漫画チック)な画像を生成するようにしたい。
arXiveTimes がまとめているデータセット一覧
日本語の崩し文字: MNIST の日本語版みたいな感じか。 The Art Institute of Chicago THE COLLECTION: シカゴ美術館の絵画などの画像。絵画以外も入っている。 Painter by Numbers(PBN): 36GB くらいあるが絵画の画像が手に入るっぽい。 Kaggle データ。 How Do Humans Sketch Objects? (TU-Berlin dataset): スケッチ画像が載っています。 Manga109: 漫画データのようです。 ただし、利用にあたり学術目的のみであり、引用元の表記などが必要になるようです。 AnimeFace Character Dataset アニメの顔画像を集めたデータセット。 LLD - Large Logo Dataset: GAN を想定しているデータセットとのこと。 favicon のような画像を集めています。 Favicons: Kaggle のデータセットで favicon の寄せ集め。 Cartoon Set 2 次元のアバターイメージのデータセット。</description></item><item><title>600円のRasPiZero【単体で】Mobilenetv2 1000 class object detection【10fps弱】を達成</title><link>https://iimuz.github.io/scrapbook/study/dl_time_raspizero_mobilenet_10fps/</link><pubDate>Thu, 28 Mar 2019 23:37:03 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/dl_time_raspizero_mobilenet_10fps/</guid><description>概要 Nikkei Robotics 2019/3 において、RasPiZero で Mobilenet v2 の 10fps を実現できたことの紹介。 日本の Idein というベンチャー企業が成し遂げた。 これにより、 1000 円前後の環境で物体認識ができるということになり、エッジデバイスの単価が下がっている。 実現のためには、 RasPi に搭載している GPU を利用できるように、独自の RasPi 用の GPU ライブラリを作成し利用している。
トピック 全ての RasPi は GPU Broadcom 社製 VideoCore を標準搭載 VideoCore は 24 ～ 28.8[GFLOPS]と最新のスマホの 1/10 以下とはいえ初代 XBox よりは高速 RasPi の大ヒットを受けて 2014 年に Broadcom 社が仕様書を公開 日ベンチャー Idein 社は仕様書を読み込み VideoCore ハードウェアからソフトウェアスタックを積み上げ TensorFlow および Chainer で叩ける【VideoCore 版 CUDA のようなもの】を内製 VideoCore の DL 実行効率は 40%に達し 1GB のメインメモリに展開できるモデルならだいたい現実的な速度で叩けるらしい</description></item><item><title>Object Recognition in 3D Scenes with Occlusions and Clustter by Hough Voting</title><link>https://iimuz.github.io/scrapbook/study/objectrecognitionin3dsceneswithocclusionsandclustterbyhoughvoting/</link><pubDate>Tue, 13 Nov 2018 13:28:54 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/objectrecognitionin3dsceneswithocclusionsandclustterbyhoughvoting/</guid><description>Abstract 2010 Fourth Pacific-Rim Symposium on Image and Video Technology
Federico Tombari, Luigi Di Stefano
In this work we propose a novel Hough voting approach for the detection of free-form shapes in a 3D space, to be used for object recognition tasks in 3D scenes with a significant degree of occlusion and clutter. The proposed method relies on matching 3D features to accumulate evidence of the presence of the objects being sought in a 3D Hough space.</description></item><item><title>Unique Signatures of Histograms for Local Surface Description (SHOT)</title><link>https://iimuz.github.io/scrapbook/study/uniquesignaturesofhistogramsforlocalsurfacedescription_shot/</link><pubDate>Tue, 06 Nov 2018 18:18:47 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/uniquesignaturesofhistogramsforlocalsurfacedescription_shot/</guid><description>Unique Signatures of Histograms for Local Surface Description (SHOT) Abstract Federico Tombari, Samuele Salti, and Luigi Di Stefano
ECCV 2010
Abstract
This paper deals with local 3D descriptors for surface matching. First, we categorize existing methods into two classes: Signatures and Histograms. Then, by discussion and experiments alike, we point out the key issues of uniqueness and repeatability of the local reference frame. Based on these observations, we formulate a novel comprehensive proposal for surface representation, which encompasses a new unique and repeatable local reference frame as well as a new 3D descriptor.</description></item><item><title>Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</title><link>https://iimuz.github.io/scrapbook/study/unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery/</link><pubDate>Sun, 02 Sep 2018 11:27:34 +0900</pubDate><guid>https://iimuz.github.io/scrapbook/study/unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery/</guid><description>arXivTimes で見つけた論文です。 GAN を利用した異常検知技術を調べていた時に見つけました。
AnoGAN というネットワークを提案しています。 医療用画像を対象に、正常画像のみから、 GAN で学習していくようです。 このとき、 Generator は潜在空間から画像生成されると仮定します。 面白いのは、 GAN だけで完結しており、 検査時の入力画像は Auto Encoder のように再構成せず、 Generator から近い画像を生成するようにしているようです。 GAN は、潜在空間から画像は生成できるが、画像から潜在空間は変換できないはずです。 そこで、潜在空間は連続的に変化することから最初はランダムランプリングで潜在空間から画像を生成し、 近い画像へ潜在空間上を移動させるという処理を行うようです。 そのため、検査時に時間がかかる可能性はあるように思います。 また、残差誤差だけでなく、 Descriminator 側の出力も考慮して異常スコアを算出しているようです。
実装例: LeeDoYup/AnoGAN 日本語で解説してくれている記事です。 GAN による医療画像の異常検知 【論文読み】GAN を利用した異常検知まとめ その他に、論文を読んでいるときに調べながら見つけた論文 Unsupervised Adversarial Anomaly Detection using One-Class Support Vector Machines One-Class Adversarial Nets for Fraud Detection</description></item><item><title>Large Scale Learning of General Visual Representations for Transfer</title><link>https://iimuz.github.io/scrapbook/study/larget_scale_learning_of_general_visual_representation_for_transfer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/larget_scale_learning_of_general_visual_representation_for_transfer/</guid><description> arXiv 著者: exander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby 日時: 2019.12.24 パラメータ数が非常に巨大な(10 億程度)のモデルです。 基本的には、事前学習によって巨大なモデルを学習し、各タスクで Fine-Tuning することでタスクを解くことを前提としています。
参考資料 2020.1.15 Qiita パラメータ数 10 億！最新の巨大画像認識モデル「BiT」爆誕 &amp;amp; 解説</description></item><item><title>Noise2Noise: Learning Image Restoration without Clean Data</title><link>https://iimuz.github.io/scrapbook/study/noise2noise_learning_image_restoration_without_clean_data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://iimuz.github.io/scrapbook/study/noise2noise_learning_image_restoration_without_clean_data/</guid><description> arXiv 著者: Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila 投稿日: 2018.3.12 ノイズ画像からノイズ除去画像を生成する場合、ノイズなし画像を常に用意し続けることは難しい。 そこで、ノイズ画像を入力として、ノイズ画像を復元するというタスクを解く。 不思議なことをしているようだが、ノイズ除去した画像を復元するネットワークを学習することができる。
参考資料 2019.12.7 Qiita Noise2Noise 解説</description></item></channel></rss>