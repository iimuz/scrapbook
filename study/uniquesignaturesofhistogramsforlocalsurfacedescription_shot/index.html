<!doctype html><html lang=en><head><title>Unique signatures of histograms for local surface description ( s h o t) | しさく</title><meta charset=utf-8><meta name=generator content="Hugo 0.63.1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta http-equiv=x-ua-compatible content="IE=edge"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Unique Signatures of Histograms for Local Surface Description (SHOT)"><meta name=description content="Unique Signatures of Histograms for Local Surface Description (SHOT)  Abstract Federico Tombari, Samuele Salti, and Luigi Di Stefano
ECCV 2010
Abstract
This …"><meta property="og:description" content="Unique Signatures of Histograms for Local Surface Description (SHOT)  Abstract Federico Tombari, Samuele Salti, and Luigi Di Stefano
ECCV 2010
Abstract
This …"><meta property="og:url" content="https://iimuz.github.io/scrapbook/study/uniquesignaturesofhistogramsforlocalsurfacedescription_shot/"><meta property="og:image" content="images/%!s()"><meta name=twitter:card content="summary_large_image"><meta name=twitter:creator content><meta name=twitter:title content="Unique Signatures of Histograms for Local Surface Description (SHOT)"><meta property="twitter:description" content="Unique Signatures of Histograms for Local Surface Description (SHOT)  Abstract Federico Tombari, Samuele Salti, and Luigi Di Stefano
ECCV 2010
Abstract
This …"><meta name=twitter:image content="images/%!s()"><link rel=apple-touch-icon sizes=180x180 href=https://iimuz.github.io/scrapbook/images/icons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://iimuz.github.io/scrapbook/images/icons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://iimuz.github.io/scrapbook/images/icons/favicon-16x16.png><link rel=manifest href=https://iimuz.github.io/scrapbook/images/icons/site.webmanifest><link rel=canonical href=https://iimuz.github.io/scrapbook/study/uniquesignaturesofhistogramsforlocalsurfacedescription_shot/><link rel=stylesheet href=https://iimuz.github.io/scrapbook/css/styles.a1e1a9f0e58a84cfca79c98c11b9e900971b61f840c5fbe7f3ddad8ebcd9798517a0ea30089562091e9105239ce941ccc4953d8c3a0c08a2f5082ace791f0186.css integrity="sha512-oeGp8OWKhM/KecmMEbnpAJcbYfhAxfvn892tjrzZeYUXoOowCJViCR6RBSOc6UHMxJU9jDoMCKL1CCrOeR8Bhg=="><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.2/css/all.min.css></head><body><div class=nav-drop><div class=nav-body><a href=https://iimuz.github.io/scrapbook/search/ class=nav_item>search</a>
<a href=https://iimuz.github.io/scrapbook/tags/ class=nav_item>tags</a>
<a href=https://iimuz.github.io/scrapbook/categories/ class=nav_item>categories</a>
<a href=https://github.com/iimuz/til/ class=nav_item>til</a><div class=nav-close></div></div></div><header class=nav><nav class=nav-menu><a href=https://iimuz.github.io/scrapbook/ class="nav-brand nav_item">しさく</a><div class=nav_bar-wrap><div class=nav_bar></div></div></nav></header><main><section class=post_header style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)><h1 class=post_title>Unique Signatures of Histograms for Local Surface Description (SHOT)</h1></section><div class=post><article class=post_content><div><i class="far fa-calendar-check"></i>2018-11-06 18:18:47 +0900 +0900
<i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div><h1 id=unique-signatures-of-histograms-for-local-surface-description-shot>Unique Signatures of Histograms for Local Surface Description (SHOT)</h1><nav id=TableOfContents></nav><h1 id=abstract>Abstract</h1><p>Federico Tombari, Samuele Salti, and Luigi Di Stefano</p><p>ECCV 2010</p><p>Abstract</p><p>This paper deals with local 3D descriptors for surface matching.
First, we categorize existing methods into two classes: Signatures and Histograms.
Then, by discussion and experiments alike,
we point out the key issues of uniqueness and repeatability of the local reference frame.
Based on these observations,
we formulate a novel comprehensive proposal for surface representation,
which encompasses a new unique and repeatable local reference frame as well as a new 3D descriptor.
The latter lays at the intersection between Signatures and Histograms,
so as to possibly achieve a better balance between descriptiveness and robustness.
Experiments on publicly available datasets as well as on range scans
obtained with Spacetime Stereo provide a thorough validation of our proposal.</p><ul><li><a href=https://vision.deis.unibo.it/fede/papers/eccv10.pdf>pdf</a></li><li><a href=http://docs.pointclouds.org/1.8.1/classpcl_1_1_s_h_o_t_estimation.html>pcl</a></li><li><a href=http://www.vision.deis.unibo.it/research/80-shot>CV Lab</a><ul><li><a href=https://github.com/CVLAB-Unibo>GitHub: CVLAB-Unibo</a></li></ul></li><li><a href=https://github.com/fedassa/SHOT>GitHub: fedassa/SHOT</a></li></ul><table><thead><tr><th align=center>ヒストグラム算出のための空間分割</th><th align=center>実験結果</th></tr></thead><tbody><tr><td align=center><img src=https://gist.github.com/iimuz/839af5f4dc7c4555f584140865cce469/raw/63c256fcc798dcc10c28a15834b1bff9d2effdbb/shot_fig4.png alt></td><td align=center><img src=https://gist.github.com/iimuz/839af5f4dc7c4555f584140865cce469/raw/63c256fcc798dcc10c28a15834b1bff9d2effdbb/shot_fig7.png alt></td></tr></tbody></table><h1 id=pclで三次元物体認識>PCLで三次元物体認識</h1><p><a href=https://qiita.com/hakuturu583/items/3e82acb083779f388640>Qiita: PCLで三次元物体認識</a></p><p><a href=http://www.pointclouds.org/documentation/tutorials/correspondence_grouping.php>PCL のチュートリアル</a> を解説しています。
ここから、中京大学 橋本先生の SHOT 解説 pdf へのリンクを見つけました。</p><h1 id=中京大学-橋本先生-の-shot-解説>中京大学 橋本先生 の SHOT 解説</h1><p>2013年 <a href=http://isl.sist.chukyo-u.ac.jp/Archives/RSJ20130516.pdf>高速物体検出 ～ ロボットに使える2次元・3次元センシング ～</a></p><p>文献自体では、 2D の特徴抽出や 3D の特徴抽出に関して記載されています。
その中で、 p.61 に SHOT 特徴量の説明があります。</p><table><thead><tr><th align=center>SHOT</th></tr></thead><tbody><tr><td align=center><img src=https://gist.github.com/iimuz/839af5f4dc7c4555f584140865cce469/raw/d8b8c9dd3b12b54f0cfcc153a24021b053b28e4a/shot-hashimoto.png alt></td></tr></tbody></table><p>主成分分析により主方向の推定が入っているため、
SIFT のように回転不変に近くなるはずです。
一方で、スケール不変ではない特徴量になるはずです。</p><div class=post_extra><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div></article><aside><h3>Referenced from</h3><ul class="posts aside"></ul><h3>論文</h3><ul class="posts aside"><li class=post_item><a class=post_card href=/scrapbook/study/paper/ title=論文 style=background-image:url(https://upload.wikimedia.org/wikipedia/commons/c/c9/Annual_report_of_the_Michigan_Academy_of_Science_%28Page_70%29_BHL5411570.jpg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/paper/>論文</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li></ul><h3>Point Cloud</h3><ul class="posts aside"><li class=post_item><a class=post_card href=/scrapbook/study/point_cloud/ title="Point Cloud" style=background-image:url(https://cdn.pixabay.com/photo/2018/03/06/06/12/icon-3202634_1280.png)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/point_cloud/>Point Cloud</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/urban_semantic_3d_reconstruction_from_multiview_satellite_imagery/ title="Urban Semantic 3D Reconstruction from Multiview Satellite Imagery" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/urban_semantic_3d_reconstruction_from_multiview_satellite_imagery/>Urban Semantic 3D Reconstruction from Multiview Satellite Imagery</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/pytorch/ title=PyTorch style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/pytorch/>PyTorch</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/objectrecognitionin3dsceneswithocclusionsandclustterbyhoughvoting/ title="Object Recognition in 3D Scenes with Occlusions and Clustter by Hough Voting" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/objectrecognitionin3dsceneswithocclusionsandclustterbyhoughvoting/>Object Recognition in 3D Scenes with Occlusions and Clustter by Hough Voting</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li></ul><h3>More from しさく</h3><ul class="posts aside"><li class=post_item><a class=post_card href=/scrapbook/study/unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery/ title="Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery/>Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/larget_scale_learning_of_general_visual_representation_for_transfer/ title="Large Scale Learning of General Visual Representations for Transfer" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/larget_scale_learning_of_general_visual_representation_for_transfer/>Large Scale Learning of General Visual Representations for Transfer</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/study/noise2noise_learning_image_restoration_without_clean_data/ title="Noise2Noise: Learning Image Restoration without Clean Data" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/study class=post_tag>study</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/study/noise2noise_learning_image_restoration_without_clean_data/>Noise2Noise: Learning Image Restoration without Clean Data</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/software/point-cloud-library-data-format/ title="Point Cloud Library で利用しているデータフォーマット" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/software class=post_tag>software</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/software/point-cloud-library-data-format/>Point Cloud Library で利用しているデータフォーマット</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li><li class=post_item><a class=post_card href=/scrapbook/software/git-credential-gnome-keyring/ title="GONEM Keyring を利用した git credential の管理" style=background-image:url(https://iimuz.github.io/scrapbook/images/thumbnail.svg)></a><div class=excerpt><div class=excerpt_meta><a href=https://iimuz.github.io/scrapbook/tags/software class=post_tag>software</a><div class=copy data-share="Share Story" data-copied="Link Copied"><svg><use xlink:href="#copy"/></svg></div></div><h3 class=post_link><a href=/scrapbook/software/git-credential-gnome-keyring/>GONEM Keyring を利用した git credential の管理</a></h3><div class=post_link><div><i class="far fa-edit"></i>2020-03-27 10:17:42 +0000 UTC</div></div></div></li></ul></aside></div><script src=https://iimuz.github.io/scrapbook/js/autosize.min.js></script><script src=https://iimuz.github.io/scrapbook/js/timeago.js></script></main><svg width="0" height="0" class="hidden"><symbol viewBox="0 0 699.428 699.428" xmlns="http://www.w3.org/2000/svg" id="copy"><path d="M502.714.0H240.428C194.178.0 153 42.425 153 87.429l-25.267.59c-46.228.0-84.019 41.834-84.019 86.838V612c0 45.004 41.179 87.428 87.429 87.428H459c46.249.0 87.428-42.424 87.428-87.428h21.857c46.25.0 87.429-42.424 87.429-87.428v-349.19zM459 655.715H131.143c-22.95.0-43.714-21.441-43.714-43.715V174.857c0-22.272 18.688-42.993 41.638-42.993l23.933-.721v393.429C153 569.576 194.178 612 240.428 612h262.286c0 22.273-20.765 43.715-43.714 43.715zm153-131.143c0 22.271-20.765 43.713-43.715 43.713H240.428c-22.95.0-43.714-21.441-43.714-43.713V87.429c0-22.272 20.764-43.714 43.714-43.714H459c-.351 50.337.0 87.975.0 87.975.0 45.419 40.872 86.882 87.428 86.882H612zm-65.572-349.715c-23.277.0-43.714-42.293-43.714-64.981V44.348L612 174.857zm-43.714 131.537H306c-12.065.0-21.857 9.77-21.857 21.835s9.792 21.835 21.857 21.835h196.714c12.065.0 21.857-9.771 21.857-21.835.0-12.065-9.792-21.835-21.857-21.835zm0 109.176H306c-12.065.0-21.857 9.77-21.857 21.834.0 12.066 9.792 21.836 21.857 21.836h196.714c12.065.0 21.857-9.77 21.857-21.836.0-12.064-9.792-21.834-21.857-21.834z"/></symbol><symbol viewBox="0 0 60.015 60.015" xmlns="http://www.w3.org/2000/svg" id="reply"><path d="M42.007.0h-24c-9.925.0-18 8.075-18 18v14c0 9.59 7.538 17.452 17 17.973v8.344a1.694 1.694.0 001.699 1.698c.44.0.873-.173 1.198-.498l1.876-1.876C26.708 52.713 33.259 50 40.227 50h1.78c9.925.0 18-8.075 18-18V18c0-9.925-8.075-18-18-18zm16 32c0 8.822-7.178 16-16 16h-1.78c-7.502.0-14.556 2.921-19.86 8.226l-1.359 1.359V44a1 1 0 10-2 0v3.949c-8.356-.52-15-7.465-15-15.949V18c0-8.822 7.178-16 16-16h24c8.822.0 16 7.178 16 16v14z"/></symbol></svg><footer class=footer><div class="footer_inner wrap pale"><p>&copy; <span class=year></span>しさく</p><p>Designed by <a href="<no value>" title="Linkedin Profile"><no value></a></p></div></footer><script src=https://iimuz.github.io/scrapbook/js/index.js></script></body></html>